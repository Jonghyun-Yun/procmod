<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.68.3"><meta name=description content="Documentation for procmod"><meta name=author content="jonghyun-yun"><link rel=icon href=../images/favicon.png type=image/png><title>Draft :: procmod</title><link href=../css/nucleus.css?1638916072 rel=stylesheet><link href=../css/fontawesome-all.min.css?1638916072 rel=stylesheet><link href=../css/hybrid.css?1638916072 rel=stylesheet><link href=../css/featherlight.min.css?1638916072 rel=stylesheet><link href=../css/perfect-scrollbar.min.css?1638916072 rel=stylesheet><link href=../css/auto-complete.css?1638916072 rel=stylesheet><link href=../css/atom-one-dark-reasonable.css?1638916072 rel=stylesheet><link href=../css/theme.css?1638916072 rel=stylesheet><link href=../css/tabs.css?1638916072 rel=stylesheet><link href=../css/hugo-theme.css?1638916072 rel=stylesheet><link href=../css/theme-green.css?1638916072 rel=stylesheet><script src=../js/jquery-3.3.1.min.js?1638916072></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script><script type=text/x-mathjax-config>
      MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
    </script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=/draft/><nav id=sidebar><div id=header-wrapper><div id=header><a id=logo href=../><svg id="grav-logo" style="width:100%;height:100%" viewBox="0 0 504 140" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:1.41421"><path d="M235.832 71.564l-7.98-.001c-1.213.001-2.197.987-2.197 2.204v15.327l-.158.132c-4.696 3.962-10.634 6.14-16.719 6.14-14.356.0-26.034-11.68-26.034-26.037.0-14.358 11.678-26.035 26.034-26.035 5.582.0 10.919 1.767 15.437 5.113.877.649 2.093.56 2.866-.211l5.69-5.69c.444-.442.675-1.055.639-1.681-.034-.627-.336-1.206-.828-1.597-6.76-5.363-15.214-8.314-23.805-8.314-21.18.0-38.414 17.233-38.414 38.415.0 21.183 17.234 38.415 38.414 38.415 10.937.0 21.397-4.705 28.698-12.914.358-.403.556-.921.556-1.46v-19.603c0-1.217-.985-2.203-2.2-2.203" style="fill:#000;fill-rule:nonzero"/><path d="M502.794 34.445c-.408-.616-1.1-.989-1.838-.989h-8.684c-.879.0-1.673.522-2.022 1.329l-24.483 56.839-24.92-56.852c-.352-.799-1.142-1.316-2.012-1.316h-8.713c-.744.0-1.44.373-1.843.995-.408.623-.476 1.408-.174 2.09l30.186 68.858c.352.799 1.143 1.317 2.017 1.317h10.992c.879.0 1.673-.527 2.021-1.329l29.655-68.861c.289-.68.222-1.461-.182-2.081" style="fill:#000;fill-rule:nonzero"/><path d="M388.683 34.772c-.353-.798-1.142-1.316-2.017-1.316h-10.988c-.879.0-1.673.522-2.021 1.329l-29.655 68.861c-.294.675-.226 1.46.182 2.077.407.619 1.096.993 1.838.993h8.684c.879.0 1.673-.526 2.022-1.329l24.478-56.842 24.92 56.854c.353.798 1.143 1.317 2.013 1.317h8.717c.744.0 1.44-.374 1.843-.993.408-.624.471-1.41.174-2.094l-30.19-68.857z" style="fill:#000;fill-rule:nonzero"/><path d="M309.196 81.525l.476-.229c8.675-4.191 14.279-13.087 14.279-22.667.0-13.881-11.295-25.174-25.176-25.174h-31.863c-1.214.0-2.199.988-2.199 2.202v68.855c0 1.219.985 2.204 2.199 2.204h7.979c1.214.0 2.2-.985 2.2-2.204V45.833h21.684c7.059.0 12.799 5.739 12.799 12.796.0 5.885-3.996 10.989-9.728 12.408-1.032.261-2.064.393-3.071.393h-7.977c-.829.0-1.585.467-1.959 1.205-.378.74-.305 1.625.187 2.296l22.62 30.884c.412.566 1.07.901 1.771.901h9.915c.827.0 1.587-.467 1.96-1.207.378-.742.302-1.629-.186-2.296l-15.91-21.688z" style="fill:#000;fill-rule:nonzero"/><path d="M107.191 80.969c-7.255-4.794-11.4-8.845-15.011-16.109-2.47 4.977-8.236 12.376-17.962 18.198-4.856 15.106-27.954 44.015-35.43 39.916-2.213-1.212-2.633-2.808-2.133-4.456.536-4.129 9.078-13.62 9.078-13.62s.18 1.992 2.913 6.187c-3.609-11.205 5.965-25.031 8.5-29.738 3.985-1.269 4.274-6.387 4.274-6.387.255-7.909-3.278-13.635-6.701-17.059 2.459 3.002 3.255 7.539 3.372 11.694v.023c.012.469.012.93.011 1.39-.117 3.439-1.157 8.19-3.383 8.19l.006.03c-2.289-.098-5.115.391-7.639 1.18l-5.582 1.334s2.977-.136 4.584 1.252c-1.79 2.915-5.769 6.533-10.206 8.588-6.457 2.995-8.312-2.964-5.034-6.838.805-.946 1.618-1.745 2.387-2.399-.495-.513-.807-1.198-.889-2.068-.001-.005-.004-.009-.005-.013-.45-1.977-.202-4.543 2.596-8.623.551-.863 1.214-1.748 2.007-2.647.025-.031.046-.059.072-.089.034-.042.072-.08.108-.121.02-.023.039-.045.059-.068.2-.228.413-.45.639-.663 3.334-3.414 8.599-6.966 16.897-10.152 9.675-14.223 13.219-16.89 13.219-16.89 1.071-1.096 2.943-2.458 3.632-2.805-5.053-8.781-6.074-21.158-4.75-24.493-.107.18-.206.365-.287.556.49-1.143.819-1.509 1.328-2.111 1.381-1.632 6.058-2.488 7.737.971.895 1.844 1.063 4.232 1.034 6.023-3.704-.193-7.063 4.036-7.063 4.036s3.067-1.448 6.879-1.473c0 0 1.015.883 2.283 2.542-1.712 3.213-4.524 10.021-2.488 17.168.338 1.408.849 2.619 1.483 3.648.024.045.044.089.069.135.051.066.096.122.144.183 3.368 5.072 9.542 5.665 9.542 5.665-2.906-1.45-5.274-3.76-6.816-6.56-.8-1.498-1.291-2.762-1.592-3.761-1.636-6.313.771-9.999 2.149-12.471 3.17-4.917 8.944-7.893 15.151-7.185 8.712.995 14.968 8.862 13.973 17.571-.608 5.321-3.781 9.723-8.142 12.117 1.049 2.839-.073 6.28-.073 6.28 2.642 3.323 2.758 5.238 2.667 7.017-3.357-.565-6.618 1.701-6.618 1.701s6.476-1.546 10.238 1.81c2.446 2.631 4.078 5.009 5.051 6.766 1.393 2.505 7.859 2.683 7.123 7.188-.737 4.499-5.669 4.542-13.401-.56M69.571.0C31.147.0.0 31.148.0 69.567c0 38.422 31.147 69.573 69.571 69.573 38.42.0 69.568-31.151 69.568-69.573.0-38.42-31.148-69.567-69.568-69.567" style="fill:#000;fill-rule:nonzero"/><path d="M73.796 51.693c.813-.814.813-2.134.0-2.947-.815-.814-2.133-.814-2.947.0-.815.813-.815 2.133.0 2.947.814.813 2.132.813 2.947.0" style="fill:#000;fill-rule:nonzero"/><path d="M66.445 53.149c-.814.813-.814 2.133.0 2.947.813.814 2.133.814 2.947.0.813-.814.813-2.134.0-2.947-.814-.813-2.134-.813-2.947.0" style="fill:#000;fill-rule:nonzero"/><path d="M79.231 54.233c-1.274-1.274-3.339-1.272-4.611.0l-2.713 2.712c-1.274 1.275-1.274 3.339.0 4.612l2.978 2.978c1.274 1.275 3.338 1.274 4.611.0l2.712-2.712c1.274-1.274 1.274-3.339.0-4.612l-2.977-2.978z" style="fill:#000;fill-rule:nonzero"/><path d="M95.759 41.445c-2.151-2.578 1.869-7.257 4.391-4.463 4.645 5.148-2.237 7.041-4.391 4.463M105.004 44.132c3.442-6.553-1.427-10.381-4.773-13.523-5.36-5.039-10.706-7.217-16.811-.241-6.102 6.977-2.226 15.068 3.356 19.061 5.584 3.994 14.782 1.255 18.228-5.297" style="fill:#000;fill-rule:nonzero"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label><input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=../js/lunr.min.js?1638916072></script><script type=text/javascript src=../js/auto-complete.js?1638916072></script><script type=text/javascript>var baseurl="https:\/\/jonghyun-yun.github.io\/procmod";</script><script type=text/javascript src=../js/search.js?1638916072></script></div><section id=homelinks><ul><li><a class=padding href=../><i class="fas fa-home"></i>Home</a></li></ul></section><div class=highlightable><ul class=topics><li data-nav-id=/piacc/ title=PIACC class=dd-item><a href=../piacc/>PIACC</a><ul><li data-nav-id=/piacc/item_results/ title="item results" class=dd-item><a href=../piacc/item_results/>item results</a></li><li data-nav-id=/piacc/item_codebook/ title="item codebook" class=dd-item><a href=../piacc/item_codebook/>item codebook</a></li><li data-nav-id=/piacc/item_home/ title="item home" class=dd-item><a href=../piacc/item_home/>item home</a></li><li data-nav-id=/piacc/clustering/ title=clustering class=dd-item><a href=../piacc/clustering/>clustering</a></li><li data-nav-id=/piacc/gender_difference/ title="gender difference" class=dd-item><a href=../piacc/gender_difference/>gender difference</a></li></ul></li><li data-nav-id=/todo/ title=TODO class=dd-item><a href=../todo/>TODO</a></li><li data-nav-id=/model/ title=Model class=dd-item><a href=../model/>Model</a><ul><li data-nav-id=/model/v2/ title="Model V2" class=dd-item><a href=../model/v2/>Model V2</a></li><li data-nav-id=/model/v1/ title="Model V1" class=dd-item><a href=../model/v1/>Model V1</a></li></ul></li><li data-nav-id=/draft/ title=Draft class="dd-item
parent
active"><a href=../draft/>Draft</a><ul><li data-nav-id=/draft/tickets/ title=tickets class=dd-item><a href=../draft/tickets/>tickets</a></li></ul></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://github.com/jonghyun-yun/procmod><i class="fab fa-github"></i>Github repo</a></li></ul></section><section id=footer><p>Built with <a href=https://github.com/matcornic/hugo-theme-learn><i class="fas fa-heart"></i></a>from <a href=https://getgrav.org>Grav</a> and <a href=https://gohugo.io/>Hugo</a></p></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i></a></span><span class=links>Draft</span></div></div></div><div id=head-tags></div><div id=chapter><div id=body-inner><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#what-s-process-data>what&rsquo;s process data?</a></li><li><a href=#benefits-of-process-data>Benefits of process data</a></li><li><a href=#challenges>Challenges</a></li><li><a href=#exisitng-methods-and-limitations>Exisitng methods and limitations</a></li><li><a href=#motivation-and-our-aim>Motivation and our Aim</a></li><li><a href=#advantages-of-the-proposed-method>Advantages of the proposed method</a></li><li><a href=#paper-org-dot>paper org.</a></li></ul></li><li><a href=#motivating-example>Motivating example</a><ul><li><a href=#problem-solving-in-technology-rich-environments>Problem Solving in Technology-Rich Environments</a><ul><li><a href=#questions-we-like-to-answer--from-dr-dot-jeon-s-proposal----posts-dot-org>Questions we like to answer (from Dr. Jeon&rsquo;s proposal)</a></li><li><a href=#illustrate-a-ticket-example>Illustrate a ticket example:</a></li><li><a href=#address-challenges-in-details>address challenges in details</a></li></ul></li></ul></li><li><a href=#methods>Methods</a><ul><li><a href=#notation>notation</a></li><li><a href=#action-embedding>Action embedding</a></li><li><a href=#data-structure>data structure</a></li><li><a href=#multistate-model>Multistate model</a></li></ul></li><li><a href=#estimation>Estimation</a><ul><li><a href=#likelihood>likelihood</a></li><li><a href=#prior>prior</a></li><li><a href=#update-kappa-m>update \(\kappa_{m}\):</a></li><li><a href=#update-tau-k>update \(\tau_{k}\)</a></li><li><a href=#update-theta-k>update \(\theta_{k}\)</a></li><li><a href=#update-sigma-2>update \(\sigma^{2}\)</a></li></ul></li><li><a href=#applications>Applications</a><ul><li><a href=#tickets>tickets</a></li><li><a href=#party-invitations-1>party_invitations-1</a></li><li><a href=#book-order>book_order</a></li></ul></li><li><a href=#references>References</a></li></ul><p><a href=~/Dropbox/research/procmod/ies_naep/main.tex>NEAP proposal</a></p><h1 id=introduction>Introduction</h1><p>%%% stolen from the neap proposal</p><h2 id=what-s-process-data>what&rsquo;s process data?</h2><p>%Advances in technology have expanded opportunities for educational measurement through changes to item design, item delivery and data collection. Some examples include simulation-, scenario-, and game-based assessment and learning environments.</p><p>The NAEP computerized testing format provides an interactive environment for students.
Students can choose among a set of available actions and take one or more steps to finish a task. All student actions are automatically recorded in system log (Kerr, Chung, \& Iseli, 2011), which can be used %immediately for providing instant feedback to students
for diagnostic and scoring purposes (DiCerbo \& Behrens, 2014).</p><h2 id=benefits-of-process-data>Benefits of process data</h2><p>The availability of process data open new research opportunities including to better understand test-takers&rsquo; behavior patterns, &mldr;, and &mldr;.</p><h2 id=challenges>Challenges</h2><p>While the availability of rich response process data during problem solving comes the great challenge of building appropriate psychometric models to analyze these data.
The raw process data are usually formatted as lines of coded and time-stamped strings.
The vast amount of data on students&rsquo; potential trial-and-error process makes it less than straightforward to detect patterns in problem solving.</p><h2 id=exisitng-methods-and-limitations>Exisitng methods and limitations</h2><p>Several data analysis techniques and models have been explored to uncover problem-solving patterns. For example, researchers used methods such as cluster analysis (Bergner, Shu, \& von Davier, 2014) and editing distance (Hao, Shu, Bergner, Zhu, \& von Davier, 2014).
Other researchers explored the method of combining Markov movesl and item response theory (IRT) framework. Process mining techniques such as Petri net were also used to study behavioral patterns (Howard, Johnson, \& Neitzel, 2010).
In addition, researchers used digraphs to visualize and analyze sequential process data collected from assessment.
Zhu, Shu, and von Davier (2016) used network visualization and analysis for understanding process data.</p><p>&lt;&chen_continuous-time_2020></p><h2 id=motivation-and-our-aim>Motivation and our Aim</h2><p>Students&rsquo; response outcomes are a result of a sequence of actions that they take.
The quality as well as quantity of actions vary across individuals as well as across items.
Understanding the action sequence and its relation to response outcomes will help us better understand the nature of response process and individual differences in the process.
Models that relate process data to process outcomes are rare in the current literature.</p><p>We propose to develop a new, network modeling framework for analyzing time-stamped sequences of actions taken by NAEP test takers. The innovative aspect of our proposal is that we view test takers’ sequences of actions collected in the computer-assisted NAEP assessment system as directed paths between actions in a network of possible actions. With our framework, researchers and policymakers can quantify and better understand how learners with disabilities process mathematics test items.</p><p>We have successfully collaborated to develop novel network-based modeling approaches for analyzing conventional assessment data on two papers (Jin \& Jeon, 2018; Jeon, Jin, Schweinberger, \& Baugh, 2020), with more papers in the pipeline. We will extend this model-based framework for analyzing NAEP process data. Since the number of possible actions is large and many test takers will choose a small subset of the possible actions, the data is sparse. To deal with the sparsity of the data, we use machine learning techniques. These machine learning techniques penalize models that are more complex than warranted by the data.</p><h2 id=advantages-of-the-proposed-method>Advantages of the proposed method</h2><p>Advantage I. An important advantage of our network-based approach is the introduction of a virtual, two-dimensional Euclidean map of the interplay between actions for different test takers. This interactive map could offer substantially enhanced insights into how and why learners with and without disabilities are different in their response behavior on the current NAEP mathematics assessment.</p><p>Advantage II. A second advantage of our network-based approach is that we can easily link the network of actions with test takers’ mathematics performance outcomes, their background information, as well as any technical accommodations they utilized during the test, which allows educators to identify which accommodations might be more effective than others in helping learners with disabilities to display their full ability within the digitalized NAEP assessment environment.</p><h2 id=paper-org-dot>paper org.</h2><p>We first develop xxx. We further develop xxx. The
remainder of this article is organized as follows.
In Section, we introduce . In Section , we present. Applications are given in
Section, followed by conclusions given in
Section.</p><h1 id=motivating-example>Motivating example</h1><h2 id=problem-solving-in-technology-rich-environments>Problem Solving in Technology-Rich Environments</h2><p>&lt;&oecd_technical_2019></p><p>Interactive tasks as implemented in the problem solving in a technology-rich
environment (PSTRE) domain in the Programme for the International Assessment of
Adult Competencies (PIAAC, OECD, 2013) and the problem solving domain in the
Programme for International Student Assessment (PISA, OECD, 2014) aim at mirroring
real-life problem-solving behavior (Goldhammer, Naumann, & Keßel, 2013). While correct
responses to such tasks can be assumed to stem from examinees having the skill set and
the motivation required to solve the task, incorrect responses can occur for a variety of
different reasons, ranging from lack of different subskills and/or metacompetencies required
to solve the task through misinterpreting instructions to examinees not exerting their best
Fo
effort and interacting quickly and superficially with the task at hand.</p><p>As a motivating example, we introduce problem solving in technology-rich environments (PSTRE) We introduce an example of pro</p><p>OECD Survey of Adult Skills (PIAAC) Log Data
Downloaded from <a href=https://piaac-logdata.tba-hosting.de/>https://piaac-logdata.tba-hosting.de/</a>
Problem Solving Items:</p><p>The Programme for the International Assessment of Adult Competencies (PIAAC) is a programme of assessment and analysis of adult skills. The major survey conducted as part of PIAAC is the Survey of Adult Skills. The Survey measures adults’ proficiency in key information-processing skills - literacy, numeracy and problem solving - and gathers information and data on how adults use their skills at home, at work and in the wider community.</p><p>This international survey is conducted in over 40 countries/economies and measures the key cognitive and workplace skills needed for individuals to participate in society and for economies to prosper.</p><p>The OECD Survey of Adult Skills (PIAAC) assesses the proficiency of adults in information processing skills. During the PIAAC assessement, user interactions were logged automatically. This means that most of the users’ actions within the assessment tool were recorded and stored with time stamps in separate files called log files.</p><blockquote><p>This refers to the ability to use technology to solve problems and accomplish complex tasks. It is not a measurement of “computer literacy”, but rather of the cognitive skills required in the information age – an age in which the accessibility of boundless information has made it essential for people to be able to decide what information they need, to evaluate it critically, and to use it to solve problems. In this survey, higher-order skills are identified along with basic proficiency.</p></blockquote><h3 id=questions-we-like-to-answer--from-dr-dot-jeon-s-proposal----posts-dot-org><a href=posts/.org>Questions we like to answer (from Dr. Jeon&rsquo;s proposal)</a></h3><ul><li>which sequences or actions are effective? given the person&rsquo;s ability and item difficulty</li><li>is the same sequence (strategy) effective for all items or not?</li><li>is the same sequence effective for all people?</li><li>if effective sequences are not the same across all items, can we extract some common features of effective sequences ?</li><li>which sequences or actions are more or less effective for students with disability?</li><li>any other person covariates? ability? that is, does the effectiveness of sequences depend on person abilities? (interaction between sequence and ability)</li><li>does the effect of the sequence change depending on how long it took? for instance, when it was taken in a shorter time, a sequence might have a positive effect, while it might have a negative effect when it was taken in a longer time.</li><li>instead of using the log time (continuous), it may be better or useful to use a categorical variable?</li><li>the effect of sequences on the success probability may be a function of item difficulty or other item features, for instance, item position, item types (e.g., multiple-choice vs. open-ended), item contents (algebra, geometry) ?</li></ul><h3 id=illustrate-a-ticket-example>Illustrate a ticket example:</h3><p></p><figure><img src=../ox-hugo/tickets_demo.png alt="Figure 1: An example of PS-TRE items. In this simulated web environment, respondents can access information required for ticket reservation."><figcaption><p>Figure 1: An example of PS-TRE items. In this simulated web environment, respondents can access information required for ticket reservation.</p></figcaption></figure><p>This item involves a scenario in which the respondent is asked to reserve all fooball game tickets that an entire group can attend. A group of friend provides thier availabilities via an online calendar. Respondents access and evaluate information from ticket-reservation web pages and online calendars in simulated web environment. Respondents are able to:</p><ul><li>Click on tabs for ticket reservation web pages and online calendar;</li><li>Click on checkboxes to choose game dates;</li><li>Manipulate drop-down menus for events, locations, and number of tikcets; and</li><li>Click on menu items or navigation icons.</li></ul><p></p><table><thead><tr><th>ID</th><th>Action</th><th>Time (sec)</th></tr></thead><tbody><tr><td>4016</td><td>START</td><td>0.0</td></tr><tr><td>4016</td><td>COMBOBOX-default_menu1.index=7</td><td>47.3</td></tr><tr><td>4016</td><td>COMBOBOX-default_menu2.index=2</td><td>51.8</td></tr><tr><td>4016</td><td>BUTTON_search-default_txt23</td><td>65.0</td></tr><tr><td>4016</td><td>CHECKBOX-check2</td><td>93.2</td></tr><tr><td>4016</td><td>BUTTON_available-pg1_txt47</td><td>96.0</td></tr><tr><td>4016</td><td>BUTTON_available-pg7_txt47</td><td>108.2</td></tr><tr><td>4016</td><td>COMBOBOX-pg2_menu1.index=19</td><td>136.7</td></tr><tr><td>4016</td><td>COMBOBOX-pg2_menu6.index=19</td><td>144.5</td></tr><tr><td>4016</td><td>BUTTON_submit-pg2_txt33</td><td>146.1</td></tr><tr><td>4016</td><td>BUTTON_submit_ok-u21p2pu5_txt2</td><td>148.9</td></tr><tr><td>4016</td><td>NEXT_INQUIRY-REQUEST</td><td>155.8</td></tr><tr><td>4016</td><td>END</td><td>157.3</td></tr></tbody></table><p>There are 172 unique observed actions.
On average, repondents spend 182 (IQR: 107) seconds on this time, and take 23 (IQR: 10) actions.</p><h3 id=address-challenges-in-details>address challenges in details</h3><p>The process data consists of pairs of actions and time stamps of each respondents.
Major challenges to establish a statistcal model taking the process data as an input are 1) unequal length of respondents&rsquo; actions sequences; 2) large number of distinct actions transitions; and 3) &mldr;</p><p>Thanks to the recent development of natual language processing</p><h1 id=methods>Methods</h1><p>%% stolen from neap proposal
We propose to develop a new modeling framework for analyzing time-stamped sequences of actions.
The innovative aspect of our proposed model is that we view users&rsquo; sequences of actions collected as ????? of possible actions.
With our framework, researchers and policymakers can quantify and better understand how learners with disabilities (??) process mathematics test items.</p><h2 id=notation>notation</h2><p>Let \(S\) denotes a set of all possible actions. For each action \(m \in S\), \(A_{m}\) denotes a set of competing actions \(\{l_1, \ldots, l_{n_m}\}\) that can be taken directly after \(m\).
Let \(Y_k(t)\) denote an action being taken by the $k$-th respondent at time \(t\). Then, a sequence of the $k$-th respondent&rsquo;s actions is \(S_{k} = \{y_{k}(t_{k,1}),y_{k}(t_{k,2}),\ldots, y_{k}(t_{k,M_{k}})\}\) whose length is \(M_{k}\).</p><p>We defion \( \delta_{k,n,m} = 1 \) if respondent \(k\)&lsquo;s $n$-th action is \(m\); \(0\) otherwise. Thus, \( \delta_{k,n,m} \delta_{k,n+1,l} = 1 \) means respondent \(k\)&lsquo;s $n$-th transition (\(n &lt; M_{k}\)) is from action \(m\) to action \(l\). Respondents are assumed to begin problem solving processes at time \(t=0\).</p><p>Let \(t_{k,n}\) denote entry time that the \(k\)-th respondent start the $n$-th action. So, his/her sojourn time of the $n$-th action is denoted by \(dt_{k,n} = t_{k,n+1} - t_{k,n}\) for \(n &lt; M_{k} - 1\).</p><h2 id=action-embedding>Action embedding</h2><p>A goal for action embedding is to substitute a symbolic representation with a vectoric representation of actions. Similar tasks are taken for natual language processing is called word embedding.</p><p>Actions that tend to “behave similarly (<em>need better term</em>)” end up close to one another in the embedding space. Instead of using the action symbol as a feature in the model, we can use its vector to exploit such similarities.</p><p>\[
p\left(w_{j} \mid w_{0}, u, v\right)=\frac{\exp \left(u\left(w_{0}\right)^{\top} v\left(w_{j}\right)\right)}{\sum_{w \in V} \exp \left(u\left(w_{0}\right)^{\top} v(w)\right)}
\]
where \(u: V \rightarrow \mathbb{R}^{k} \) and \(v: V \rightarrow \mathbb{R}^{k}\) are functions which map words to a word embedding—one for the pivot words, and the other for context.</p><p>Word2Vec &lt;mikolov_distributed_2013>.</p><ul><li>word2vec: skip-gram models coupled with negative sampling</li><li>negative sampling: to reduce computational cost of the cosine similarity (the denominator is summation of all words in vocabulary).</li><li>skip-gram: to maximize the probability of predicting context words given a target word. The probability is defined by the cosine similarity (softmax function) based on word embeddings. Words close in the Euclidean space are words 1) with similar meanings, 2) associated with the same part of a sentence, 3) with semantic association. The similarity can be learned from a large corpus. Unseen words in the training sample are embedded, so one can exploit the similarity information.</li><li>a bag-of-words (CBOW): to predict a target word given context words (neighbor of a target word)</li></ul><p>Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence. While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (n-gram is a contiguous sequence of n items from a given sample of text or speech). The context of a word can be represented through a set of skip-gram pairs of <code>(target_word, context_word)</code> where <code>context_word</code> appears in the neighboring context of <code>target_word</code>.</p><p>The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a <code>target_word</code> that can be considered context word. Take a look at this table of skip-grams for <code>target_words</code> based on different window sizes.</p><p>The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words, the objective can be written as the average log probability
\[
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)
\]
where <code>c</code> is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function.
\[
p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime}{ }^{\top} v_{w_{I}}\right)}
\]
where \(u: V \rightarrow \mathbb{R}^{k} \) and \(v: V \rightarrow \mathbb{R}^{k}\) are functions which map words to a word embedding.</p><p>Word embeddings tend to cluster together when the words they denote behave similarly. The notion of “behavior” in this case usually remains underspecified, but could refer to syntactic categorization (i.e., words most often associated with the same part of speech will cluster together) or semantic association (words that are semantically related cluster together). The similarity between word embedding vectors is often measured through such measures as the dot product or cosine similarity.
Most prominently, word embeddings assist with the treatment of words which do not appear in the training data of a given problem (such as parsing or part-of-speech tagging). The word embedding function can be learned by exploiting co-occurence data on a large corpus (without any annotation), and thus the vocabulary over which the word embedding function is constructed is larger than the one that the training data consists of, and covers a significant amount of the words in the test data, including “unseen words.” The reliance on co-occurrence statistics is based on the distributional hypothesis (Harris, 1954) which states that co-occurrence of words in similar contexts implies that they have similar meanings.
This model learns parameters that lead to a high-valued dot product for embeddings of frequently co-occuring pivot and context words (as the probability is pushed to the maximum in such cases). Therefore, through the contexts, words that are similar to each other in their co-occurrence patterns map to vectors that are close to each other in the Euclidean space.
Skip-gram modeling of the above form coupled with negative sampling is often referred to as one of the <code>word2vec</code> models &lt;mikolov_distributed_2013>. A second proposed model of word2vec is the continuous bag-of-words model (CBOW), which predicts a word from the context—in reverse from the skip-gram model.</p><p><a href=posts/.org>Word2Vec | Skip-grams | TensorFlow Core</a></p><p>How to convert an action sequence to a sequence
skip-gram
negative sampling
cosine similarity</p><h2 id=data-structure>data structure</h2><p><a href=https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv>https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv</a>
Given a configured transition matrix, \texttt{msm} &lt;&jackson_multi-state_2011></p><p>transform data to a desired &ldquo;long&rdquo; format:</p><table><thead><tr><th>person</th><th>entry</th><th>exit</th><th>from</th><th>to</th><th>observed</th><th>cov1</th><th>cov2</th><th>time cov</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>10</td><td>1</td><td>2</td><td>1</td><td>D12</td><td>θ D12</td><td></td></tr><tr><td>1</td><td>0</td><td>10</td><td>1</td><td>3</td><td>0</td><td>D12</td><td>θ D12</td><td></td></tr></tbody></table><h2 id=multistate-model>Multistate model</h2><p>The intensity function \(q_{ml}(\cdot)\) represents the instantaneous rate of jumping from action \(m\) to \(l\):</p><p>\begin{align*}
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t},
\end{align*}</p><p>where \(m \neq l\), \(m, l \in S\), and \(\mathcal{F}_t\) denotes the process up to time \(t\).</p><p>Action transition is assumed to follow Semi-Markovian, which means the intensity depends on the sojourn time (\(t - t_{m}\) ; time spent on the current action). This is often called &ldquo;clock reset&rdquo; approach as opposed to &ldquo;clock forward&rdquo; approach. Let \(dt_{m}\) denote the sojourn time.</p><p>Cox model</p><p>\begin{align}
q_{ml}\left(t ; \mathcal{F}_{t}\right) = & q_{ml} (t - t_{m}; \bm{\lambda}, \bm{\beta}, \mathbf{z}(t))\\<br>= & \lambda_{ml}(dt_{m}) e^{(\bm{\beta}&rsquo; \mathbf{z}(t) + \theta_{k}) D_{ml}},
\end{align}</p><p>for person \(k = 1,\ldots,N\), where \(\mathbf{z}(t)\) is time-varying covariates, \(\lambda_{kml}(t)\) is a baseline intensity function, \(D_{ml} \in [-1,1]\) denotes the cosine similarity between actions \(m\) and \(l\). The cosine similarity is obtained using <code>word2vec</code> on action sequences of an item. The closer the cosine value to 1, the greater the similarity between actions. The closer the cosine value to -1, the greater the dis-similarity between actions. This mean there are \(n_{m}\) corresponding intensity functions for state \(m\), and overall \(\sum_{m in S} n_m\) intensity functions.</p><p>We use the constant baseline hazard based on out-of-state transition speed and person&rsquo;s transition speed:
\[
\lambda_{ml}(dt) = \kappa_{m} \tau_{k} \text{ for } l \in A_{m}.
\]</p><p>A running model has no coviarate terms:
\[
q_{ml}\left(t ; \mathcal{F}_{t}\right) = q_{ml}(dt) = \kappa_{m} \tau_{k} e^{\theta_{k} D_{ml} }.
\]</p><ul><li><p>larger \(\kappa_{m}\) shorter time staying on action \(m\) (faster out-of-state transition)</p></li><li><p>larger \(\tau_{k}\), faster transition speed</p></li><li><p>larger \(\theta_{k}\), larger trasition rate towards a similar action. A person with large \(\theta_{k}\) tends to choose more coherent actions</p><p>%% stolen from the neap proposal
[multi-state survival model]</p></li></ul><p>%We take one-partite network view.
We take a network view on action sequences, where nodes are a set of predefined action and links represent action transitions.
Given that item \(k\) is chosen,
the action network of student \(i\) is represented by \(L \times L\) adjacency matrix.
Suppose student \(i\) at item \(k\) has chosen action \(A_{i,k,l}\). The transition probability of moving from \(A_{i,k,l}\) to some other action \(A_{i,k,m}\) among \(L\) actions is modeled with a multinomial logistic model</p><p>\begin{equation}\label{eq:action}
\mathbb{P} ( A_{i,k,m} | A_{i,k,l} )
= \frac{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ) }{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,1} ) + \cdots + \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,L} )},
\end{equation}</p><p>\noindent
where $ α(A)_m$ and \(\alpha^{(A)}_l\) are the main effects of the current and previous actions \(m\) and \(l\), and \(\alpha^{(A)}_{m,l}\) is the interaction effect of the two actions.
\(\beta_{m,l}^{(A)}\) represents the effect of moving from action \(A_{i,k,l}\) to \(A_{i,k,m}\), while
$ zi,k,l,m$ indicate observed or unobserved covariates that capture the movement.
For example, \(z_{i,k,l,m}\) can represent a distance between the two actions as in a latent space modeling approach (reference).
Figure \ref{fg:sequence} illustrates the direct paths for the sequences of actions taken by
two students, one represented with dashed paths and the other with solid paths.</p><p>\textcolor{red}{MJ: can we handle directions? choosing the same actions? }</p><p>\textcolor{cyan}{JY: incorporating action times in the transition probability&mldr;}
We assume symmetric transition probabilities between actions.
We define a function describing transition intensity (hazard) between actions \(m\) and \(l\) (\(m \neq l\)):</p><p>\begin{align*}
h (t ; A_{i,k,l} \rightarrow A_{i,k,m} ) = & \lim_{\delta t \to 0} \frac{P(A_{i,k}(t + \delta t) = m | A_{i,k}(t) = l)}{\delta t} \\<br>= & \lambda_{k,l\rightarrow m}(t) \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ),
\end{align*}</p><p>where \(\lambda_{k,l\rightarrow m}(t)\) is a baseline intensity function and \(A_{i,k}(t)\) is an action taken by person \(i\) at \(t\) for item \(k\).
The non-transition intensity of action \(m\) is
\[
h (t ; A_{i,k,m} \rightarrow A_{i,k,m} ) = \lambda_{k,m\rightarrow m}(t) \exp( \alpha^{(A)}_m).
\]</p><p>Then, the corresponding transition probability can be defined as</p><p>\begin{align*}
\mathbb{P} (t ; A_{i,k,l} \rightarrow A_{i,k,m} ) = & \frac{h(t; A_{i,k,l} \rightarrow A_{i,k,m})}{\sum_{l=1}^{L} h(t; A_{i,k,l} \rightarrow A_{i,k,m})}
\end{align*}</p><p>It is possible to include the outcome in this multi-state survival modeling framework. In such case, however, identifying meaningful ``subsequence of actions&rsquo;&rsquo; would not be straightforward as appeared in \eqref{eq:no-response1}. Perhaps, we can use this model for parsing action sequence, and use the subsequence for \eqref{eq:no-response1}?</p><h1 id=estimation>Estimation</h1><ul><li><a href=///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::79;;1>3.2 Normal data with a noninformative prior distribution org-id:{ce3939d9-fb55-4b01-8747-0f486c98c9e7}:org-id</a></li><li><a href=///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::591;;1>Continuous distributions org-id:{5c29e214-f86d-41d5-89a0-e164602bf6b8}:org-id</a></li></ul><h2 id=likelihood>likelihood</h2><p>\(\bm{\tau} = (\tau_{1},\ldots,\tau_{N})'\)
\(\bm{\theta} = (\theta_{1},\ldots,\theta_{N})'\)
\(\bm{\kappa} = (\kappa_{1},\ldots,\kappa_{M})'\)</p><p>\begin{align*}
q_{ml} (t ; \bm{\kappa, \theta, \tau}, \bm{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t) e^{(\bm{\beta}&rsquo; \mathbf{z}(t) + \theta_{k}) D_{ml}}\\<br>q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S
\end{align*}</p><p>The survival function is
\[
S_{ml}(dt) = e^{-\int_{0}^{dt_{m}} q_{ml}(x) \dd x}.
\]
Let \(\nu_{mlk}(t) = 1\) if person \(k\) jump from actions \(m\) to \(l\) at time \(t\); 0 otherwise.
\[
f_{ml}(t) = q_{ml}(t) S_{ml}(t)
\]
\[
likelihood =\prod_{k} f_{ml}(dt) \prod_{g \in A_{m}} S_{mg}(t),
\]
\[
f_{ml} = q_{ml}(t) S_{ml}(t),
S_{ml}(t) = e^{-\int_{0}^{t^{stop} - t^{start}} q_{ml}(t)\dd t}
\]</p><p>\[
S_{ml}(dt) = e^{-dt \kappa_{m} \omega_{l} \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }}
\]</p><p>\(n = 1,\ldots,M_{k}\): n-th action of k-th person, \(M_k\): sequence length</p><p>\( \delta_{k,n,m} = 1 \) if person k&rsquo;s n-th action is m.</p><p>\( \delta_{k,n,m} \delta_{k,n+1,l} = 1 \) for \(n &lt; M_{k}\) if person k&rsquo;s n-th transition is m to l.</p><p>time at starting state (one after START) is set to the first action (n=1), and the corresponding time is set to 0.
We present a fully Bayesian approach for estimating the proposed model.</p><h2 id=prior>prior</h2><p>For each \(m\), \(k\), we specify independent priors as follows:</p><p>\begin{align*}
\pi\left(\kappa_{m}\right) & \sim \operatorname{Gamma}(a_{\kappa}, b_{\kappa}); \\<br>\pi\left(\tau_{k}\right) & \sim \operatorname{Gamma}(a_{\kappa}, b_{\kappa}); \\<br>\pi\left(\theta_{k} | \sigma^{2}\right) & \sim \operatorname{N}(0, \sigma^{2}); \\<br>\pi\left(\sigma^{2}\right) & \sim \operatorname{Inv-Gamma}(a_{\sigma}, b_{\sigma}),\\<br>\end{align*}</p><p>\(\mbox{Inv-Gamma}(\alpha,\beta)\) denotes the inverse gamma distribution with shape \(\alpha >0\) and scale \(\beta >0\) whose density is</p><p>\[
\operatorname{Inv-Gamma}(y|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-(\alpha+1)} \exp( -\beta \frac{1}{y} ).
\]</p><p>where hyperparameters are chosen as
\[a_{\sigma}=0.0001, b_{\sigma}=0.0001, \mu_{\theta}=0, \text { and } &mldr;.\]</p><p>Based on our experience, the inference of \(\mathbf{\Theta}\) is highly sensitive to its variance \(\sigma^2\). Also, the configuration of latent embeddings highly depends on the scale parameter \(\gamma\) of the latent space. Rather than choosing sub-optimal tuning parameters, we use a layer of hyper-priors to learn optimal values of these parameters from data. We choose hyperparameters such that priors are minimally informative to facilitate the flexible Bayesian learning.</p><h2 id=update-kappa-m>update \(\kappa_{m}\):</h2><p>For each \(m\), we draw \(\kappa_m^{(t)}\) from
\(\mbox{Gamma}\left( a_{\tau} + \sum_{n=1}^{M_{k}} \sum_{k=1}^N \mbox{I}(\delta_{k,n,m} = 1) ,b_{\tau} + \sum_{n=1}^{M_{k}-1}\sum_{k=1}^{N} \sum_{ l \in A_m } dt_{k,n} \tau_{k}e^{(\theta_{k} + \beta) D_{ml}}\right)\)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>  <span style=color:#66d9ef>double</span> post_a <span style=color:#f92672>=</span> a_kappa <span style=color:#f92672>+</span> <span style=color:#ae81ff>1.0</span>;
  <span style=color:#66d9ef>double</span> post_b <span style=color:#f92672>=</span> b_kappa;
  <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>ii : oos_m) {
    <span style=color:#66d9ef>if</span> (status.at(ii)<span style=color:#f92672>==</span><span style=color:#ae81ff>1</span>) post_a <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1.0</span>;
    post_b <span style=color:#f92672>+=</span> tdiff(ii) <span style=color:#f92672>*</span> tau(sid.at(ii)) <span style=color:#f92672>*</span> std<span style=color:#f92672>::</span>exp((theta(sid.at(ii)) <span style=color:#f92672>+</span> beta) <span style=color:#f92672>*</span> dist(ii));
  }
  kappa <span style=color:#f92672>=</span> stan<span style=color:#f92672>::</span>math<span style=color:#f92672>::</span>gamma_rng(post_a, post_b, rng);
</code></pre></div><h2 id=update-tau-k>update \(\tau_{k}\)</h2><p>For each \(k\), we draw \(\tau_k^{(t)}\) from
\(\mbox{Gamma}\left( a_{\tau} + M_k, b_{\tau} + \sum_{n=1}^{M_{k}} \sum_{m \in S, l \in A_m } dt_{k,n} \kappa_{m}e^{(\theta_{k} + \beta) D_{ml}}\right)\)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>double</span> post_a <span style=color:#f92672>=</span> a_tau <span style=color:#f92672>+</span> <span style=color:#ae81ff>1.0</span>;
<span style=color:#66d9ef>double</span> post_b <span style=color:#f92672>=</span> b_tau;
<span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>ii : person_k) {
    <span style=color:#66d9ef>if</span> (status.at(ii) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>)
        post_a <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1.0</span>;
    post_b <span style=color:#f92672>+=</span>
        tdiff(ii) <span style=color:#f92672>*</span> kappa(acfrom.at(ii)) <span style=color:#f92672>*</span> std<span style=color:#f92672>::</span>exp((theta_k <span style=color:#f92672>+</span> beta) <span style=color:#f92672>*</span> dist(ii));
}
tau <span style=color:#f92672>=</span> stan<span style=color:#f92672>::</span>math<span style=color:#f92672>::</span>gamma_rng(post_a, post_b, rng);
</code></pre></div><h2 id=update-theta-k>update \(\theta_{k}\)</h2><p>For each \(k\), we draw \(\theta_k^{* }\)$ from a symmetric MH jumping distribution, and accept \(\theta_{m}^{(l)} = \theta_{m}^{* }\) with probability \(\min(1, r_{{\theta_{m}}^{* }})\) where</p><p>\begin{align*}
\log r_{{\theta_{k}}^{* }} =& \sum_{m \in S} \sum_{n=1}^{M_{k}} \left[ \delta_{k,n,m} (\theta_{k}^{* } - \theta_{k}^{(l-1)})D_{ml} -\sum_{l \in A_m} dt_{k,n} \kappa_{m} \tau_{k} e^{ \beta D_{ml} }(e^{\theta_{k}^{* }D_{ml}} - e^{\theta_{k}^{(l-1)} D_{ml} }) + \log \frac{\pi(\theta_{k}^{* })}{\pi(\theta_{k}^{(l-1)})} \right].\\<br>\end{align*}</p><h2 id=update-sigma-2>update \(\sigma^{2}\)</h2><p>We draw \((\sigma^{2})^{(t)}\) from
\[
p( \sigma^2|e.e.) \propto \mbox{Inv-Gamma}(\sigma^{2}|a,b) \prod N(\theta_{k} | \mu, \sigma^2)
\]
\(\sigma^{2} \sim \mbox{Inv-Gamma}(a + 0.5 * N, b + 0.5 + \sum \theta_{k}^2)\)
c.f) with flat prior:
\(\sigma^{2} \sim inv-gamma(0.5 * N, 0.5 + \sum \theta_{k}^2)\)</p><p>\(\mbox{Inv-Gamma}(\alpha,\beta)\) denotes the inverse gamma distribution with a density
\[
\mbox{Inv-Gamma}(y|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-(\alpha+1)} \exp \left(-\beta \frac{1}{y}\right).
\]</p><h1 id=applications>Applications</h1><h2 id=tickets>tickets</h2><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>out_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;tickets/&#34;</span>
cd $out_dir
cd figure
convert -density <span style=color:#ae81ff>300</span> tau_action.pdf tau_action.png
convert -density <span style=color:#ae81ff>300</span> theta_tau_res.pdf theta_tau_res.png
convert -density <span style=color:#ae81ff>300</span> time_action_more.pdf time_action_more-%d.png
convert -density <span style=color:#ae81ff>300</span> time_action.pdf time_action-%d.png
</code></pre></div><ul><li>τ: person&rsquo;s baseline hazard for action transition</li><li>θ: person&rsquo;s xxx to jump to a similar action for the next one</li></ul><table><thead><tr><th align=left>Name</th><th align=left>Label</th><th align=left>Value scheme</th></tr></thead><tbody><tr><td align=left>AGEG5LFS</td><td align=left>Age groups in 5-year intervals based on LFS groupings (derived)</td><td align=left>Derived - Age groups in equal 5 year intervals (1-10)</td></tr><tr><td align=left>NFEHRS</td><td align=left>Number of hours of participation in non-formal education (derived)</td><td align=left>NA</td></tr><tr><td align=left>EARNHRDCL</td><td align=left>Hourly earnings excluding bonuses for wage and salary earners, in deciles (derived)</td><td align=left>Derived - Decile</td></tr><tr><td align=left>LEARNATWORK</td><td align=left>Index of learning at work (derived)</td><td align=left>NA</td></tr><tr><td align=left>ICTHOME</td><td align=left>Index of use of ICT skills at home (derived)</td><td align=left>NA</td></tr><tr><td align=left>ICTWORK</td><td align=left>Index of use of ICT skills at work (derived)</td><td align=left>NA</td></tr><tr><td align=left>INFLUENCE</td><td align=left>Index of use of influencing skills at work (derived)</td><td align=left>NA</td></tr><tr><td align=left>NUMHOME</td><td align=left>Index of use of numeracy skills at home (basic and advanced - derived)</td><td align=left>NA</td></tr><tr><td align=left>NUMWORK</td><td align=left>Index of use of numeracy skills at work (basic and advanced - derived)</td><td align=left>NA</td></tr><tr><td align=left>READHOME</td><td align=left>Index of use of reading skills at home (prose and document texts - derived)</td><td align=left>NA</td></tr><tr><td align=left>READWORK</td><td align=left>Index of use of reading skills at work (prose and document texts - derived)</td><td align=left>NA</td></tr><tr><td align=left>TASKDISC</td><td align=left>Index of use of task discretion at work (derived)</td><td align=left>NA</td></tr><tr><td align=left>WRITHOME</td><td align=left>Index of use of writing skills at home (derived)</td><td align=left>NA</td></tr><tr><td align=left>WRITWORK</td><td align=left>Index of use of writing skills at work (derived)</td><td align=left>NA</td></tr></tbody></table><p><img src=../ox-hugo/lpa_plot-0.png alt>
<img src=../ox-hugo/lpa_plot-1.png alt></p><figure><img src=../ox-hugo/lpa_back_line.png></figure><p>Response: the smaller, the better</p><h2 id=mean>mean</h2><table><thead><tr><th align=right>tau</th><th align=right>theta</th><th align=right>naction</th><th align=right>spd</th><th align=right>res</th></tr></thead><tbody><tr><td align=right>-0.4123013</td><td align=right>0.1939430</td><td align=right>0.0105633</td><td align=right>0.2949513</td><td align=right>3.710588</td></tr><tr><td align=right>0.9344900</td><td align=right>-0.6655596</td><td align=right>0.3971435</td><td align=right>0.1263112</td><td align=right>5.219081</td></tr><tr><td align=right>1.1296467</td><td align=right>-1.7866636</td><td align=right>-1.9230219</td><td align=right>-1.9932064</td><td align=right>7.000000</td></tr><tr><td align=right>-0.9718218</td><td align=right>1.3403763</td><td align=right>0.1527688</td><td align=right>-0.0648598</td><td align=right>3.145251</td></tr></tbody></table><h2 id=sd>sd</h2><table><thead><tr><th align=right>tau</th><th align=right>theta</th><th align=right>naction</th><th align=right>spd</th><th align=right>res</th></tr></thead><tbody><tr><td align=right>0.4111353</td><td align=right>0.384638</td><td align=right>0.6375695</td><td align=right>0.2628682</td><td align=right>2.989527</td></tr><tr><td align=right>0.9065238</td><td align=right>0.553052</td><td align=right>1.2095532</td><td align=right>0.3520752</td><td align=right>2.745995</td></tr><tr><td align=right>0.9047816</td><td align=right>1.085768</td><td align=right>0.1671354</td><td align=right>2.6734464</td><td align=right>0.000000</td></tr><tr><td align=right>0.2688267</td><td align=right>0.354953</td><td align=right>0.5181867</td><td align=right>0.3940398</td><td align=right>2.883724</td></tr></tbody></table><h2 id=n>n</h2><table><thead><tr><th align=right>tau</th><th align=right>theta</th><th align=right>naction</th><th align=right>spd</th><th align=right>res</th></tr></thead><tbody><tr><td align=right>425</td><td align=right>425</td><td align=right>425</td><td align=right>425</td><td align=right>425</td></tr><tr><td align=right>283</td><td align=right>283</td><td align=right>283</td><td align=right>283</td><td align=right>283</td></tr><tr><td align=right>75</td><td align=right>75</td><td align=right>75</td><td align=right>75</td><td align=right>75</td></tr><tr><td align=right>179</td><td align=right>179</td><td align=right>179</td><td align=right>179</td><td align=right>179</td></tr></tbody></table><p>τ&lsquo;s covaritates:
θ&lsquo;s covaritates:
<img src=../ox-hugo/theta_tau_res.png alt>
<img src=../ox-hugo/tau_action.png alt>
<img src=../ox-hugo/time_action-3.png alt>
<img src=../ox-hugo/time_action_more-2.png alt>
<img src=../ox-hugo/time_action_more-5.png alt>
<img src=../ox-hugo/time_action_more-7.png alt>
<img src=../ox-hugo/time_action_more-8.png alt>
<img src=../ox-hugo/time_action_more-9.png alt>
<img src=../ox-hugo/time_action_more-10.png alt>
<img src=../ox-hugo/time_action_more-11.png alt>
<img src=../ox-hugo/time_action_more-13.png alt></p><h2 id=party-invitations-1>party_invitations-1</h2><p><img src=../ox-hugo/lpa_plot-0.png alt>
<img src=../ox-hugo/lpa_plot-1.png alt></p><figure><img src=../ox-hugo/lpa_back_line.png></figure><p>Response: the larger, the better</p><h3 id=clustering-w-tau-and-theta>clustering w/ tau and theta</h3><table><thead><tr><th align=right>tau</th><th align=right>theta</th><th align=right>naction</th><th align=right>spd</th><th align=right>res</th><th align=right>n</th></tr></thead><tbody><tr><td align=right>2.06 (1.10)</td><td align=right>-1.49 (0.96)</td><td align=right>0.08 (3.72)</td><td align=right>-4.43 (11.77)</td><td align=right>0.43 (1.13)</td><td align=right>7.00 (0.00)</td></tr><tr><td align=right>0.02 (0.65)</td><td align=right>0.63 (0.43)</td><td align=right>-0.21 (0.38)</td><td align=right>0.03 (0.00)</td><td align=right>2.77 (0.56)</td><td align=right>443.00 (0.00)</td></tr><tr><td align=right>-0.65 (0.57)</td><td align=right>-0.58 (1.11)</td><td align=right>-0.28 (0.70)</td><td align=right>0.03 (0.00)</td><td align=right>1.18 (1.32)</td><td align=right>309.00 (0.00)</td></tr><tr><td align=right>0.83 (1.33)</td><td align=right>-0.43 (0.89)</td><td align=right>0.84 (1.51)</td><td align=right>0.03 (0.00)</td><td align=right>1.90 (1.24)</td><td align=right>211.00 (0.00)</td></tr></tbody></table><h3 id=clustering-wo-tau-and-theta>clustering w/o tau and theta</h3><table><thead><tr><th align=right>naction</th><th align=right>spd</th><th align=right>CPROB1</th><th align=right>CPROB2</th><th align=right>res</th><th align=right>n</th></tr></thead><tbody><tr><td align=right>1.82 (4.16)</td><td align=right>-3.09 (9.85)</td><td align=right>0.98 (0.05)</td><td align=right>0.02 (0.05)</td><td align=right>0.70 (1.25)</td><td align=right>10.00 (0.00)</td></tr><tr><td align=right>-0.02 (0.90)</td><td align=right>0.03 (0.00)</td><td align=right>0.00 (0.01)</td><td align=right>1.00 (0.01)</td><td align=right>2.07 (1.23)</td><td align=right>960.00 (0.00)</td></tr></tbody></table><h2 id=book-order>book_order</h2><p><img src=../ox-hugo/lpa_plot-0.png alt>
<img src=../ox-hugo/lpa_plot-1.png alt></p><figure><img src=../ox-hugo/lpa_back_line.png></figure><p>Response: the larger, the better</p><h3 id=w-tau-and-theta>w/ tau and theta</h3><table><thead><tr><th align=right>tau</th><th align=right>theta</th><th align=right>naction</th><th align=right>spd</th><th align=right>res</th><th align=right>n</th></tr></thead><tbody><tr><td align=right>-0.18 (0.88)</td><td align=right>0.37 (0.49)</td><td align=right>0.21 (0.68)</td><td align=right>0.11 (0.42)</td><td align=right>2.27 (2.45)</td><td align=right>450.00 (0.00)</td></tr><tr><td align=right>0.83 (1.06)</td><td align=right>-1.67 (0.83)</td><td align=right>-1.34 (0.12)</td><td align=right>-0.25 (0.80)</td><td align=right>6.32 (1.92)</td><td align=right>88.00 (0.00)</td></tr><tr><td align=right>0.31 (1.32)</td><td align=right>-0.79 (1.73)</td><td align=right>0.95 (2.48)</td><td align=right>-0.96 (3.97)</td><td align=right>4.23 (3.05)</td><td align=right>26.00 (0.00)</td></tr></tbody></table><h3 id=wo-tau-and-theta>w/o tau and theta</h3><table><thead><tr><th align=right>naction</th><th align=right>spd</th><th align=right>CPROB1</th><th align=right>CPROB2</th><th align=right>res</th><th align=right>n</th></tr></thead><tbody><tr><td align=right>-0.04 (0.83)</td><td align=right>0.08 (0.45)</td><td align=right>0.99 (0.03)</td><td align=right>0.01 (0.03)</td><td align=right>2.91 (2.80)</td><td align=right>535.00 (0.00)</td></tr><tr><td align=right>0.73 (2.54)</td><td align=right>-1.45 (3.73)</td><td align=right>0.11 (0.16)</td><td align=right>0.89 (0.16)</td><td align=right>4.52 (3.01)</td><td align=right>29.00 (0.00)</td></tr></tbody></table><h1 id=references>References</h1><p>&lt;~/Zotero/myref.bib></p><footer class=footline></footer></div></div></div><div id=navigation><a class="nav nav-prev" href=../model/v1/ title="Model V1"><i class="fa fa-chevron-left"></i></a><a class="nav nav-next" href=../draft/tickets/ title=tickets style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=../js/clipboard.min.js?1638916072></script><script src=../js/perfect-scrollbar.min.js?1638916072></script><script src=../js/perfect-scrollbar.jquery.min.js?1638916072></script><script src=../js/jquery.sticky.js?1638916072></script><script src=../js/featherlight.min.js?1638916072></script><script src=../js/highlight.pack.js?1638916072></script><script>hljs.initHighlightingOnLoad();</script><script src=../js/modernizr.custom-3.6.0.js?1638916072></script><script src=../js/learn.js?1638916072></script><script src=../js/hugo-learn.js?1638916072></script><script src=../mermaid/mermaid.js?1638916072></script><script>mermaid.initialize({startOnLoad:true});</script></body></html>