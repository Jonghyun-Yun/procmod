#+title: PIACC data processing

#+hugo_base_dir: ./docs
#+hugo_front_matter_format: toml
#+hugo_level_offset: 0

# https://orgmode.org/manual/Export-Settings.html#Export-Settings
#+options: H:10 num:nil toc:t \n:nil @:t ::t |:t ^:nil ^:{} -:t f:t *:t <:t ':nil -:nil pri:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:nil

#+startup: overview noinlineimages logdone indent

#+latex_class: article
#+latex_class_options: [letterpaper,11pt]

#+latex_compiler: pdflatex

# comment out for reveal.js
# #+setupfile: ~/setup/my-theme-readtheorg.setup
#+setupfile: ~/org/latex_header.setup
#+setupfile: ~/org/orgmode_header.setup

#+property: header-args :eval never-export
#+property: header-args:R :eval never-export
#+property: header-args:ein :session localhost
#+property: header-args:jupyter-python :session *jupyter-piacc* :kernel tf

#+BEGIN_SRC emacs-lisp
  (jyun/org-latex-set-options 2.5)
#+END_SRC
* Home :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: ./
:EXPORT_TITLE: Process data modeling
:END:
#+begin_export html
# WIP: Process data modeling
To be shared during meetings.
#+end_export
* TODOs :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: todo
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 1
:EXPORT_TITLE: TODO
:END:

** pattern discovery
- divide tau by 1) number of actions
- use 2) total time for visualization
- 1) divided by 2)
- find persons (small # of actions, large # of actions: right vs wrong)
  - 시간이 많이 걸리고 맞은사람 vs 적게 걸리고 맞은 사람.
  - 적은 액션으로 맞은 사람 vs 많은 액션으로 틀린 사람.
- 무조건 빨리 푼다고 잘하는게 아니고, 느리거나 혹은 상이한 액션 개수로 정답에 이르는 프로세스 발견에 초점.
** justification + what item to use
select a few items fulfilling the justification sheme!

https://cran.r-project.org/web/packages/tidyLPA/vignettes/Introduction_to_tidyLPA.html
분화된 그룹 (더 많은 그룹) 이 있으면 OK

observed covaritates + response group classification
observed covaritates + tau and theta + response group classification

*** no gender effect?
Suppose a gender variable was significant RF for tau and theta.
this could be because of effect of other covariates
plot gender against tau or theta, and see if there was diff.
after control gender no significant
find gender diff in EDA, after age control no difference.

https://data-edu.github.io/tidyLPA/reference/AHP.html

Some items show the gender difference. However, they are not chosen ones for the further analysis.
** cluster analysis
characteristics of newly discovered groups using hidden traits
what these groups can tell you about???

** model validation
see if we can use model validation tech for mutil-state survival model.
See [[id:1237b9e6-288a-413a-a484-e535d817a3c1][multi-state survival model validation]]: posterior predictive checking

* PIACC :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: piacc
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 1
:EXPORT_TITLE: PIACC
:END:

#+begin_src toml :front_matter_extra t :exports none
pre = "<b>1. </b>"
#+end_src

#+begin_export html
### Chapter 1

# PIACC
#+end_export

Programme for the International Assessment of Adult Competencies (PIAAC)

** item home
:PROPERTIES:
:header-args:R: :tangle R/itemcode.R :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: item_home
:END:

See the codebook for details....... [[file:data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv][PS_BOOKLET_ITEM.csv]]

#+begin_src R
booklet = readr::read_csv("~/Dropbox/research/procmod/procmod-code/data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv")
booklet$NAME = stringr::str_replace_all(booklet$NAME, " ", "_")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "_-", "-")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "-_", "-")
booklet$NAME = tolower(booklet$NAME)
#+end_src

*** party_invitations-1 :ATTACH:
:PROPERTIES:
:ID:       67fea2ad-8240-45dc-a4eb-da07f34a9e45
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U01a000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01a_(.*)","\\1\\2"), c("id=u01a_",  ""), c("\\*\\$target=",  ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_171956pi-start.png]]

*** party_invitations-2 :ATTACH:
:PROPERTIES:
:ID:       da5e5603-defc-43fb-a01c-16bfb293c5d8
:END:
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)

if (item_code == "U01b000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01b_(.*)","\\1\\2"), c("id=u01b_",  ""), c("\\*\\$target=",  ""), c("\\*\\$value=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172034pi-home.png]]

*** cd_tally :ATTACH:
:PROPERTIES:
:ID:       0cfd0f21-14b5-4993-a006-d6c13f9027ef
:END:
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)
if (item_code == "U03a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172043cd-start.png]]

*** sprained_ankle-1 :ATTACH:
:PROPERTIES:
:ID:       9c54e9eb-4153-4770-8b4c-0616c15bcebe
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U06a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

[[attachment:_20210904_172056sa-start.png]]

*** sprained_ankle-2 :ATTACH:
:PROPERTIES:
:ID:       c9b8890f-d9d6-4aa1-9505-48ed98d4f657
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U06b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172105sa-start2.png]]

*** tickets :ATTACH:
:PROPERTIES:
:ID:       47fa0d97-e0ab-44e8-87c7-dc6f6946f786
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U21x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172128ft-home.png]]

*** class_attendance :ATTACH:
:PROPERTIES:
:ID:       994a4a41-2040-4276-bc5f-f67966d69bde
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U04a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172140ca-email-home.png]]

*** club_membership-1 :ATTACH:
:PROPERTIES:
:ID:       50a5f4a2-d333-4bb2-8639-9a0dbf3d7a09
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U19a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172148cm-start.png]]

*** club_membership-2 :ATTACH:
:PROPERTIES:
:ID:       549f8614-8703-4373-b745-b44e3a7b84e3
:END:
#+begin_src R
if (item_code == "U19b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172157cm-home.png]]

*** book_order :ATTACH:
:PROPERTIES:
:ID:       fcf87b1e-7433-422d-9ce4-3770e71cbb6a
:END:
#+begin_src R
if (item_code == "U07x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172208bo-start.png]]

*** meeting_room :ATTACH:
:PROPERTIES:
:ID:       d9736569-11b6-4084-94a8-9782473453e1
:END:
#+begin_src R
if (item_code == "U16x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172218mr-email-home.png]]

*** reply_all :ATTACH:
:PROPERTIES:
:ID:       265dfcf6-ec9b-483e-b82b-8745a606ad0d
:END:
#+begin_src R
if (item_code == "U02x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172228ra-home.png]]

*** locate_email :ATTACH:
:PROPERTIES:
:ID:       ccbb321c-a463-47ba-bcf7-ac40fe1e8ff6
:END:
#+begin_src R
if (item_code == "U11b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172236le-home.png]]

*** lamp_return :ATTACH:
:PROPERTIES:
:ID:       e0aef17d-55dd-4641-a529-f63b2acd9d92
:END:
#+begin_src R
if (item_code == "U23x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172248lr-home.png]]

** item codebook
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: item_codebook
:END:

| ITEM  | NAME                  | CODEBOOK | RESPONSE                                                                 |
|-------+-----------------------+----------+--------------------------------------------------------------------------|
| PS1_1 | Party Invitations - 1 | U01a000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS1_2 | Party Invitations - 2 | U01b000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_3 | CD Tally              | U03a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_4 | Sprained Ankle - 1    | U06a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_5 | Sprained Ankle - 2    | U06b000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_6 | Tickets               | U21x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_7 | Class Attendance      | U04a000S | POLYTOMOUS (0 to 2), No response (R ), Not reached / not attempted (N)   |
| PS2_1 | Club Membership - 1   | U19a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_2 | Club Membership - 2   | U19b000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_3 | Book Order            | U07x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_4 | Meeting Room          | U02x000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_5 | Reply All             | U16x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_6 | Locate Email          | U11b000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_7 | Lamp Return           | U23x000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
** item results
:PROPERTIES:
:header-args: :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: item_results
:END:
*** important background covaraites
Random forest regression is used with \tau or \theta as a response and background variables in some categoires as covariates. P-value for the importance is reported. The selected categories are
c("Sampling / weighting", "Not assigned" ,"Sampling / weighting (derived)", "Background questionnaire (trend)"  ,"Background questionnaire", "Background questionnaire (derived)"

*** variables
- ~ftime~: time until the first action taken
- ~time~: total time of a person's process
- ~naction~ or ~#action~: the number of actions of a person's process

*** summaries
- Total or first action time has weak association with \theta.
- # of actions and first action time have weak or no assciation.
- # of actions has some association with the response.
- \tau and first action time has negative correaltion.
- \tau and # of actions have weak or no association
- # of actions and \theta time have positive correlation.
*** LPA
Latent profile analysis is done by =tidyLAP=. This R package uses multivariate Gaussian mixture models and reports many model selection criteria. See https://cran.r-project.org/web/packages/tidyLPA/vignettes/Introduction_to_tidyLPA.html
The clustering is perfomed based on person characteristics. We compare models based on our estimated parameters, observed covariates and both.

party_invitations-1, tickets, book_order: these items give more clusters using hidden traits + observed traits than using observed traits alone.
*** party_invitations-1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: party_invitations-1
:END:
#+begin_src sh :async
out_dir="party_invitations-1/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src

**** Covariates
\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-1/tau_imp.txt")
    (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-1/theta_imp.txt")
    (buffer-string))
#+end_src

[[file:party_invitations-1/figure/theta_tau_res.png]]
[[file:party_invitations-1/figure/tau_action.png]]
[[file:party_invitations-1/figure/time_action-3.png]]
[[file:party_invitations-1/figure/time_action_more-2.png]]
[[file:party_invitations-1/figure/time_action_more-5.png]]
[[file:party_invitations-1/figure/time_action_more-7.png]]
[[file:party_invitations-1/figure/time_action_more-8.png]]
[[file:party_invitations-1/figure/time_action_more-9.png]]
[[file:party_invitations-1/figure/time_action_more-10.png]]
[[file:party_invitations-1/figure/time_action_more-11.png]]
[[file:party_invitations-1/figure/time_action_more-13.png]]

*** party_invitations-2
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: party_invitations-2
:END:

#+begin_src sh :async
out_dir="party_invitations-2/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-2/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-2/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:party_inviations-2/figure/theta_tau_res.png]]
[[file:party_inviations-2/figure/tau_action.png]]
[[file:party_inviations-2/figure/time_action-3.png]]
[[file:party_inviations-2/figure/time_action_more-2.png]]
[[file:party_inviations-2/figure/time_action_more-5.png]]
[[file:party_inviations-2/figure/time_action_more-7.png]]
[[file:party_inviations-2/figure/time_action_more-8.png]]
[[file:party_inviations-2/figure/time_action_more-9.png]]
[[file:party_inviations-2/figure/time_action_more-10.png]]
[[file:party_inviations-2/figure/time_action_more-11.png]]
[[file:party_inviations-2/figure/time_action_more-13.png]]

*** cd_tally
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: cd_tally
:END:

#+begin_src sh :async
out_dir="cd_tally/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "cd_tally/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "cd_tally/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:cd_tally/figure/theta_tau_res.png]]
[[file:cd_tally/figure/tau_action.png]]
[[file:cd_tally/figure/time_action-3.png]]
[[file:cd_tally/figure/time_action_more-2.png]]
[[file:cd_tally/figure/time_action_more-5.png]]
[[file:cd_tally/figure/time_action_more-7.png]]
[[file:cd_tally/figure/time_action_more-8.png]]
[[file:cd_tally/figure/time_action_more-9.png]]
[[file:cd_tally/figure/time_action_more-10.png]]
[[file:cd_tally/figure/time_action_more-11.png]]
[[file:cd_tally/figure/time_action_more-13.png]]

*** sprained_ankle-1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: sprained_ankle-1
:END:
:#+begin_src sh :async
out_dir="sprained_ankle-1/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates
\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "sprained_ankle-1/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "sprained_ankle-1/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:sprained_ankle-1/figure/theta_tau_res.png]]
[[file:sprained_ankle-1/figure/tau_action.png]]
[[file:sprained_ankle-1/figure/time_action-3.png]]
[[file:sprained_ankle-1/figure/time_action_more-2.png]]
[[file:sprained_ankle-1/figure/time_action_more-5.png]]
[[file:sprained_ankle-1/figure/time_action_more-7.png]]
[[file:sprained_ankle-1/figure/time_action_more-8.png]]
[[file:sprained_ankle-1/figure/time_action_more-9.png]]
[[file:sprained_ankle-1/figure/time_action_more-10.png]]
[[file:sprained_ankle-1/figure/time_action_more-11.png]]
[[file:sprained_ankle-1/figure/time_action_more-13.png]]

*** sprained_ankle-2 :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: sprained_ankle-2
END:

+begin_src sh :async
ut_dir="sprained_ankle-2/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "sprained_ankle-2/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "sprained_ankle-2/theta_imp.txt")
   (buffer-string))
+end_src

[file:sprained_ankle-2/figure/theta_tau_res.png]]
[file:sprained_ankle-2/figure/tau_action.png]]
[file:sprained_ankle-2/figure/time_action-3.png]]
[file:sprained_ankle-2/figure/time_action_more-2.png]]
[file:sprained_ankle-2/figure/time_action_more-5.png]]
[file:sprained_ankle-2/figure/time_action_more-7.png]]
[file:sprained_ankle-2/figure/time_action_more-8.png]]
[file:sprained_ankle-2/figure/time_action_more-9.png]]
[file:sprained_ankle-2/figure/time_action_more-10.png]]
[file:sprained_ankle-2/figure/time_action_more-11.png]]
[file:sprained_ankle-2/figure/time_action_more-13.png]]

*** tickets
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: tickets
:END:

#+begin_src sh :async
out_dir="tickets/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "tickets/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "tickets/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:tickets/figure/theta_tau_res.png]]
[[file:tickets/figure/tau_action.png]]
[[file:tickets/figure/time_action-3.png]]
[[file:tickets/figure/time_action_more-2.png]]
[[file:tickets/figure/time_action_more-5.png]]
[[file:tickets/figure/time_action_more-7.png]]
[[file:tickets/figure/time_action_more-8.png]]
[[file:tickets/figure/time_action_more-9.png]]
[[file:tickets/figure/time_action_more-10.png]]
[[file:tickets/figure/time_action_more-11.png]]
[[file:tickets/figure/time_action_more-13.png]]

*** class_attendance :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: class_attendance
END:

+begin_src sh :async
ut_dir="class_attendance/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src
**** LPA
#+begin_src emacs-lisp :exports results :results html :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "class_attendance/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "class_attendance/theta_imp.txt")
   (buffer-string))
+end_src

[file:class_attendance/figure/theta_tau_res.png]]
[file:class_attendance/figure/tau_action.png]]
[file:class_attendance/figure/time_action-3.png]]
[file:class_attendance/figure/time_action_more-2.png]]
[file:class_attendance/figure/time_action_more-5.png]]
[file:class_attendance/figure/time_action_more-7.png]]
[file:class_attendance/figure/time_action_more-8.png]]
[file:class_attendance/figure/time_action_more-9.png]]
[file:class_attendance/figure/time_action_more-10.png]]
[file:class_attendance/figure/time_action_more-11.png]]
[file:class_attendance/figure/time_action_more-13.png]]

*** club_membership-1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: club_membership-1
:END:

#+begin_src sh :async
out_dir="club_membership-1/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates
\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "club_membership-1/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "club_membership-1/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:club_membership-1/figure/theta_tau_res.png]]
[[file:club_membership-1/figure/tau_action.png]]
[[file:club_membership-1/figure/time_action-3.png]]
[[file:club_membership-1/figure/time_action_more-2.png]]
[[file:club_membership-1/figure/time_action_more-5.png]]
[[file:club_membership-1/figure/time_action_more-7.png]]
[[file:club_membership-1/figure/time_action_more-8.png]]
[[file:club_membership-1/figure/time_action_more-9.png]]
[[file:club_membership-1/figure/time_action_more-10.png]]
[[file:club_membership-1/figure/time_action_more-11.png]]
[[file:club_membership-1/figure/time_action_more-13.png]]

*** club_membership-2 :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: club_membership-2
END:

+begin_src sh
ut_dir="club_membership-2/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src

#+begin_src emacs-lisp :exports results :results html :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "club_membership-2/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "club_membership-2/theta_imp.txt")
   (buffer-string))
+end_src

[file:club_membership-2/figure/theta_tau_res.png]]
[file:club_membership-2/figure/tau_action.png]]
[file:club_membership-2/figure/time_action-3.png]]
[file:club_membership-2/figure/time_action_more-2.png]]
[file:club_membership-2/figure/time_action_more-5.png]]
[file:club_membership-2/figure/time_action_more-7.png]]
[file:club_membership-2/figure/time_action_more-8.png]]
[file:club_membership-2/figure/time_action_more-9.png]]
[file:club_membership-2/figure/time_action_more-10.png]]
[file:club_membership-2/figure/time_action_more-11.png]]
[file:club_membership-2/figure/time_action_more-13.png]]

*** book_order
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: book_order
:END:

#+begin_src sh :async
out_dir="book_order/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "book_order/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "book_order/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:book_order/figure/theta_tau_res.png]]
[[file:book_order/figure/tau_action.png]]
[[file:book_order/figure/time_action-3.png]]
[[file:book_order/figure/time_action_more-2.png]]
[[file:book_order/figure/time_action_more-5.png]]
[[file:book_order/figure/time_action_more-7.png]]
[[file:book_order/figure/time_action_more-8.png]]
[[file:book_order/figure/time_action_more-9.png]]
[[file:book_order/figure/time_action_more-10.png]]
[[file:book_order/figure/time_action_more-11.png]]
[[file:book_order/figure/time_action_more-13.png]]

*** meeting_room
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: meeting_room
:END:

#+begin_src sh :async
out_dir="meeting_room/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "meeting_room/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "meeting_room/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:meeting_room/figure/theta_tau_res.png]]
[[file:meeting_room/figure/tau_action.png]]
[[file:meeting_room/figure/time_action-3.png]]
[[file:meeting_room/figure/time_action_more-2.png]]
[[file:meeting_room/figure/time_action_more-5.png]]
[[file:meeting_room/figure/time_action_more-7.png]]
[[file:meeting_room/figure/time_action_more-8.png]]
[[file:meeting_room/figure/time_action_more-9.png]]
[[file:meeting_room/figure/time_action_more-10.png]]
[[file:meeting_room/figure/time_action_more-11.png]]
[[file:meeting_room/figure/time_action_more-13.png]]

*** reply_all :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: reply_all
END:

+begin_src sh :async
ut_dir="reply_all/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src
**** LPA
#+begin_src emacs-lisp :exports results :results html :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "reply_all/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "reply_all/theta_imp.txt")
   (buffer-string))
+end_src

[file:reply_all/figure/theta_tau_res.png]]
[file:reply_all/figure/tau_action.png]]
[file:reply_all/figure/time_action-3.png]]
[file:reply_all/figure/time_action_more-2.png]]
[file:reply_all/figure/time_action_more-5.png]]
[file:reply_all/figure/time_action_more-7.png]]
[file:reply_all/figure/time_action_more-8.png]]
[file:reply_all/figure/time_action_more-9.png]]
[file:reply_all/figure/time_action_more-10.png]]
[file:reply_all/figure/time_action_more-11.png]]
[file:reply_all/figure/time_action_more-13.png]]

*** locate_email
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: locate_email
:END:
:#+begin_src sh :async
out_dir="locate_email/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports none :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "locate_email/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "locate_email/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:locate_email/figure/theta_tau_res.png]]
[[file:locate_email/figure/tau_action.png]]
[[file:locate_email/figure/time_action-3.png]]
[[file:locate_email/figure/time_action_more-2.png]]
[[file:locate_email/figure/time_action_more-5.png]]
[[file:locate_email/figure/time_action_more-7.png]]
[[file:locate_email/figure/time_action_more-8.png]]
[[[file:locate_email/figure/time_action_more-9.png]]
[[file:locate_email/figure/time_action_more-10.png]]
[[file:locate_email/figure/time_action_more-11.png]]
[[file:locate_email/figure/time_action_more-13.png]]

*** lamp_return
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: lamp_return
:END:

#+begin_src sh :async
out_dir="lamp_return/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "lamp_return/tau_imp.txt")
    (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "lamp_return/theta_imp.txt")
    (buffer-string))
#+end_src

[[file:lamp_return/figure/theta_tau_res.png]]
[[file:lamp_return/figure/tau_action.png]]
[[file:lamp_return/figure/time_action-3.png]]
[[file:lamp_return/figure/time_action_more-2.png]]
[[file:lamp_return/figure/time_action_more-5.png]]
[[file:lamp_return/figure/time_action_more-7.png]]
[[file:lamp_return/figure/time_action_more-8.png]]
[[file:lamp_return/figure/time_action_more-9.png]]
[[file:lamp_return/figure/time_action_more-10.png]]
[[file:lamp_return/figure/time_action_more-11.png]]
[[file:lamp_return/figure/time_action_more-13.png]]

** gender difference
:PROPERTIES:
:header-args: :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: gender_difference
:END:

Wilcoxon p-values for gender difference in \tau and \theta
| output                 |          \tau |          \theta |
|------------------------+------------+------------|
| party_invitations-1/ * |  0.1755171 |  0.3172373 |
| party_invitations-2/   |  0.4404884 |  0.6082382 |
| cd_tally/              | 0.07097494 | 0.09468705 |
| sprained_ankle-1/      | 0.01260001 |  0.6150152 |
| tickets/ *             | 0.08539706 |   0.730782 |
| club_membership-1/     |  0.3772467 |  0.1507705 |
| book_order/ *          |  0.9709181 |  0.3778057 |
| meeting_room/          |  0.4096292 | 0.04091108 |
| locate_email/          |   0.991704 |  0.7721417 |
| lamp_return/           | 0.03020509 |    0.86869 |

=*= is used to denote items justified by LPA.
** clustering
:PROPERTIES:
:header-args: :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: clustering
:END:
#+begin_src R :exports code
ftime = timestamp[1] / 1000, naction = n(), time = timestamp[n()] / 1000, spd = naction / (ftime - time)
#+end_src
- \tau: person's baseline hazard for action transition
- \theta: person's xxx to jump to a similar action for the next one
#+begin_export html
|Name        |Label                                                                               |Value scheme                                          |
|:-----------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------|
|AGEG5LFS    |Age groups in 5-year intervals based on LFS groupings (derived)                     |Derived - Age groups in equal 5 year intervals (1-10) |
|NFEHRS      |Number of hours of participation in non-formal education (derived)                  |NA                                                    |
|EARNHRDCL   |Hourly earnings excluding bonuses for wage and salary earners, in deciles (derived) |Derived - Decile                                      |
|LEARNATWORK |Index of learning at work (derived)                                                 |NA                                                    |
|ICTHOME     |Index of use of ICT skills at home (derived)                                        |NA                                                    |
|ICTWORK     |Index of use of ICT skills at work (derived)                                        |NA                                                    |
|INFLUENCE   |Index of use of influencing skills at work (derived)                                |NA                                                    |
|NUMHOME     |Index of use of numeracy skills at home (basic and advanced - derived)              |NA                                                    |
|NUMWORK     |Index of use of numeracy skills at work (basic and advanced - derived)              |NA                                                    |
|READHOME    |Index of use of reading skills at home (prose and document texts - derived)         |NA                                                    |
|READWORK    |Index of use of reading skills at work (prose and document texts - derived)         |NA                                                    |
|TASKDISC    |Index of use of task discretion at work (derived)                                   |NA                                                    |
|WRITHOME    |Index of use of writing skills at home (derived)                                    |NA                                                    |
|WRITWORK    |Index of use of writing skills at work (derived)                                    |NA                                                    |
#+end_export
*** party_invitations-1
#+begin_src sh :exports none :var out_dir=(jyun/get-heading)
cd $out_dir
cd figure
convert -density 300 lpa_plot.pdf lpa_plot-%d.png
# convert -density 300 lpa_back.pdf lpa_back.png
convert -density 300 lpa_back_line.pdf lpa_back_line.png
#+end_src

#+RESULTS:

[[file:party_invitations-1/figure/lpa_plot-0.png]]
[[file:party_invitations-1/figure/lpa_plot-1.png]]
# [[file:party_invitations-1/figure/lpa_back.png]]
[[file:party_invitations-1/figure/lpa_back_line.png]]

Response: the larger, the better

#+begin_src R :exports results :var ob_out_dir=(jyun/get-heading) :results output html
library(diprom)
load(paste0(ob_out_dir, "/summaryw_res.RData"))
cat("\n### w/ tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
load(paste0(ob_out_dir, "/mod3_summaryw_res.RData"))
cat("\n### w/o tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
#+end_src

#+RESULTS:
#+begin_export html

### clustering w/ tau and theta


|          tau|        theta|      naction|           spd|         res|             n|
|------------:|------------:|------------:|-------------:|-----------:|-------------:|
|  2.06 (1.10)| -1.49 (0.96)|  0.08 (3.72)| -4.43 (11.77)| 0.43 (1.13)|   7.00 (0.00)|
|  0.02 (0.65)|  0.63 (0.43)| -0.21 (0.38)|   0.03 (0.00)| 2.77 (0.56)| 443.00 (0.00)|
| -0.65 (0.57)| -0.58 (1.11)| -0.28 (0.70)|   0.03 (0.00)| 1.18 (1.32)| 309.00 (0.00)|
|  0.83 (1.33)| -0.43 (0.89)|  0.84 (1.51)|   0.03 (0.00)| 1.90 (1.24)| 211.00 (0.00)|

### clustering w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
|  1.82 (4.16)| -3.09 (9.85)| 0.98 (0.05)| 0.02 (0.05)| 0.70 (1.25)|  10.00 (0.00)|
| -0.02 (0.90)|  0.03 (0.00)| 0.00 (0.01)| 1.00 (0.01)| 2.07 (1.23)| 960.00 (0.00)|
#+end_export

*** tickets

#+begin_src sh :async :exports none :var out_dir=(jyun/get-heading)
cd $out_dir
cd figure
convert -density 300 lpa_plot.pdf lpa_plot-%d.png
# convert -density 300 lpa_back.pdf lpa_back.png
convert -density 300 lpa_back_line.pdf lpa_back_line.png
#+end_src

#+RESULTS:

[[file:tickets/figure/lpa_plot-0.png]]
[[file:tickets/figure/lpa_plot-1.png]]
# [[file:tickets/figure/lpa_back.png]]
[[file:tickets/figure/lpa_back_line.png]]

Response: the smaller, the better
#+begin_src R :exports results :var ob_out_dir=(jyun/get-heading) :results output html
library(diprom)
load(paste0(ob_out_dir, "/summaryw_res.RData"))
cat("\n### w/ tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
load(paste0(ob_out_dir, "/mod3_summaryw_res.RData"))
cat("\n### w/o tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
#+end_src

#+RESULTS:
#+begin_export html
## mean


|        tau|      theta|    naction|        spd|      res|
|----------:|----------:|----------:|----------:|--------:|
| -0.4123013|  0.1939430|  0.0105633|  0.2949513| 3.710588|
|  0.9344900| -0.6655596|  0.3971435|  0.1263112| 5.219081|
|  1.1296467| -1.7866636| -1.9230219| -1.9932064| 7.000000|
| -0.9718218|  1.3403763|  0.1527688| -0.0648598| 3.145251|

## sd


|       tau|    theta|   naction|       spd|      res|
|---------:|--------:|---------:|---------:|--------:|
| 0.4111353| 0.384638| 0.6375695| 0.2628682| 2.989527|
| 0.9065238| 0.553052| 1.2095532| 0.3520752| 2.745995|
| 0.9047816| 1.085768| 0.1671354| 2.6734464| 0.000000|
| 0.2688267| 0.354953| 0.5181867| 0.3940398| 2.883724|

## n


| tau| theta| naction| spd| res|
|---:|-----:|-------:|---:|---:|
| 425|   425|     425| 425| 425|
| 283|   283|     283| 283| 283|
|  75|    75|      75|  75|  75|
| 179|   179|     179| 179| 179|
#+end_export

*** book_order

#+begin_src sh :async :exports none :var out_dir=(jyun/get-heading)
cd $out_dir
cd figure
convert -density 300 lpa_plot.pdf lpa_plot-%d.png
# convert -density 300 lpa_back.pdf lpa_back.png
convert -density 300 lpa_back_line.pdf lpa_back_line.png
#+end_src

#+RESULTS:

[[file:book_order/figure/lpa_plot-0.png]]
[[file:book_order/figure/lpa_plot-1.png]]
# [[file:book_order/figure/lpa_back.png]]
[[file:book_order/figure/lpa_back_line.png]]

Response: the larger, the better
#+begin_src R :exports results :var ob_out_dir=(jyun/get-heading) :results output html
library(diprom)
load(paste0(ob_out_dir, "/summaryw_res.RData"))
cat("\n### w/ tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
load(paste0(ob_out_dir, "/mod3_summaryw_res.RData"))
cat("\n### w/o tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
#+end_src

#+RESULTS:
#+begin_export html

### w/ tau and theta


|          tau|        theta|      naction|          spd|         res|             n|
|------------:|------------:|------------:|------------:|-----------:|-------------:|
| -0.18 (0.88)|  0.37 (0.49)|  0.21 (0.68)|  0.11 (0.42)| 2.27 (2.45)| 450.00 (0.00)|
|  0.83 (1.06)| -1.67 (0.83)| -1.34 (0.12)| -0.25 (0.80)| 6.32 (1.92)|  88.00 (0.00)|
|  0.31 (1.32)| -0.79 (1.73)|  0.95 (2.48)| -0.96 (3.97)| 4.23 (3.05)|  26.00 (0.00)|

### w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
| -0.04 (0.83)|  0.08 (0.45)| 0.99 (0.03)| 0.01 (0.03)| 2.91 (2.80)| 535.00 (0.00)|
|  0.73 (2.54)| -1.45 (3.73)| 0.11 (0.16)| 0.89 (0.16)| 4.52 (3.01)|  29.00 (0.00)|
#+end_export

* Model :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: model
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 2
:END:

** Model V1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: V1
:END:

See [[cite:&jackson_flexsurv_2016]]for available baseline functions.
Proportional baseline:
\[
  \lambda_{ml}(dt_{k,n}) = \lambda_{m0}(t) \lambda_{l} \tau_{k} \text{ for } l \in S_{m} \text{and} \lambda_{s_{m,1}}=1.
\]
\(dt_{k,n}\) denotes t_{k,n}^{stop} - t_{k,n}^{start}

Proportional hazard term:
\[
  e^{(\theta_{k} + \beta) D_{ml} }
\]
- add covariate later.
out-of-state, item, person parameters.
- no incercept term in prop. hazard if baseline contains constant in the same level.
- action m leads to more/less coherent action
- \(D_{ml}\) is bi-directional similarity mapping.
- including \(\beta_m\) doesn't make it directional.
- is \(\beta_k\) meaningful for item-specific action space? certainly not! this opens up the question about how actions should be defined. loosely defined without event_desciption or not.
*** option1: similar items share the same action space
no event description should be used.
*** option2: each item has its own action space (item-specific action space)

- use multi-state modeling framework to explain?
- target journal:
- grant application (check deadline)
- meeting at 4pm (CST)
- online learning platform: interaction with online resources, with instructors, with other people (communication length, contents) - team collaboration.
  - data will be available on Aug.
  + team science program (NIH)


The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$.

\begin{align*}
  q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}},
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated
Common out of state transition: \(\beta_{ml}=\beta_{m}\).

Baseline hazard:
\[
  \lambda_{ml}(t) = \alpha_{m1}(t) \alpha_{l} + \theta_{\lambda} \text{ for } l \neq 1.
\]
Proportional hazard term:
\[
  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}}
\]
- \(D_{ml}\) is bi-directional similarity embedding between actions $m$ and $l$.

The piecewise-constant baseline hazard is used:
\begin{equation}
\label{eq:1}
\lambda(t) = \lambda_j \text{ if } s_{j-1} \le t < s_{j},
\end{equation}
for $j = 1,\ldots,J$. $\lambda_{j}$ could be a function of the similarity. This would be similar to have a piecewise constant transition matrix (time-inhomogeneous Markov chain), but much simpler as you have a parametric model for constants. The cosine similiarity should be normalized before used.
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t) \exp( \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta} d_{i,m,l} ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.
** Model V2
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: V2
:END:

Let $S$ denotes a set of all possible action. For each state $m \in S$, $A_{m}$ denotes a set of competing transitions $\{l_1, \ldots, l_{n_m}\}$ that can be taken directly after $m$.
Let \(Y_k(t)\) denote an action of k-th respondent at time $t$. All respondents assmed to begin problem solving processes at time $t=0$.
*** Action embedding

*** Multistate model
The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$:
\begin{align*}
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S,
\end{align*}
where $\mathcal{F}_t$ denotes the process up to time $t$.

Action transition is assumed to follow Semi-Markovian, which means the intensity depends on the sojourn time (\(t - t_{m}\) ; time spent on the current action). This is often called "clock reset" approach as opposed to "clock forward" approach. Let $dt_{m}$ denote the sojourn time.

Cox model
\begin{align}
q_{ml}\left(t ; \mathcal{F}_{t}\right) = & q_{ml} (t - t_{m}; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t))\\
= & \lambda_{ml}(dt_{m})  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}},
\end{align}

for person $k = 1,\ldots,N$, where $\mathbf{z}(t)$ is time-varying covariates, $\lambda_{kml}(t)$ is a baseline intensity function, \(D_{ml} \in [-1,1]\) denotes the cosine similarity between actions $m$ and $l$. The cosine similarity is obtained using ~word2vec~ on action sequences of an item. The closer the cosine value to 1, the greater the similarity between actions. The closer the cosine value to -1, the greater the dis-similarity between actions. This mean there are $n_{m}$ corresponding intensity functions for state $m$, and overall $\sum_{m in S} n_m$ intensity functions.

We use the constant baseline hazard based on out-of-state transition speed and person's transition speed:
\[
  \lambda_{ml}(dt) = \kappa_{m} \tau_{k} \text{ for } l \in A_{m}.
\]

A running model has no coviarate terms:
\[
q_{ml}\left(t ; \mathcal{F}_{t}\right) = q_{ml}(dt) = \kappa_{m} \tau_{k} e^{\theta_{k} D_{ml} }.
\]

- larger $\kappa_{m}$ shorter time staying on action $m$ (faster out-of-state transition)
- larger $\tau_{k}$, faster transition speed
- larger $\theta_{k}$, larger trasition rate towards a similar action. A person with large $\theta_{k}$ tends to choose more coherent actions

*** likelihood

\begin{align*}
    q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}}\\
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S
\end{align*}


The survival function is
\[
  S_{ml}(dt) = e^{-\int_{0}^{dt_{m}} q_{ml}(x) \dd x}.
\]
Let $\nu_{mlk}(t) = 1$ if person $k$ jump from actions $m$ to $l$ at time $t$; 0 otherwise.
\[
  f_{ml}(t) = q_{ml}(t) S_{ml}(t)
\]
\[
  likelihood =\prod_{k} f_{ml}(dt) \prod_{g \in A_{m}} S_{mg}(t),
\]
\[
  f_{ml} = q_{ml}(t) S_{ml}(t),
  S_{ml}(t) = e^{-\int_{0}^{t^{stop} - t^{start}} q_{ml}(t)\dd t}
\]

\[
  S_{ml}(dt) =  e^{-dt \kappa_{m} \omega_{l} \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }}
\]

\(n = 1,\ldots,M_{k}\): n-th action of k-th person, $M_k$: sequence length

\(  \delta_{k,n,m} = 1 \) if person k's n-th action is m.

\( \delta_{k,n,m}  \delta_{k,n+1,l} = 1 \) for $n < M_{k}$ if person k's n-th transition is m to l.

time at starting state (one after START) is set to the first action (n=1), and the corresponding time is set to 0.
*** prior
The proposed method use a fully Bayesian approach for estimating the proposed latent space model, using MCMC methods. Our prior specification is as follows:

\begin{align*}
\pi\left(\kappa_{m}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\tau_{k}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\theta_{k} | \sigma^{2}\right) & \sim \mathrm{N}\left(0, \sigma^{2}\right); \\
\pi\left(\sigma^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma}, b_{\sigma}\right); \\
\end{align*}

# \pi\left(\beta_{k} |\mu_{\beta}, \sigma_{\beta}^{2}\right) & \sim \mathrm{N}\left(\mu_{\beta}, \sigma_{\beta}^{2}\right); \\
# \pi\left(\mu_{\beta}|\sigma_{\beta}) & \sim \mathrm{N}\left(0, \sigma^{2}_{\mu_{\beta}}\right);\\
# \pi\left(\sigma_{\beta}^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma_{\beta}}, b_{\sigma_{\beta}}\right),\\

# \begin{align*}
# \pi\left(\sigma^{2}\right) & \propto \sigma^{-2}\\
# \pi\left(\mu_{\beta}, \sigma_{\beta}^{2}\right) & \propto \sigma_{\beta}^{-2},
# \end{align*}

inv-Gamma(\theta|\alpha,\beta)
\[
p(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha+1)} e^{-\beta / \theta}, \quad \theta>0
\]

where hyperparameters are chosen as
\[a_{\sigma}=0.0001, b_{\sigma}=0.0001, \mu_{\theta}=0, \text { and } ....\]

Based on our experience, the inference of $\mathbf{\Theta}$ is highly sensitive to its variance $\sigma^2$. Also, the configuration of latent embeddings highly depends on the scale parameter $\gamma$ of the latent space. Rather than choosing sub-optimal tuning parameters, we use a layer of hyper-priors to learn optimal values of these parameters from data. We choose hyperparameters such that priors are minimally informative to facilitate the flexible Bayesian learning.
*** pseudo code
:PROPERTIES:
:ID:       5c29e214-f86d-41d5-89a0-e164602bf6b8
:END:
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::79;;1][3.2 Normal data with a noninformative prior distribution org-id:{ce3939d9-fb55-4b01-8747-0f486c98c9e7}:org-id]]
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::591;;1][Continuous distributions org-id:{5c29e214-f86d-41d5-89a0-e164602bf6b8}:org-id]]
**** update \(\kappa_{m}\):
- For each $k,c$, a symmetric MH jumping $J(\kappa_{m}^{(l-1)} \rightarrow \kappa_{m}^{* })$ is used to propose a new sample.
- all $k$ person's having action m, all l \in A_m (all possible actions that can jump from m)
- transition start and stop time $dt_{k,n}$ for all $\delta_{k,n,m} = 1$
- We accept $\kappa_{m}^{(l)} = \kappa_{m}^{* }$ with probability $\min(1, r_{{\kappa_{m}}^{* }})$ where
\begin{align*}
\log r_{{\kappa_{m}}^{* }} =&
\sum \delta_{k,n,m} (\log \kappa_{m}^{* } - \log \kappa_{m}^{(l-1)})\\
&-\sum dt (\kappa_{m}^{* } - \kappa_{m}^{(l-1)}) \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }
+ \log \frac{\pi(\kappa_{m}^{* })}{\pi(\kappa_{m}^{t})}.
\end{align*}
**** update \(\tau_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
\begin{align*}
\log r_{{\tau_{k}}^*} =&
\sum \delta_{k,n,m} (\log \tau_{k}^* - \log \tau_{k}^{(l-1)})\\
&-\sum dt \kappa_{m}e^{(\theta_{k} + \beta) D_{ml}} ( \tau_{k}^* -  \tau_{k}^{(l-1)} )
+ \log \frac{\pi(\tau_{k}^*)}{\pi(\theta_{k}^{t})}.
\end{align*}
**** update \(\theta_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
- For each $k$, a symmetric MH jumping $J(\theta_{k}^{(l-1)} \rightarrow \theta_{k}^{* }$ is used to propose a new sample.
- We accept $\theta_{k}^{(l)} = \theta_{k}^{* }$ with probability $\min(1, r_{{\theta_{k}}^{* )}})$ where

\begin{align*}
\log r_{{\theta_{k}}^{* }} =& \sum \delta_{k,n,m} (\theta_{k}^{* } - \theta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \beta D_{ml} }(e^{\theta_{k}^{* }D_{ml}} -  e^{\theta_{k}^{(l-1)} D_{ml} })
+ \log \frac{\pi(\theta_{k}^{* })}{\pi(\theta_{k}^{(l-1)})}.\\
\end{align*}
**** update \(\sigma\)
\[
 p( \sigma^2|e.e.) \propto invGamma(\sigma^{2}|a,b) \prod N(\theta_{k} | \mu, \sigma^2)
\]
\(\sigma^{2} \sim inv-gamma(a + 0.5 * N, b + 0.5 + \sum \theta_{k}^2)\)
with flat prior:
\(\sigma^{2} \sim inv-gamma(0.5 * N, 0.5 + \sum \theta_{k}^2)\)
**** update \(\omega_{l}\) :noexport:ARCHIVE:
- all $k$ person's having action l, all m \in B_l (all possible actions that can jump to l)
- transition start and stop time $dt_{k,n-1}$ for all $\delta_{k,n,l} = 1$
**** update \(\beta\) :noexport:ARCHIVE:
- a symmetric MH jumping $J(\beta_{k}^{(l-1)} \rightarrow \beta_{k}^{* })$ is used to propose a new sample.
- We accept $\beta_{k}^{(l)} = \beta_{k}^{* }$ with probability $\min(1, r_{{\beta_{k}}^{* )}})$ where
\begin{align*}
\log r_{{\beta_{k}}^{* }} =&
\sum \delta_{k,n,m} (\beta_{k}^{* } - \beta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \theta_k D_{ml} }(e^{\beta^{* }D_{ml}} -  e^{\beta^{(l-1)} D_{ml} })
+ \log \frac{\pi(\beta_{k}^{* })}{\pi(\beta_{k}^{(l-1)})}.
\end{align*}
**** update \(\mu_{\beta}, \sigma_{\beta}\) :noexport:ARCHIVE:
\begin{align*}
  \rho &= 1/\sigma_{\beta}^{2} + 1/\sigma_{\mu_{\beta}}^2 \\
  p(\mu_{\beta}|..)&= N(\frac{1/\sigma_{\beta}^2 \times \beta}{\rho}, 1/\rho )
\end{align*}
\[
  p(\sigma_{\beta}^{2}|ee) = inv-Gamma(a + 0.5, b + 0.5 (\beta - \mu_{\beta})^2)
\]
*** data structure
**** mstate R package :noexport:
- how to format data to follow the below?
| person | entry time | exit time | from | to | observed | covariate1 | covariate2 | covariate3    | time dependent covaraite |
|      1 |          0 |        10 |    1 |  2 |        1 | D_{12}     | \theta D_{12}   | total actions | none                     |
|      1 |          0 |        10 |    1 |  3 |        0 | D_{12}     | \theta D_{12}   | total actions | none                     |
|        |            |           |      |    |          |            |            |               |                          |
|        |            |           |      |    |          |            |            |               |                          |
- mstate: cannot handle recurrent states
#+begin_src R
library(mstate)
# Transition matrix for illness-death model
tmat <- trans.illdeath()
# Data in wide format, for transition 1 this is dataset E1 of
# Therneau & Grambsch (T&G)
tg <- data.frame(id=1:6,illt=c(1,1,6,6,8,9),ills=c(1,0,1,1,0,1),
                 dt=c(5,1,9,7,8,12),ds=c(1,1,1,1,1,1),
                 x1=c(1,1,1,0,0,0),x2=c(6:1))
# Data in long format using msprep
tglong <- msprep(time=c(NA,"illt","dt"),status=c(NA,"ills","ds"),
                 data=tg,keep=c("x1","x2"),trans=tmat, id="id")
#+end_src

#+RESULTS:
: Loading required package: survival

#+begin_src R
# Same thing in etm format
tra <- trans2tra(tmat)
tgetm <- msdata2etm(tglong, id="id")
tgetm <- msdata2etm(tglong, id="id", covs=c("x1", "x2")) # with covariates
# And back
etm2msdata(tgetm, id="id", tra=tra)
etm2msdata(tgetm, id="id", tra=tra, covs=c("x1", "x2")) # with covariates
#+end_src

#+RESULTS:
: Error in trans2tra(tmat) : could not find function "trans2tra"
: Error in msdata2etm(tglong, id = "id") :
:   could not find function "msdata2etm"
: Error in msdata2etm(tglong, id = "id", covs = c("x1", "x2")) :
:   could not find function "msdata2etm"
: Error in etm2msdata(tgetm, id = "id", tra = tra) :
:   could not find function "etm2msdata"
: Error in etm2msdata(tgetm, id = "id", tra = tra, covs = c("x1", "x2")) :
:   could not find function "etm2msdata"

**** msm R package
https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv
#+begin_src R
library(msm)
msmdat <- data.frame(
 subj = c(1, 1, 1, 1, 1, 2, 2, 2),
 days = c(0, 27, 75, 97, 1106, 0, 90, 1037),
 status = c(1, 2, 3, 4, 4, 1, 2, 2),
 age = c(66, 66, 66, 66, 69, 49, 49, 51),
 treat = c(1, 1, 1, 1, 1, 0, 0, 0)
)
# transitions only allowed to next state up or state 4
Q <- rbind(c(1, 1, 0, 1),
           c(0, 1, 1, 1),
           c(0, 0, 1, 1),
           c(0, 0, 0, 0))
dat <- msm2Surv(data=msmdat, subject="subj", time="days", state="status",
         Q=Q)
dat
attr(dat, "trans")
#+end_src
* Draft
:PROPERTIES:
:header-args:R: :exports none :eval never-export
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: draft
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 3
:END:
[[file:~/Dropbox/research/procmod/ies_naep/main.tex::\section{Significance}][NEAP proposal]]

** Introduction
%%% stolen from the neap proposal

*** what's process data
%Advances in technology have expanded opportunities for educational measurement through changes to item design, item delivery and data collection. Some examples include simulation-, scenario-, and game-based assessment and learning environments.

The NAEP computerized testing format provides an interactive environment for students.
Students can choose among a set of available actions and take one or more steps to finish a task. All student actions are automatically recorded in system log (Kerr, Chung, \& Iseli, 2011), which can be used %immediately for providing instant feedback to students
for diagnostic and scoring purposes (DiCerbo \& Behrens, 2014).

*** Benefits of process data
The availability of process data open new research opportunities including to better understand test-takers' behavior patterns, ..., and ....


*** Challenges
While the availability of rich response process data during problem solving comes the great challenge of building appropriate psychometric models to analyze these data.
The raw process data are usually formatted as lines of coded and time-stamped strings.
The vast amount of data on students' potential trial-and-error process makes it less than straightforward to detect patterns in problem solving.


*** Exisitng methods and limitations
Several data analysis techniques and models have been explored to uncover problem-solving patterns. For example, researchers used methods such as cluster analysis (Bergner, Shu, \& von Davier, 2014) and editing distance (Hao, Shu, Bergner, Zhu, \& von Davier, 2014).
Other researchers explored the method of combining Markov movesl and item response theory (IRT) framework. Process mining techniques such as Petri net were also used to study behavioral patterns (Howard, Johnson, \& Neitzel, 2010).
In addition, researchers used digraphs to visualize and analyze sequential process data collected from assessment.
Zhu, Shu, and von Davier (2016) used network visualization and analysis for understanding process data.


*** Motivation and our Aim
Students' response outcomes are a result of a sequence of actions that they take.
The quality as well as quantity of actions vary across individuals as well as across items.
Understanding the action sequence and its relation to response outcomes will help us better understand the nature of response process and individual differences in the process.
Models that relate process data to process outcomes are rare in the current literature.

We propose to develop a new, network modeling framework for analyzing time-stamped sequences of actions taken by NAEP test takers. The innovative aspect of our proposal is that we view test takers’ sequences of actions collected in the computer-assisted NAEP assessment system as directed paths between actions in a network of possible actions.  With our framework, researchers and policymakers can quantify and better understand how learners with disabilities process mathematics test items.

We have successfully collaborated to develop novel network-based modeling approaches for analyzing conventional assessment data on two papers (Jin \& Jeon, 2018; Jeon, Jin, Schweinberger, \& Baugh, 2020), with more papers in the pipeline. We will extend this model-based framework for analyzing NAEP process data. Since the number of possible actions is large and many test takers will choose a small subset of the possible actions, the data is sparse. To deal with the sparsity of the data, we use machine learning techniques. These machine learning techniques penalize models that are more complex than warranted by the data.


*** Advantages of the proposed method
Advantage I. An important advantage of our network-based approach is the introduction of a virtual, two-dimensional Euclidean map of the interplay between actions for different test takers. This interactive map could offer substantially enhanced insights into how and why learners with and without disabilities are different in their response behavior on the current NAEP mathematics assessment.

Advantage II. A second advantage of our network-based approach is that we can easily link the network of actions with test takers’ mathematics performance outcomes, their background information, as well as any technical accommodations they utilized during the test, which allows educators to identify which accommodations might be more effective than others in helping learners with disabilities to display their full ability within the digitalized NAEP assessment environment.

*** paper org.
We first develop xxx. We further develop xxx. The
remainder of this article is organized as follows.
In Section, we introduce . In Section , we present. Applications are given in
Section, followed by conclusions given in
Section.

** Motivating example
*** Problem Solving in Technology-Rich Environments

OECD Survey of Adult Skills (PIAAC) Log Data
Downloaded from https://piaac-logdata.tba-hosting.de/
Problem Solving Items:

The Programme for the International Assessment of Adult Competencies (PIAAC) is a programme of assessment and analysis of adult skills. The major survey conducted as part of PIAAC is the Survey of Adult Skills. The Survey measures adults’ proficiency in key information-processing skills - literacy, numeracy and problem solving - and gathers information and data on how adults use their skills at home, at work and in the wider community.

This international survey is conducted in over 40 countries/economies and measures the key cognitive and workplace skills needed for individuals to participate in society and for economies to prosper.

The OECD Survey of Adult Skills (PIAAC) assesses the proficiency of adults in information processing skills. During the PIAAC assessement, user interactions were logged automatically. This means that most of the users’ actions within the assessment tool were recorded and stored with time stamps in separate files called log files.

#+begin_quote
This refers to the ability to use technology to solve problems and accomplish complex tasks. It is not a measurement of “computer literacy”, but rather of the cognitive skills required in the information age – an age in which the accessibility of boundless information has made it essential for people to be able to decide what information they need, to evaluate it critically, and to use it to solve problems. In this survey, higher-order skills are identified along with basic proficiency.
#+end_quote

**** [[id:32a1c1a4-f67b-428e-8458-16f4691b77a3][Questions we like to answer (from Dr. Jeon's proposal)]]
- which sequences or actions are effective? given the person's ability and item difficulty
- is the same sequence (strategy) effective for all items or not?
- is the same sequence effective for all people?
- if effective sequences are not the same across all items, can we extract some common features of effective sequences ?
- which sequences or actions are more or less effective for students with disability?
- any other person covariates? ability? that is, does the effectiveness of sequences depend on person abilities? (interaction between sequence and ability)
- does the effect of the sequence change depending on how long it took? for instance,  when it was taken in a shorter time, a sequence might have a positive effect, while it might have a negative effect when it was taken in a longer time.
- instead of using the log time (continuous), it may be better or useful to use a categorical variable?
- the effect of sequences on the success probability may be a function of item difficulty or other item features, for instance, item position, item types (e.g., multiple-choice vs. open-ended), item contents (algebra, geometry) ?

**** Illustrate a ticket example:

#+attr_latex: :placement [ht] :width 0.5\textwidth
#+label: fig:tickets_home
#+caption: An example of PS-TRE items. In this simulated web environment, respondents can access information required for ticket reservation.
[[file:tickets_demo.png]]

This  item  involves  a  scenario  in  which  the respondent  is asked to reserve all fooball game tickets that an entire group can attend. A group of friend provides thier availabilities via an online calendar. Respondents  access  and  evaluate  information from ticket-reservation web pages and online calendars in simulated web environment. Respondents are able to:
- Click on tabs for ticket reservation web pages and online calendar;
- Click on checkboxes to choose game dates;
- Manipulate drop-down menus for events, locations, and number of tikcets; and
- Click on menu items or navigation icons.

#+begin_src R :results silent
sub_str = rbind(
  ## c("(.*)\\*\\$target=u021_(.*)","\\1\\2"),
  c("TEST_TIME=(.+)",""),
  c("id=u021_", ""),
  c("id=",""),
  c("\\*\\$target=_", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", "index="),
  c("\\*\\$href=", "href="),
  c("\\|","\\.")
  )
ignore_desc = c(
"START",
"END",
"KEYPRESS")
piacc_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata"

item_code = "U21x000S"

item = read_piacc(piacc_path, item_code, sub_str, ignore_str)
unique(item$event_description)
length(unique(item$word))
#+end_src

#+begin_src R
library(kableExtra)
dd = item %>% filter(response == 1) %>% select(SEQID, word, timestamp)
aa = dd %>% filter(SEQID == 4016) %>% mutate(timestamp = round(timestamp / 1000,1))
aa %>% kbl(booktab = T, format = "latex")
#+end_src

#+attr_latex: :placement [H] :center t :font \sffamily
#+label: tab:df_action
#+caption: An example action sequence of one respondent. A sequence of actions (2nd column) taken is recorded with timestamp (3rd column).
|   ID | Action                         | Time (sec) |
|------+--------------------------------+------------|
| 4016 | START                          |        0.0 |
| 4016 | COMBOBOX-default_menu1.index=7 |       47.3 |
| 4016 | COMBOBOX-default_menu2.index=2 |       51.8 |
| 4016 | BUTTON_search-default_txt23    |       65.0 |
| 4016 | CHECKBOX-check2                |       93.2 |
| 4016 | BUTTON_available-pg1_txt47     |       96.0 |
| 4016 | BUTTON_available-pg7_txt47     |      108.2 |
| 4016 | COMBOBOX-pg2_menu1.index=19    |      136.7 |
| 4016 | COMBOBOX-pg2_menu6.index=19    |      144.5 |
| 4016 | BUTTON_submit-pg2_txt33        |      146.1 |
| 4016 | BUTTON_submit_ok-u21p2pu5_txt2 |      148.9 |
| 4016 | NEXT_INQUIRY-REQUEST           |      155.8 |
| 4016 | END                            |      157.3 |


#+begin_src R
item %>% group_by(SEQID) %>% summarise(mnum = max(event_num), time = max(timestamp)/1000) %>% summary %>%
#+end_src

#+RESULTS:
: `summarise()` ungrouping output (override with `.groups` argument)
:      SEQID           mnum         time
:  Min.   :   2   Min.   : 3   Min.   :  5.579
:  1st Qu.:1186   1st Qu.:18   1st Qu.:134.172
:  Median :2511   Median :23   Median :182.487
:  Mean   :2490   Mean   :23   Mean   :192.281
:  3rd Qu.:3778   3rd Qu.:28   3rd Qu.:241.393
:  Max.   :5010   Max.   :80   Max.   :833.395

There are 172 unique observed actions.
On average, repondents spend 182 (IQR: 107) seconds on this time, and take 23 (IQR: 10) actions.

**** address challenges in details
The process data consists of pairs of actions and time stamps of each respondents.
Major challenges to establish a statistcal model taking the process data as an input are 1) unequal length of respondents' actions sequences; 2) large number of distinct actions transitions; and 3) ...

Thanks to the recent development of natual language processing

** Methods
We propose to develop a new modeling framework for analyzing time-stamped sequences of actions taken by PS-TRE test takers. The innovative aspect of our proposal is that we view test takers’ sequences of actions collected in the computer-assisted PS-TRE assessment system as ????? of possible actions.  With our framework, researchers and policymakers can quantify and better understand how learners with disabilities (??) process mathematics test items.

Let $S$ denotes a set of all possible action. For each state $m \in S$, $A_{m}$ denotes a set of competing transitions $\{l_1, \ldots, l_{n_m}\}$ that can be taken directly after $m$.
Let \(Y_k(t)\) denote an action of k-th respondent at time $t$.The $k$-th respondent takes a sequence of $M_{k}$ actions. We use \(S_{k} = \{y_{k}(t_{k,1}),y_{k}(t_{k,2}),\ldots, y_{k}(t_{k,M_{k}})\}\) to denote the $k$-th respondent's action sequence.

We define \(  \delta_{k,n,m} = 1 \) if person k's n-th action is m; 0 otherwise. Thus, \( \delta_{k,n,m}  \delta_{k,n+1,l} = 1 \) menas person k's n-th transition ($n < M_{k}$) is m to l.
Respondents assumed to begin problem solving processes at time $t=0$.

Let \(t_{k,n}\) denote entry time that the \(k\)-th respondent start the $n$-th action. The sojourn time is denoted by $dt_{k,n} = t_{k,n+1} - t_{k,n}$ for $n < M_{k} - 1$.

*** Action embedding

A goal for action embedding is to substitute a symbolic representation with a vectoric representation of actions. Similar tasks are taken for natual language processing is called word embedding.

Actions that tend to “behave similarly (/need better term/)” end up close to one another in the embedding space. Instead of using the action symbol as a feature in the model, we can use its vector to exploit such similarities.

\[
  p\left(w_{j} \mid w_{0}, u, v\right)=\frac{\exp \left(u\left(w_{0}\right)^{\top} v\left(w_{j}\right)\right)}{\sum_{w \in V} \exp \left(u\left(w_{0}\right)^{\top} v(w)\right)}
\]
where \(u: V \rightarrow \mathbb{R}^{k} \) and \(v: V \rightarrow \mathbb{R}^{k}\) are functions which map words to a word embedding—one for the pivot words, and the other for context.

Word2Vec citep:mikolov_distributed_2013.

- word2vec: skip-gram models coupled with negative sampling
- negative sampling: to reduce computational cost of the cosine similarity (the denominator is summation of all words in vocabulary).
- skip-gram: to maximize the probability of predicting context words given a target word. The probability is defined by the cosine similarity (softmax function) based on word embeddings. Words close in the Euclidean space are words 1) with similar meanings, 2) associated with the same part of a sentence, 3) with semantic association. The similarity can be learned from a large corpus. Unseen words in the training sample are embedded, so one can exploit the similarity information.
- a bag-of-words (CBOW): to predict a target word given context words (neighbor of a target word)

Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence. While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (n-gram is a contiguous sequence of n items from a given sample of text or speech). The context of a word can be represented through a set of skip-gram pairs of ~(target_word, context_word)~ where ~context_word~ appears in the neighboring context of ~target_word~.

The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a ~target_word~ that can be considered context word. Take a look at this table of skip-grams for ~target_words~ based on different window sizes.


The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words, the objective can be written as the average log probability
\[
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)
\]
where ~c~ is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function.
\[
p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime}{ }^{\top} v_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime}{ }^{\top} v_{w_{I}}\right)}
\]
where \(u: V \rightarrow \mathbb{R}^{k} \) and \(v: V \rightarrow \mathbb{R}^{k}\) are functions which map words to a word embedding.

Word embeddings tend to cluster together when the words they denote behave similarly. The notion of “behavior” in this case usually remains underspecified, but could refer to syntactic categorization (i.e., words most often associated with the same part of speech will cluster together) or semantic association (words that are semantically related cluster together). The similarity between word embedding vectors is often measured through such measures as the dot product or cosine similarity.
Most prominently, word embeddings assist with the treatment of words which do not appear in the training data of a given problem (such as parsing or part-of-speech tagging). The word embedding function can be learned by exploiting co-occurence data on a large corpus (without any annotation), and thus the vocabulary over which the word embedding function is constructed is larger than the one that the training data consists of, and covers a significant amount of the words in the test data, including “unseen words.” The reliance on co-occurrence statistics is based on the distributional hypothesis (Harris, 1954) which states that co-occurrence of words in similar contexts implies that they have similar meanings.
This model learns parameters that lead to a high-valued dot product for embeddings of frequently co-occuring pivot and context words (as the probability is pushed to the maximum in such cases). Therefore, through the contexts, words that are similar to each other in their co-occurrence patterns map to vectors that are close to each other in the Euclidean space.
Skip-gram modeling of the above form coupled with negative sampling is often referred to as one of the =word2vec= models citep:mikolov_distributed_2013. A second proposed model of word2vec is the continuous bag-of-words model (CBOW), which predicts a word from the context—in reverse from the skip-gram model.


[[id:1b2724b9-324b-463b-967a-3b064a0f115e][Word2Vec | Skip-grams | TensorFlow Core]]

How to convert an action sequence to a sequence
skip-gram
negative sampling
cosine similarity
*** data structure
https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv
Given a configured transition matrix, \texttt{msm} [[citep:&jackson_multi-state_2011]]

 transform data to a desired "long" format:
| person | entry | exit | from | to | observed | cov1   | cov2     | time cov |
|      1 |     0 |   10 |    1 |  2 |        1 | D_{12} | \theta D_{12} |          |
|      1 |     0 |   10 |    1 |  3 |        0 | D_{12} | \theta D_{12} |          |
*** Multistate model

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$:
\begin{align*}
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S,
\end{align*}
where $\mathcal{F}_t$ denotes the process up to time $t$.

Action transition is assumed to follow Semi-Markovian, which means the intensity depends on the sojourn time (\(t - t_{m}\) ; time spent on the current action). This is often called "clock reset" approach as opposed to "clock forward" approach. Let $dt_{m}$ denote the sojourn time.

Cox model
\begin{align}
q_{ml}\left(t ; \mathcal{F}_{t}\right) = & q_{ml} (t - t_{m}; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t))\\
= & \lambda_{ml}(dt_{m})  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}},
\end{align}

for person $k = 1,\ldots,N$, where $\mathbf{z}(t)$ is time-varying covariates, $\lambda_{kml}(t)$ is a baseline intensity function, \(D_{ml} \in [-1,1]\) denotes the cosine similarity between actions $m$ and $l$. The cosine similarity is obtained using ~word2vec~ on action sequences of an item. The closer the cosine value to 1, the greater the similarity between actions. The closer the cosine value to -1, the greater the dis-similarity between actions. This mean there are $n_{m}$ corresponding intensity functions for state $m$, and overall $\sum_{m in S} n_m$ intensity functions.

We use the constant baseline hazard based on out-of-state transition speed and person's transition speed:
\[
  \lambda_{ml}(dt) = \kappa_{m} \tau_{k} \text{ for } l \in A_{m}.
\]

A running model has no coviarate terms:
\[
q_{ml}\left(t ; \mathcal{F}_{t}\right) = q_{ml}(dt) = \kappa_{m} \tau_{k} e^{\theta_{k} D_{ml} }.
\]

- larger $\kappa_{m}$ shorter time staying on action $m$ (faster out-of-state transition)
- larger $\tau_{k}$, faster transition speed
- larger $\theta_{k}$, larger trasition rate towards a similar action. A person with large $\theta_{k}$ tends to choose more coherent actions

  %% stolen from the neap proposal
  [multi-state survival model]
%We take one-partite network view.
We take a network view on action sequences, where nodes are a set of predefined action and links represent action transitions.
Given that item $k$ is chosen,
the action network of student $i$ is represented by $L \times L$ adjacency matrix.
Suppose student $i$ at item $k$ has chosen action $A_{i,k,l}$. The transition probability of moving from $A_{i,k,l}$ to some other action $A_{i,k,m}$ among $L$ actions is modeled with a multinomial logistic model
\begin{equation}\label{eq:action}
     \mathbb{P} ( A_{i,k,m}  |  A_{i,k,l} )
= \frac{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ) }{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,1} ) + \cdots + \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,L} )},
\end{equation}
\noindent
where $ \alpha^{(A)}_m$ and $\alpha^{(A)}_l$  are the main effects of the current and previous actions $m$ and $l$, and  $\alpha^{(A)}_{m,l}$ is the interaction effect of the two actions.
$\beta_{m,l}^{(A)}$ represents the effect of moving from action $A_{i,k,l}$ to $A_{i,k,m}$, while
$ z_{i,k,l,m}$ indicate observed or unobserved covariates that capture the movement.
For example, $z_{i,k,l,m}$ can represent a distance between the two actions as in a latent space modeling approach (reference).
Figure \ref{fg:sequence} illustrates the direct paths for the sequences of actions taken by
two students, one represented with dashed paths  and the other with solid paths.

\textcolor{red}{MJ: can we handle directions? choosing the same actions? }

\textcolor{cyan}{JY: incorporating action times in the transition probability...} \\
We assume symmetric transition probabilities between actions.
We define a function describing transition intensity (hazard) between actions $m$ and $l$ ($m \neq l$):
\begin{align*}
  h (t ;  A_{i,k,l} \rightarrow  A_{i,k,m}  ) = & \lim_{\delta t \to 0}  \frac{P(A_{i,k}(t + \delta t) = m | A_{i,k}(t) = l)}{\delta t} \\
  = & \lambda_{k,l\rightarrow m}(t) \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ),
\end{align*}
where $\lambda_{k,l\rightarrow m}(t)$ is a baseline intensity function and $A_{i,k}(t)$ is an action taken by person $i$ at $t$ for item $k$.
The non-transition intensity of action $m$ is
\[
  h (t ;  A_{i,k,m} \rightarrow  A_{i,k,m}  ) =   \lambda_{k,m\rightarrow m}(t) \exp( \alpha^{(A)}_m).
\]

Then, the corresponding transition probability can be defined as
\begin{align*}
  \mathbb{P} (t ;  A_{i,k,l} \rightarrow  A_{i,k,m} ) = & \frac{h(t; A_{i,k,l} \rightarrow A_{i,k,m})}{\sum_{l=1}^{L}  h(t; A_{i,k,l} \rightarrow A_{i,k,m})}
\end{align*}

It is possible to include the outcome in this multi-state survival modeling framework. In such case, however, identifying meaningful ``subsequence of actions'' would not be straightforward as appeared in \eqref{eq:no-response1}. Perhaps, we can use this model for parsing action sequence, and use the subsequence for \eqref{eq:no-response1}?

*** likelihood

\begin{align*}
    q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}}\\
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S
\end{align*}


The survival function is
\[
  S_{ml}(dt) = e^{-\int_{0}^{dt_{m}} q_{ml}(x) \dd x}.
\]
Let $\nu_{mlk}(t) = 1$ if person $k$ jump from actions $m$ to $l$ at time $t$; 0 otherwise.
\[
  f_{ml}(t) = q_{ml}(t) S_{ml}(t)
\]
\[
  likelihood =\prod_{k} f_{ml}(dt) \prod_{g \in A_{m}} S_{mg}(t),
\]
\[
  f_{ml} = q_{ml}(t) S_{ml}(t),
  S_{ml}(t) = e^{-\int_{0}^{t^{stop} - t^{start}} q_{ml}(t)\dd t}
\]

\[
  S_{ml}(dt) =  e^{-dt \kappa_{m} \omega_{l} \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }}
\]

\(n = 1,\ldots,M_{k}\): n-th action of k-th person, $M_k$: sequence length

\(  \delta_{k,n,m} = 1 \) if person k's n-th action is m.

\( \delta_{k,n,m}  \delta_{k,n+1,l} = 1 \) for $n < M_{k}$ if person k's n-th transition is m to l.

time at starting state (one after START) is set to the first action (n=1), and the corresponding time is set to 0.
**** prior
The proposed method use a fully Bayesian approach for estimating the proposed latent space model, using MCMC methods. Our prior specification is as follows:

\begin{align*}
\pi\left(\kappa_{m}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\tau_{k}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\theta_{k} | \sigma^{2}\right) & \sim \mathrm{N}\left(0, \sigma^{2}\right); \\
\pi\left(\sigma^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma}, b_{\sigma}\right); \\
\end{align*}

# \pi\left(\beta_{k} |\mu_{\beta}, \sigma_{\beta}^{2}\right) & \sim \mathrm{N}\left(\mu_{\beta}, \sigma_{\beta}^{2}\right); \\
# \pi\left(\mu_{\beta}|\sigma_{\beta}) & \sim \mathrm{N}\left(0, \sigma^{2}_{\mu_{\beta}}\right);\\
# \pi\left(\sigma_{\beta}^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma_{\beta}}, b_{\sigma_{\beta}}\right),\\

# \begin{align*}
# \pi\left(\sigma^{2}\right) & \propto \sigma^{-2}\\
# \pi\left(\mu_{\beta}, \sigma_{\beta}^{2}\right) & \propto \sigma_{\beta}^{-2},
# \end{align*}

inv-Gamma(\theta|\alpha,\beta)
\[
p(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha+1)} e^{-\beta / \theta}, \quad \theta>0
\]

where hyperparameters are chosen as
\[a_{\sigma}=0.0001, b_{\sigma}=0.0001, \mu_{\theta}=0, \text { and } ....\]

Based on our experience, the inference of $\mathbf{\Theta}$ is highly sensitive to its variance $\sigma^2$. Also, the configuration of latent embeddings highly depends on the scale parameter $\gamma$ of the latent space. Rather than choosing sub-optimal tuning parameters, we use a layer of hyper-priors to learn optimal values of these parameters from data. We choose hyperparameters such that priors are minimally informative to facilitate the flexible Bayesian learning.
** Estimation
:PROPERTIES:
:ID:       5c29e214-f86d-41d5-89a0-e164602bf6b8
:END:
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::79;;1][3.2 Normal data with a noninformative prior distribution org-id:{ce3939d9-fb55-4b01-8747-0f486c98c9e7}:org-id]]
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::591;;1][Continuous distributions org-id:{5c29e214-f86d-41d5-89a0-e164602bf6b8}:org-id]]
*** update \(\kappa_{m}\):
:PROPERTIES:
:ID:       bffeec1d-053a-4e74-8d30-40278b23d60c
:END:
For each $m$, we draw \(\kappa_m^{(t)}\) from
$\mbox{Gamma}\left(  a_{\tau} + \sum_{n=1}^{M_{k}} \sum_{k=1}^N \mbox{I}(\delta_{k,n,m} = 1) ,b_{\tau} + \sum_{n=1}^{M_{k}-1}\sum_{k=1}^{N} \sum_{ l \in A_m } dt_{k,n} \tau_{k}e^{(\theta_{k} + \beta) D_{ml}}\right)$

#+BEGIN_SRC cpp
  double post_a = a_kappa + 1.0;
  double post_b = b_kappa;
  for (auto &ii : oos_m) {
    if (status.at(ii)==1) post_a += 1.0;
    post_b += tdiff(ii) * tau(sid.at(ii)) * std::exp((theta(sid.at(ii)) + beta) * dist(ii));
  }
  kappa = stan::math::gamma_rng(post_a, post_b, rng);
#+END_SRC
*** update \(\tau_{k}\)
For each $k$, we draw \(\tau_k^{(t)}\) from
$\mbox{Gamma}\left(  a_{\tau} + M_k, b_{\tau} + \sum_{n=1}^{M_{k}} \sum_{m \in S, l \in A_m } dt_{k,n} \kappa_{m}e^{(\theta_{k} + \beta) D_{ml}}\right)$

#+BEGIN_SRC cpp
double post_a = a_tau + 1.0;
double post_b = b_tau;
for (auto &ii : person_k) {
    if (status.at(ii) == 1)
        post_a += 1.0;
    post_b +=
        tdiff(ii) * kappa(acfrom.at(ii)) * std::exp((theta_k + beta) * dist(ii));
}
tau = stan::math::gamma_rng(post_a, post_b, rng);
#+END_SRC
*** update \(\theta_{k}\)
For each $k$, we draw \(\theta_k^{* }\)$ from a symmetric MH jumping distribution, and accept $\theta_{m}^{(l)} = \theta_{m}^{* }$ with probability $\min(1, r_{{\theta_{m}}^{* }})$ where
\begin{align*}
\log r_{{\theta_{k}}^{* }} =& \sum_{m \in S} \sum_{n=1}^{M_{k}} \left[ \delta_{k,n,m} (\theta_{k}^{* } - \theta_{k}^{(l-1)})D_{ml} -\sum_{l \in A_m} dt_{k,n} \kappa_{m} \tau_{k} e^{ \beta D_{ml} }(e^{\theta_{k}^{* }D_{ml}} -  e^{\theta_{k}^{(l-1)} D_{ml} }) + \log \frac{\pi(\theta_{k}^{* })}{\pi(\theta_{k}^{(l-1)})} \right].\\
\end{align*}

*** update \(\sigma^{2}\)
We draw \((\sigma^{2})^{(t)}\) from
\[
 p( \sigma^2|e.e.) \propto \mbox{Inv-Gamma}(\sigma^{2}|a,b) \prod N(\theta_{k} | \mu, \sigma^2)
\]
\(\sigma^{2} \sim \mbox{Inv-Gamma}(a + 0.5 * N, b + 0.5 + \sum \theta_{k}^2)\)
c.f) with flat prior:
\(\sigma^{2} \sim inv-gamma(0.5 * N, 0.5 + \sum \theta_{k}^2)\)

$\mbox{Inv-Gamma}(\alpha,\beta)$ denotes the inverse gamma distribution with a density
\[
\mbox{Inv-Gamma}(y|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-(\alpha+1)} \exp \left(-\beta \frac{1}{y}\right).
\]

*** update \(\beta\) :noexport:ARCHIVE:
- a symmetric MH jumping $J(\beta_{k}^{(l-1)} \rightarrow \beta_{k}^{* })$ is used to propose a new sample.
- We accept $\beta_{k}^{(l)} = \beta_{k}^{* }$ with probability $\min(1, r_{{\beta_{k}}^{* )}})$ where
\begin{align*}
\log r_{{\beta_{k}}^{* }} =&
\sum \delta_{k,n,m} (\beta_{k}^{* } - \beta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \theta_k D_{ml} }(e^{\beta^{* }D_{ml}} -  e^{\beta^{(l-1)} D_{ml} })
+ \log \frac{\pi(\beta_{k}^{* })}{\pi(\beta_{k}^{(l-1)})}.
\end{align*}
*** update \(\mu_{\beta}, \sigma_{\beta}\) :noexport:ARCHIVE:
\begin{align*}
  \rho &= 1/\sigma_{\beta}^{2} + 1/\sigma_{\mu_{\beta}}^2 \\
  p(\mu_{\beta}|..)&= N(\frac{1/\sigma_{\beta}^2 \times \beta}{\rho}, 1/\rho )
\end{align*}
\[
  p(\sigma_{\beta}^{2}|ee) = inv-Gamma(a + 0.5, b + 0.5 (\beta - \mu_{\beta})^2)
\]
** Application
*** tickets
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: tickets
:END:

#+BEGIN_SRC sh
out_dir="tickets/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+END_SRC

#+BEGIN_SRC R
ftime = timestamp[1] / 1000, naction = n(), time = timestamp[n()] / 1000, spd = naction / (ftime - time)
#+END_SRC
- \tau: person's baseline hazard for action transition
- \theta: person's xxx to jump to a similar action for the next one
#+begin_export html
|Name        |Label                                                                               |Value scheme                                          |
|:-----------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------|
|AGEG5LFS    |Age groups in 5-year intervals based on LFS groupings (derived)                     |Derived - Age groups in equal 5 year intervals (1-10) |
|NFEHRS      |Number of hours of participation in non-formal education (derived)                  |NA                                                    |
|EARNHRDCL   |Hourly earnings excluding bonuses for wage and salary earners, in deciles (derived) |Derived - Decile                                      |
|LEARNATWORK |Index of learning at work (derived)                                                 |NA                                                    |
|ICTHOME     |Index of use of ICT skills at home (derived)                                        |NA                                                    |
|ICTWORK     |Index of use of ICT skills at work (derived)                                        |NA                                                    |
|INFLUENCE   |Index of use of influencing skills at work (derived)                                |NA                                                    |
|NUMHOME     |Index of use of numeracy skills at home (basic and advanced - derived)              |NA                                                    |
|NUMWORK     |Index of use of numeracy skills at work (basic and advanced - derived)              |NA                                                    |
|READHOME    |Index of use of reading skills at home (prose and document texts - derived)         |NA                                                    |
|READWORK    |Index of use of reading skills at work (prose and document texts - derived)         |NA                                                    |
|TASKDISC    |Index of use of task discretion at work (derived)                                   |NA                                                    |
|WRITHOME    |Index of use of writing skills at home (derived)                                    |NA                                                    |
|WRITWORK    |Index of use of writing skills at work (derived)                                    |NA                                                    |
#+end_export

[[file:tickets/figure/lpa_plot-0.png]]
[[file:tickets/figure/lpa_plot-1.png]]
# [[file:tickets/figure/lpa_back.png]]
[[file:tickets/figure/lpa_back_line.png]]

Response: the smaller, the better
#+RESULTS:
#+begin_export html
## mean


|        tau|      theta|    naction|        spd|      res|
|----------:|----------:|----------:|----------:|--------:|
| -0.4123013|  0.1939430|  0.0105633|  0.2949513| 3.710588|
|  0.9344900| -0.6655596|  0.3971435|  0.1263112| 5.219081|
|  1.1296467| -1.7866636| -1.9230219| -1.9932064| 7.000000|
| -0.9718218|  1.3403763|  0.1527688| -0.0648598| 3.145251|

## sd


|       tau|    theta|   naction|       spd|      res|
|---------:|--------:|---------:|---------:|--------:|
| 0.4111353| 0.384638| 0.6375695| 0.2628682| 2.989527|
| 0.9065238| 0.553052| 1.2095532| 0.3520752| 2.745995|
| 0.9047816| 1.085768| 0.1671354| 2.6734464| 0.000000|
| 0.2688267| 0.354953| 0.5181867| 0.3940398| 2.883724|

## n


| tau| theta| naction| spd| res|
|---:|-----:|-------:|---:|---:|
| 425|   425|     425| 425| 425|
| 283|   283|     283| 283| 283|
|  75|    75|      75|  75|  75|
| 179|   179|     179| 179| 179|
#+end_export


\tau's covaritates:
\theta's covaritates:
[[file:tickets/figure/theta_tau_res.png]]
[[file:tickets/figure/tau_action.png]]
[[file:tickets/figure/time_action-3.png]]
[[file:tickets/figure/time_action_more-2.png]]
[[file:tickets/figure/time_action_more-5.png]]
[[file:tickets/figure/time_action_more-7.png]]
[[file:tickets/figure/time_action_more-8.png]]
[[file:tickets/figure/time_action_more-9.png]]
[[file:tickets/figure/time_action_more-10.png]]
[[file:tickets/figure/time_action_more-11.png]]
[[file:tickets/figure/time_action_more-13.png]]

*** party_invitations-1
[[file:party_invitations-1/figure/lpa_plot-0.png]]
[[file:party_invitations-1/figure/lpa_plot-1.png]]
# [[file:party_invitations-1/figure/lpa_back.png]]
[[file:party_invitations-1/figure/lpa_back_line.png]]

Response: the larger, the better

#+RESULTS:
#+begin_export html

### clustering w/ tau and theta


|          tau|        theta|      naction|           spd|         res|             n|
|------------:|------------:|------------:|-------------:|-----------:|-------------:|
|  2.06 (1.10)| -1.49 (0.96)|  0.08 (3.72)| -4.43 (11.77)| 0.43 (1.13)|   7.00 (0.00)|
|  0.02 (0.65)|  0.63 (0.43)| -0.21 (0.38)|   0.03 (0.00)| 2.77 (0.56)| 443.00 (0.00)|
| -0.65 (0.57)| -0.58 (1.11)| -0.28 (0.70)|   0.03 (0.00)| 1.18 (1.32)| 309.00 (0.00)|
|  0.83 (1.33)| -0.43 (0.89)|  0.84 (1.51)|   0.03 (0.00)| 1.90 (1.24)| 211.00 (0.00)|

### clustering w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
|  1.82 (4.16)| -3.09 (9.85)| 0.98 (0.05)| 0.02 (0.05)| 0.70 (1.25)|  10.00 (0.00)|
| -0.02 (0.90)|  0.03 (0.00)| 0.00 (0.01)| 1.00 (0.01)| 2.07 (1.23)| 960.00 (0.00)|
#+end_export

*** book_order

[[file:book_order/figure/lpa_plot-0.png]]
[[file:book_order/figure/lpa_plot-1.png]]
# [[file:book_order/figure/lpa_back.png]]
[[file:book_order/figure/lpa_back_line.png]]

Response: the larger, the better
#+RESULTS:
#+begin_export html

### w/ tau and theta


|          tau|        theta|      naction|          spd|         res|             n|
|------------:|------------:|------------:|------------:|-----------:|-------------:|
| -0.18 (0.88)|  0.37 (0.49)|  0.21 (0.68)|  0.11 (0.42)| 2.27 (2.45)| 450.00 (0.00)|
|  0.83 (1.06)| -1.67 (0.83)| -1.34 (0.12)| -0.25 (0.80)| 6.32 (1.92)|  88.00 (0.00)|
|  0.31 (1.32)| -0.79 (1.73)|  0.95 (2.48)| -0.96 (3.97)| 4.23 (3.05)|  26.00 (0.00)|

### w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
| -0.04 (0.83)|  0.08 (0.45)| 0.99 (0.03)| 0.01 (0.03)| 2.91 (2.80)| 535.00 (0.00)|
|  0.73 (2.54)| -1.45 (3.73)| 0.11 (0.16)| 0.89 (0.16)| 4.52 (3.01)|  29.00 (0.00)|
#+end_export

** References
#+csl_style: ~/Zotero/styles/chicago-author-date.csl
#+latex: \printbibliography
# for html export with bib

#+bibliography: ~/Zotero/myref.bib
#+bibliography: here

bibliographystyle:unsrt
bibliography:~/Zotero/myref.bib

* COMMENT Local Variables
# Local Variables:
# eval: (jyun/set-org-babel-default-header-args:R)
# eval: (flyspell-mode -1)
# eval: (spell-fu-mode -1)
# End:
