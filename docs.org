#+title: PIACC data processing

#+hugo_base_dir: ./docs
#+hugo_front_matter_format: toml
#+hugo_level_offset: 0

# https://orgmode.org/manual/Export-Settings.html#Export-Settings
#+options: H:10 num:nil toc:t \n:nil @:t ::t |:t ^:nil ^:{} -:t f:t *:t <:t ':nil -:nil pri:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:nil

#+startup: overview noinlineimages logdone indent

#+latex_class: article
#+latex_class_options: [letterpaper,11pt]

#+latex_compiler: pdflatex

# comment out for reveal.js
# #+setupfile: ~/setup/my-theme-readtheorg.setup
#+setupfile: ~/org/latex_header.setup
#+setupfile: ~/org/orgmode_header.setup

#+property: header-args :eval never-export
#+property: header-args:R :eval never-export
#+property: header-args:ein :session localhost
#+property: header-args:jupyter-python :session *jupyter-piacc* :kernel tf

#+BEGIN_SRC emacs-lisp
  (jyun/org-latex-set-options 2.5)
#+END_SRC

* Home :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: ./
:EXPORT_TITLE: Process data modeling
:END:
#+begin_export html
# WIP: Process data modeling
To be shared during meetings.
#+end_export
* TODOs :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: todo
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 1
:EXPORT_TITLE: TODO
:END:

** pattern discovery
- divide tau by 1) number of actions
- use 2) total time for visualization
- 1) divided by 2)
- find persons (small # of actions, large # of actions: right vs wrong)
  - 시간이 많이 걸리고 맞은사람 vs 적게 걸리고 맞은 사람.
  - 적은 액션으로 맞은 사람 vs 많은 액션으로 틀린 사람.
- 무조건 빨리 푼다고 잘하는게 아니고, 느리거나 혹은 상이한 액션 개수로 정답에 이르는 프로세스 발견에 초점.
** justification + what item to use
select a few items fulfilling the justification sheme!

https://cran.r-project.org/web/packages/tidyLPA/vignettes/Introduction_to_tidyLPA.html
분화된 그룹 (더 많은 그룹) 이 있으면 OK

observed covaritates + response group classification
observed covaritates + tau and theta + response group classification

*** no gender effect?
Suppose a gender variable was significant RF for tau and theta.
this could be because of effect of other covariates
plot gender against tau or theta, and see if there was diff.
after control gender no significant
find gender diff in EDA, after age control no difference.

https://data-edu.github.io/tidyLPA/reference/AHP.html

Some items show the gender difference. However, they are not chosen ones for the further analysis.
** cluster analysis
characteristics of newly discovered groups using hidden traits
what these groups can tell you about???

** model validation
see if we can use model validation tech for mutil-state survival model.
See [[id:1237b9e6-288a-413a-a484-e535d817a3c1][multi-state survival model validation]]: posterior predictive checking

* PIACC :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: piacc
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 1
:EXPORT_TITLE: PIACC
:END:

#+begin_src toml :front_matter_extra t :exports none
pre = "<b>1. </b>"
#+end_src

#+begin_export html
### Chapter 1

# PIACC
#+end_export

Programme for the International Assessment of Adult Competencies (PIAAC)

** item home
:PROPERTIES:
:header-args:R: :tangle R/itemcode.R :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: item_home
:END:

See the codebook for details....... [[file:data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv][PS_BOOKLET_ITEM.csv]]

#+begin_src R
booklet = readr::read_csv("~/Dropbox/research/procmod/procmod-code/data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv")
booklet$NAME = stringr::str_replace_all(booklet$NAME, " ", "_")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "_-", "-")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "-_", "-")
booklet$NAME = tolower(booklet$NAME)
#+end_src

*** party_invitations-1 :ATTACH:
:PROPERTIES:
:ID:       67fea2ad-8240-45dc-a4eb-da07f34a9e45
:END:
#+begin_src R :results value list drawer silent :tangle no
if (item_code == "U01a000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01a_(.*)","\\1\\2"), c("id=u01a_",  ""), c("\\*\\$target=",  ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+begin_src R
if (item_code == "U01a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u01a_(.*)","\\1\\2"),
  c("TEST_TIME=(.+)",""),
  c("id=u01a_",  ""),
  c("id=",""),
  c("\\*\\$target=_", "target="),
  c("\\*\\$value=", "value="),
  c("\\*\\$index=", "index="),
  c("\\*\\$href=", "href="),
  c("\\*\\$", ""),
  c("\"", ""),
  c("\\|","\\."))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_171956pi-start.png]]

*** party_invitations-2 :ATTACH:
:PROPERTIES:
:ID:       da5e5603-defc-43fb-a01c-16bfb293c5d8
:END:
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)

if (item_code == "U01b000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01b_(.*)","\\1\\2"), c("id=u01b_",  ""), c("\\*\\$target=",  ""), c("\\*\\$value=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172034pi-home.png]]

*** cd_tally :ATTACH:
:PROPERTIES:
:ID:       0cfd0f21-14b5-4993-a006-d6c13f9027ef
:END:
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)
if (item_code == "U03a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172043cd-start.png]]

*** sprained_ankle-1 :ATTACH:
:PROPERTIES:
:ID:       9c54e9eb-4153-4770-8b4c-0616c15bcebe
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U06a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

[[attachment:_20210904_172056sa-start.png]]

*** sprained_ankle-2 :ATTACH:
:PROPERTIES:
:ID:       c9b8890f-d9d6-4aa1-9505-48ed98d4f657
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U06b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172105sa-start2.png]]

*** tickets :ATTACH:
:PROPERTIES:
:ID:       47fa0d97-e0ab-44e8-87c7-dc6f6946f786
:END:
#+begin_src R :results value list drawer silent :tangle no
if (item_code == "U21x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+begin_src R
if (item_code == "U21x000S") {
sub_str = rbind(
  ## c("(.*)\\*\\$target=u021_(.*)","\\1\\2"),
  c("TEST_TIME=(.+)",""),
  c("id=u021_", ""),
  c("id=",""),
  c("\\*\\$target=_", "target="),
  c("\\*\\$value=", "value="),
  c("\\*\\$index=", "index="),
  c("\\*\\$href=", "href="),
  c("\\*\\$", ""),
  c("\"", ""),
  c("\\|","\\.")
  )
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172128ft-home.png]]

*** class_attendance :ATTACH:
:PROPERTIES:
:ID:       994a4a41-2040-4276-bc5f-f67966d69bde
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U04a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172140ca-email-home.png]]

*** club_membership-1 :ATTACH:
:PROPERTIES:
:ID:       50a5f4a2-d333-4bb2-8639-9a0dbf3d7a09
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U19a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172148cm-start.png]]

*** club_membership-2 :ATTACH:
:PROPERTIES:
:ID:       549f8614-8703-4373-b745-b44e3a7b84e3
:END:
#+begin_src R
if (item_code == "U19b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172157cm-home.png]]

*** book_order :ATTACH:
:PROPERTIES:
:ID:       fcf87b1e-7433-422d-9ce4-3770e71cbb6a
:END:
#+begin_src R :tangle no
if (item_code == "U07x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+begin_src R
if (item_code == "U07x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u07_(.*)","\\1\\2"),
  c("TEST_TIME=(.+)",""),
  c("id=u07_",  ""),
  c("id=",""),
  c("\\*\\$target=_", "target="),
  c("\\*\\$value=", "value="),
  c("\\*\\$index=", "index="),
  c("\\*\\$href=", "href="),
  c("\\*\\$", ""),
  c("\"", ""),
  c("\\|","\\."))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}

#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172208bo-start.png]]

*** meeting_room :ATTACH:
:PROPERTIES:
:ID:       d9736569-11b6-4084-94a8-9782473453e1
:END:
#+begin_src R
if (item_code == "U16x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172218mr-email-home.png]]

*** reply_all :ATTACH:
:PROPERTIES:
:ID:       265dfcf6-ec9b-483e-b82b-8745a606ad0d
:END:
#+begin_src R
if (item_code == "U02x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172228ra-home.png]]

*** locate_email :ATTACH:
:PROPERTIES:
:ID:       ccbb321c-a463-47ba-bcf7-ac40fe1e8ff6
:END:
#+begin_src R
if (item_code == "U11b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172236le-home.png]]

*** lamp_return :ATTACH:
:PROPERTIES:
:ID:       e0aef17d-55dd-4641-a529-f63b2acd9d92
:END:
#+begin_src R
if (item_code == "U23x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172248lr-home.png]]

** item codebook
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: item_codebook
:END:

| ITEM  | NAME                  | CODEBOOK | RESPONSE                                                                 |
|-------+-----------------------+----------+--------------------------------------------------------------------------|
| PS1_1 | Party Invitations - 1 | U01a000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS1_2 | Party Invitations - 2 | U01b000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_3 | CD Tally              | U03a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_4 | Sprained Ankle - 1    | U06a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_5 | Sprained Ankle - 2    | U06b000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_6 | Tickets               | U21x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_7 | Class Attendance      | U04a000S | POLYTOMOUS (0 to 2), No response (R ), Not reached / not attempted (N)   |
| PS2_1 | Club Membership - 1   | U19a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_2 | Club Membership - 2   | U19b000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_3 | Book Order            | U07x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_4 | Meeting Room          | U02x000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_5 | Reply All             | U16x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_6 | Locate Email          | U11b000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_7 | Lamp Return           | U23x000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
** item results
:PROPERTIES:
:header-args: :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: item_results
:END:
*** important background covaraites
Random forest regression is used with \tau or \theta as a response and background variables in some categoires as covariates. P-value for the importance is reported. The selected categories are
c("Sampling / weighting", "Not assigned" ,"Sampling / weighting (derived)", "Background questionnaire (trend)"  ,"Background questionnaire", "Background questionnaire (derived)"

*** variables
- ~ftime~: time until the first action taken
- ~time~: total time of a person's process
- ~naction~ or ~#action~: the number of actions of a person's process

*** summaries
- Total or first action time has weak association with \theta.
- # of actions and first action time have weak or no assciation.
- # of actions has some association with the response.
- \tau and first action time has negative correaltion.
- \tau and # of actions have weak or no association
- # of actions and \theta time have positive correlation.
*** LPA
Latent profile analysis is done by =tidyLAP=. This R package uses multivariate Gaussian mixture models and reports many model selection criteria. See https://cran.r-project.org/web/packages/tidyLPA/vignettes/Introduction_to_tidyLPA.html
The clustering is perfomed based on person characteristics. We compare models based on our estimated parameters, observed covariates and both.

party_invitations-1, tickets, book_order: these items give more clusters using hidden traits + observed traits than using observed traits alone.
*** party_invitations-1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: party_invitations-1
:END:
#+begin_src sh :async
out_dir="party_invitations-1/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src

**** Covariates
\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-1/tau_imp.txt")
    (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-1/theta_imp.txt")
    (buffer-string))
#+end_src

[[file:party_invitations-1/figure/theta_tau_res.png]]
[[file:party_invitations-1/figure/tau_action.png]]
[[file:party_invitations-1/figure/time_action-3.png]]
[[file:party_invitations-1/figure/time_action_more-2.png]]
[[file:party_invitations-1/figure/time_action_more-5.png]]
[[file:party_invitations-1/figure/time_action_more-7.png]]
[[file:party_invitations-1/figure/time_action_more-8.png]]
[[file:party_invitations-1/figure/time_action_more-9.png]]
[[file:party_invitations-1/figure/time_action_more-10.png]]
[[file:party_invitations-1/figure/time_action_more-11.png]]
[[file:party_invitations-1/figure/time_action_more-13.png]]

*** party_invitations-2
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: party_invitations-2
:END:

#+begin_src sh :async
out_dir="party_invitations-2/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-2/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "party_invitations-2/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:party_inviations-2/figure/theta_tau_res.png]]
[[file:party_inviations-2/figure/tau_action.png]]
[[file:party_inviations-2/figure/time_action-3.png]]
[[file:party_inviations-2/figure/time_action_more-2.png]]
[[file:party_inviations-2/figure/time_action_more-5.png]]
[[file:party_inviations-2/figure/time_action_more-7.png]]
[[file:party_inviations-2/figure/time_action_more-8.png]]
[[file:party_inviations-2/figure/time_action_more-9.png]]
[[file:party_inviations-2/figure/time_action_more-10.png]]
[[file:party_inviations-2/figure/time_action_more-11.png]]
[[file:party_inviations-2/figure/time_action_more-13.png]]

*** cd_tally
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: cd_tally
:END:

#+begin_src sh :async
out_dir="cd_tally/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "cd_tally/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "cd_tally/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:cd_tally/figure/theta_tau_res.png]]
[[file:cd_tally/figure/tau_action.png]]
[[file:cd_tally/figure/time_action-3.png]]
[[file:cd_tally/figure/time_action_more-2.png]]
[[file:cd_tally/figure/time_action_more-5.png]]
[[file:cd_tally/figure/time_action_more-7.png]]
[[file:cd_tally/figure/time_action_more-8.png]]
[[file:cd_tally/figure/time_action_more-9.png]]
[[file:cd_tally/figure/time_action_more-10.png]]
[[file:cd_tally/figure/time_action_more-11.png]]
[[file:cd_tally/figure/time_action_more-13.png]]

*** sprained_ankle-1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: sprained_ankle-1
:END:
:#+begin_src sh :async
out_dir="sprained_ankle-1/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates
\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "sprained_ankle-1/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "sprained_ankle-1/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:sprained_ankle-1/figure/theta_tau_res.png]]
[[file:sprained_ankle-1/figure/tau_action.png]]
[[file:sprained_ankle-1/figure/time_action-3.png]]
[[file:sprained_ankle-1/figure/time_action_more-2.png]]
[[file:sprained_ankle-1/figure/time_action_more-5.png]]
[[file:sprained_ankle-1/figure/time_action_more-7.png]]
[[file:sprained_ankle-1/figure/time_action_more-8.png]]
[[file:sprained_ankle-1/figure/time_action_more-9.png]]
[[file:sprained_ankle-1/figure/time_action_more-10.png]]
[[file:sprained_ankle-1/figure/time_action_more-11.png]]
[[file:sprained_ankle-1/figure/time_action_more-13.png]]

*** sprained_ankle-2 :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: sprained_ankle-2
END:

+begin_src sh :async
ut_dir="sprained_ankle-2/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "sprained_ankle-2/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "sprained_ankle-2/theta_imp.txt")
   (buffer-string))
+end_src

[file:sprained_ankle-2/figure/theta_tau_res.png]]
[file:sprained_ankle-2/figure/tau_action.png]]
[file:sprained_ankle-2/figure/time_action-3.png]]
[file:sprained_ankle-2/figure/time_action_more-2.png]]
[file:sprained_ankle-2/figure/time_action_more-5.png]]
[file:sprained_ankle-2/figure/time_action_more-7.png]]
[file:sprained_ankle-2/figure/time_action_more-8.png]]
[file:sprained_ankle-2/figure/time_action_more-9.png]]
[file:sprained_ankle-2/figure/time_action_more-10.png]]
[file:sprained_ankle-2/figure/time_action_more-11.png]]
[file:sprained_ankle-2/figure/time_action_more-13.png]]

*** tickets
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: tickets
:END:

#+begin_src sh :async
out_dir="tickets/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "tickets/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "tickets/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:tickets/figure/theta_tau_res.png]]
[[file:tickets/figure/tau_action.png]]
[[file:tickets/figure/time_action-3.png]]
[[file:tickets/figure/time_action_more-2.png]]
[[file:tickets/figure/time_action_more-5.png]]
[[file:tickets/figure/time_action_more-7.png]]
[[file:tickets/figure/time_action_more-8.png]]
[[file:tickets/figure/time_action_more-9.png]]
[[file:tickets/figure/time_action_more-10.png]]
[[file:tickets/figure/time_action_more-11.png]]
[[file:tickets/figure/time_action_more-13.png]]

*** class_attendance :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: class_attendance
END:

+begin_src sh :async
ut_dir="class_attendance/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src
**** LPA
#+begin_src emacs-lisp :exports results :results html :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "class_attendance/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "class_attendance/theta_imp.txt")
   (buffer-string))
+end_src

[file:class_attendance/figure/theta_tau_res.png]]
[file:class_attendance/figure/tau_action.png]]
[file:class_attendance/figure/time_action-3.png]]
[file:class_attendance/figure/time_action_more-2.png]]
[file:class_attendance/figure/time_action_more-5.png]]
[file:class_attendance/figure/time_action_more-7.png]]
[file:class_attendance/figure/time_action_more-8.png]]
[file:class_attendance/figure/time_action_more-9.png]]
[file:class_attendance/figure/time_action_more-10.png]]
[file:class_attendance/figure/time_action_more-11.png]]
[file:class_attendance/figure/time_action_more-13.png]]

*** club_membership-1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: club_membership-1
:END:

#+begin_src sh :async
out_dir="club_membership-1/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates
\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "club_membership-1/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "club_membership-1/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:club_membership-1/figure/theta_tau_res.png]]
[[file:club_membership-1/figure/tau_action.png]]
[[file:club_membership-1/figure/time_action-3.png]]
[[file:club_membership-1/figure/time_action_more-2.png]]
[[file:club_membership-1/figure/time_action_more-5.png]]
[[file:club_membership-1/figure/time_action_more-7.png]]
[[file:club_membership-1/figure/time_action_more-8.png]]
[[file:club_membership-1/figure/time_action_more-9.png]]
[[file:club_membership-1/figure/time_action_more-10.png]]
[[file:club_membership-1/figure/time_action_more-11.png]]
[[file:club_membership-1/figure/time_action_more-13.png]]

*** club_membership-2 :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: club_membership-2
END:

+begin_src sh
ut_dir="club_membership-2/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src

#+begin_src emacs-lisp :exports results :results html :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "club_membership-2/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "club_membership-2/theta_imp.txt")
   (buffer-string))
+end_src

[file:club_membership-2/figure/theta_tau_res.png]]
[file:club_membership-2/figure/tau_action.png]]
[file:club_membership-2/figure/time_action-3.png]]
[file:club_membership-2/figure/time_action_more-2.png]]
[file:club_membership-2/figure/time_action_more-5.png]]
[file:club_membership-2/figure/time_action_more-7.png]]
[file:club_membership-2/figure/time_action_more-8.png]]
[file:club_membership-2/figure/time_action_more-9.png]]
[file:club_membership-2/figure/time_action_more-10.png]]
[file:club_membership-2/figure/time_action_more-11.png]]
[file:club_membership-2/figure/time_action_more-13.png]]

*** book_order
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: book_order
:END:

#+begin_src sh :async
out_dir="book_order/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "book_order/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "book_order/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:book_order/figure/theta_tau_res.png]]
[[file:book_order/figure/tau_action.png]]
[[file:book_order/figure/time_action-3.png]]
[[file:book_order/figure/time_action_more-2.png]]
[[file:book_order/figure/time_action_more-5.png]]
[[file:book_order/figure/time_action_more-7.png]]
[[file:book_order/figure/time_action_more-8.png]]
[[file:book_order/figure/time_action_more-9.png]]
[[file:book_order/figure/time_action_more-10.png]]
[[file:book_order/figure/time_action_more-11.png]]
[[file:book_order/figure/time_action_more-13.png]]

*** meeting_room
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: meeting_room
:END:

#+begin_src sh :async
out_dir="meeting_room/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "meeting_room/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "meeting_room/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:meeting_room/figure/theta_tau_res.png]]
[[file:meeting_room/figure/tau_action.png]]
[[file:meeting_room/figure/time_action-3.png]]
[[file:meeting_room/figure/time_action_more-2.png]]
[[file:meeting_room/figure/time_action_more-5.png]]
[[file:meeting_room/figure/time_action_more-7.png]]
[[file:meeting_room/figure/time_action_more-8.png]]
[[file:meeting_room/figure/time_action_more-9.png]]
[[file:meeting_room/figure/time_action_more-10.png]]
[[file:meeting_room/figure/time_action_more-11.png]]
[[file:meeting_room/figure/time_action_more-13.png]]

*** reply_all :noexport:ARCHIVE:
PROPERTIES:
EXPORT_FILE_NAME: _index
EXPORT_HUGO_BUNDLE: reply_all
END:

+begin_src sh :async
ut_dir="reply_all/"
d $out_dir
d figure
onvert -density 300 theta_tau_res.pdf theta_tau_res.png
onvert -density 300 time_action_more.pdf time_action_more-%d.png
onvert -density 300 time_action.pdf time_action-%d.png
+end_src
**** LPA
#+begin_src emacs-lisp :exports results :results html :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "reply_all/tau_imp.txt")
   (buffer-string))
+end_src

's covaritates:
+begin_src emacs-lisp :exports results :results html
with-temp-buffer
insert-file-contents "reply_all/theta_imp.txt")
   (buffer-string))
+end_src

[file:reply_all/figure/theta_tau_res.png]]
[file:reply_all/figure/tau_action.png]]
[file:reply_all/figure/time_action-3.png]]
[file:reply_all/figure/time_action_more-2.png]]
[file:reply_all/figure/time_action_more-5.png]]
[file:reply_all/figure/time_action_more-7.png]]
[file:reply_all/figure/time_action_more-8.png]]
[file:reply_all/figure/time_action_more-9.png]]
[file:reply_all/figure/time_action_more-10.png]]
[file:reply_all/figure/time_action_more-11.png]]
[file:reply_all/figure/time_action_more-13.png]]

*** locate_email
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: locate_email
:END:
:#+begin_src sh :async
out_dir="locate_email/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports none :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "locate_email/tau_imp.txt")
   (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "locate_email/theta_imp.txt")
   (buffer-string))
#+end_src

[[file:locate_email/figure/theta_tau_res.png]]
[[file:locate_email/figure/tau_action.png]]
[[file:locate_email/figure/time_action-3.png]]
[[file:locate_email/figure/time_action_more-2.png]]
[[file:locate_email/figure/time_action_more-5.png]]
[[file:locate_email/figure/time_action_more-7.png]]
[[file:locate_email/figure/time_action_more-8.png]]
[[[file:locate_email/figure/time_action_more-9.png]]
[[file:locate_email/figure/time_action_more-10.png]]
[[file:locate_email/figure/time_action_more-11.png]]
[[file:locate_email/figure/time_action_more-13.png]]

*** lamp_return
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: lamp_return
:END:

#+begin_src sh :async
out_dir="lamp_return/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+begin_src emacs-lisp :exports results :var out_dir=(jyun/get-heading)
(with-temp-buffer
(insert-file-contents (format "%s/lpa_mods.txt" out_dir))
    (buffer-string))
#+end_src
**** Covariates

\tau's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "lamp_return/tau_imp.txt")
    (buffer-string))
#+end_src

\theta's covaritates:
#+begin_src emacs-lisp :exports results :results html
(with-temp-buffer
(insert-file-contents "lamp_return/theta_imp.txt")
    (buffer-string))
#+end_src

[[file:lamp_return/figure/theta_tau_res.png]]
[[file:lamp_return/figure/tau_action.png]]
[[file:lamp_return/figure/time_action-3.png]]
[[file:lamp_return/figure/time_action_more-2.png]]
[[file:lamp_return/figure/time_action_more-5.png]]
[[file:lamp_return/figure/time_action_more-7.png]]
[[file:lamp_return/figure/time_action_more-8.png]]
[[file:lamp_return/figure/time_action_more-9.png]]
[[file:lamp_return/figure/time_action_more-10.png]]
[[file:lamp_return/figure/time_action_more-11.png]]
[[file:lamp_return/figure/time_action_more-13.png]]

** gender difference
:PROPERTIES:
:header-args: :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: gender_difference
:END:

Wilcoxon p-values for gender difference in \tau and \theta
| output                 |          \tau |          \theta |
|------------------------+------------+------------|
| party_invitations-1/ * |  0.1755171 |  0.3172373 |
| party_invitations-2/   |  0.4404884 |  0.6082382 |
| cd_tally/              | 0.07097494 | 0.09468705 |
| sprained_ankle-1/      | 0.01260001 |  0.6150152 |
| tickets/ *             | 0.08539706 |   0.730782 |
| club_membership-1/     |  0.3772467 |  0.1507705 |
| book_order/ *          |  0.9709181 |  0.3778057 |
| meeting_room/          |  0.4096292 | 0.04091108 |
| locate_email/          |   0.991704 |  0.7721417 |
| lamp_return/           | 0.03020509 |    0.86869 |

=*= is used to denote items justified by LPA.
** clustering
:PROPERTIES:
:header-args: :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: clustering
:END:
#+begin_src R :exports code
ftime = timestamp[1] / 1000, naction = n(), time = timestamp[n()] / 1000, spd = naction / (ftime - time)
#+end_src
- \tau: person's baseline hazard for action transition
- \theta: person's xxx to jump to a similar action for the next one
#+begin_export html
|Name        |Label                                                                               |Value scheme                                          |
|:-----------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------|
|AGEG5LFS    |Age groups in 5-year intervals based on LFS groupings (derived)                     |Derived - Age groups in equal 5 year intervals (1-10) |
|NFEHRS      |Number of hours of participation in non-formal education (derived)                  |NA                                                    |
|EARNHRDCL   |Hourly earnings excluding bonuses for wage and salary earners, in deciles (derived) |Derived - Decile                                      |
|LEARNATWORK |Index of learning at work (derived)                                                 |NA                                                    |
|ICTHOME     |Index of use of ICT skills at home (derived)                                        |NA                                                    |
|ICTWORK     |Index of use of ICT skills at work (derived)                                        |NA                                                    |
|INFLUENCE   |Index of use of influencing skills at work (derived)                                |NA                                                    |
|NUMHOME     |Index of use of numeracy skills at home (basic and advanced - derived)              |NA                                                    |
|NUMWORK     |Index of use of numeracy skills at work (basic and advanced - derived)              |NA                                                    |
|READHOME    |Index of use of reading skills at home (prose and document texts - derived)         |NA                                                    |
|READWORK    |Index of use of reading skills at work (prose and document texts - derived)         |NA                                                    |
|TASKDISC    |Index of use of task discretion at work (derived)                                   |NA                                                    |
|WRITHOME    |Index of use of writing skills at home (derived)                                    |NA                                                    |
|WRITWORK    |Index of use of writing skills at work (derived)                                    |NA                                                    |
#+end_export
*** party_invitations-1
#+begin_src sh :exports none :var out_dir=(jyun/get-heading)
cd $out_dir
cd figure
convert -density 300 lpa_plot.pdf lpa_plot-%d.png
# convert -density 300 lpa_back.pdf lpa_back.png
convert -density 300 lpa_back_line.pdf lpa_back_line.png
#+end_src

#+RESULTS:

[[file:party_invitations-1/figure/lpa_plot-0.png]]
[[file:party_invitations-1/figure/lpa_plot-1.png]]
# [[file:party_invitations-1/figure/lpa_back.png]]
[[file:party_invitations-1/figure/lpa_back_line.png]]

Response: the larger, the better

#+begin_src R :exports results :var ob_out_dir=(jyun/get-heading) :results output html
library(diprom)
load(paste0(ob_out_dir, "/summaryw_res.RData"))
cat("\n### w/ tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
load(paste0(ob_out_dir, "/mod3_summaryw_res.RData"))
cat("\n### w/o tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
#+end_src

#+RESULTS:
#+begin_export html

### clustering w/ tau and theta


|          tau|        theta|      naction|           spd|         res|             n|
|------------:|------------:|------------:|-------------:|-----------:|-------------:|
|  2.06 (1.10)| -1.49 (0.96)|  0.08 (3.72)| -4.43 (11.77)| 0.43 (1.13)|   7.00 (0.00)|
|  0.02 (0.65)|  0.63 (0.43)| -0.21 (0.38)|   0.03 (0.00)| 2.77 (0.56)| 443.00 (0.00)|
| -0.65 (0.57)| -0.58 (1.11)| -0.28 (0.70)|   0.03 (0.00)| 1.18 (1.32)| 309.00 (0.00)|
|  0.83 (1.33)| -0.43 (0.89)|  0.84 (1.51)|   0.03 (0.00)| 1.90 (1.24)| 211.00 (0.00)|

### clustering w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
|  1.82 (4.16)| -3.09 (9.85)| 0.98 (0.05)| 0.02 (0.05)| 0.70 (1.25)|  10.00 (0.00)|
| -0.02 (0.90)|  0.03 (0.00)| 0.00 (0.01)| 1.00 (0.01)| 2.07 (1.23)| 960.00 (0.00)|
#+end_export

*** tickets

#+begin_src sh :async :exports none :var out_dir=(jyun/get-heading)
cd $out_dir
cd figure
convert -density 300 lpa_plot.pdf lpa_plot-%d.png
# convert -density 300 lpa_back.pdf lpa_back.png
convert -density 300 lpa_back_line.pdf lpa_back_line.png
#+end_src

#+RESULTS:

[[file:tickets/figure/lpa_plot-0.png]]
[[file:tickets/figure/lpa_plot-1.png]]
# [[file:tickets/figure/lpa_back.png]]
[[file:tickets/figure/lpa_back_line.png]]

Response: the smaller, the better
#+begin_src R :exports results :var ob_out_dir=(jyun/get-heading) :results output html
library(diprom)
load(paste0(ob_out_dir, "/summaryw_res.RData"))
cat("\n### w/ tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
load(paste0(ob_out_dir, "/mod3_summaryw_res.RData"))
cat("\n### w/o tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
#+end_src

#+RESULTS:
#+begin_export html
## mean


|        tau|      theta|    naction|        spd|      res|
|----------:|----------:|----------:|----------:|--------:|
| -0.4123013|  0.1939430|  0.0105633|  0.2949513| 3.710588|
|  0.9344900| -0.6655596|  0.3971435|  0.1263112| 5.219081|
|  1.1296467| -1.7866636| -1.9230219| -1.9932064| 7.000000|
| -0.9718218|  1.3403763|  0.1527688| -0.0648598| 3.145251|

## sd


|       tau|    theta|   naction|       spd|      res|
|---------:|--------:|---------:|---------:|--------:|
| 0.4111353| 0.384638| 0.6375695| 0.2628682| 2.989527|
| 0.9065238| 0.553052| 1.2095532| 0.3520752| 2.745995|
| 0.9047816| 1.085768| 0.1671354| 2.6734464| 0.000000|
| 0.2688267| 0.354953| 0.5181867| 0.3940398| 2.883724|

## n


| tau| theta| naction| spd| res|
|---:|-----:|-------:|---:|---:|
| 425|   425|     425| 425| 425|
| 283|   283|     283| 283| 283|
|  75|    75|      75|  75|  75|
| 179|   179|     179| 179| 179|
#+end_export

*** book_order

#+begin_src sh :async :exports none :var out_dir=(jyun/get-heading)
cd $out_dir
cd figure
convert -density 300 lpa_plot.pdf lpa_plot-%d.png
# convert -density 300 lpa_back.pdf lpa_back.png
convert -density 300 lpa_back_line.pdf lpa_back_line.png
#+end_src

#+RESULTS:

[[file:book_order/figure/lpa_plot-0.png]]
[[file:book_order/figure/lpa_plot-1.png]]
# [[file:book_order/figure/lpa_back.png]]
[[file:book_order/figure/lpa_back_line.png]]

Response: the larger, the better
#+begin_src R :exports results :var ob_out_dir=(jyun/get-heading) :results output html
library(diprom)
load(paste0(ob_out_dir, "/summaryw_res.RData"))
cat("\n### w/ tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
load(paste0(ob_out_dir, "/mod3_summaryw_res.RData"))
cat("\n### w/o tau and theta\n")
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
#+end_src

#+RESULTS:
#+begin_export html

### w/ tau and theta


|          tau|        theta|      naction|          spd|         res|             n|
|------------:|------------:|------------:|------------:|-----------:|-------------:|
| -0.18 (0.88)|  0.37 (0.49)|  0.21 (0.68)|  0.11 (0.42)| 2.27 (2.45)| 450.00 (0.00)|
|  0.83 (1.06)| -1.67 (0.83)| -1.34 (0.12)| -0.25 (0.80)| 6.32 (1.92)|  88.00 (0.00)|
|  0.31 (1.32)| -0.79 (1.73)|  0.95 (2.48)| -0.96 (3.97)| 4.23 (3.05)|  26.00 (0.00)|

### w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
| -0.04 (0.83)|  0.08 (0.45)| 0.99 (0.03)| 0.01 (0.03)| 2.91 (2.80)| 535.00 (0.00)|
|  0.73 (2.54)| -1.45 (3.73)| 0.11 (0.16)| 0.89 (0.16)| 4.52 (3.01)|  29.00 (0.00)|
#+end_export

* Model :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: model
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 2
:END:

** Model V1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: V1
:END:

See [[cite:&jackson_flexsurv_2016]]for available baseline functions.
Proportional baseline:
\[
  \lambda_{ml}(dt_{k,n}) = \lambda_{m0}(t) \lambda_{l} \tau_{k} \text{ for } l \in S_{m} \text{and} \lambda_{s_{m,1}}=1.
\]
\(dt_{k,n}\) denotes t_{k,n}^{stop} - t_{k,n}^{start}

Proportional hazard term:
\[
  e^{(\theta_{k} + \beta) D_{ml} }
\]
- add covariate later.
out-of-state, item, person parameters.
- no incercept term in prop. hazard if baseline contains constant in the same level.
- action m leads to more/less coherent action
- \(D_{ml}\) is bi-directional similarity mapping.
- including \(\beta_m\) doesn't make it directional.
- is \(\beta_k\) meaningful for item-specific action space? certainly not! this opens up the question about how actions should be defined. loosely defined without event_desciption or not.
*** option1: similar items share the same action space
no event description should be used.
*** option2: each item has its own action space (item-specific action space)

- use multi-state modeling framework to explain?
- target journal:
- grant application (check deadline)
- meeting at 4pm (CST)
- online learning platform: interaction with online resources, with instructors, with other people (communication length, contents) - team collaboration.
  - data will be available on Aug.
  + team science program (NIH)


The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$.

\begin{align*}
  q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}},
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated
Common out of state transition: \(\beta_{ml}=\beta_{m}\).

Baseline hazard:
\[
  \lambda_{ml}(t) = \alpha_{m1}(t) \alpha_{l} + \theta_{\lambda} \text{ for } l \neq 1.
\]
Proportional hazard term:
\[
  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}}
\]
- \(D_{ml}\) is bi-directional similarity embedding between actions $m$ and $l$.

The piecewise-constant baseline hazard is used:
\begin{equation}
\label{eq:1}
\lambda(t) = \lambda_j \text{ if } s_{j-1} \le t < s_{j},
\end{equation}
for $j = 1,\ldots,J$. $\lambda_{j}$ could be a function of the similarity. This would be similar to have a piecewise constant transition matrix (time-inhomogeneous Markov chain), but much simpler as you have a parametric model for constants. The cosine similiarity should be normalized before used.
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t) \exp( \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta} d_{i,m,l} ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.
** Model V2
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: V2
:END:

Let $S$ denotes a set of all possible action. For each state $m \in S$, $A_{m}$ denotes a set of competing transitions $\{l_1, \ldots, l_{n_m}\}$ that can be taken directly after $m$.
Let \(Y_k(t)\) denote an action of k-th respondent at time $t$. All respondents assmed to begin problem solving processes at time $t=0$.
*** Action embedding

*** Multistate model
The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$:
\begin{align*}
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S,
\end{align*}
where $\mathcal{F}_t$ denotes the process up to time $t$.

Action transition is assumed to follow Semi-Markovian, which means the intensity depends on the sojourn time (\(t - t_{m}\) ; time spent on the current action). This is often called "clock reset" approach as opposed to "clock forward" approach. Let $dt_{m}$ denote the sojourn time.

Cox model
\begin{align}
q_{ml}\left(t ; \mathcal{F}_{t}\right) = & q_{ml} (t - t_{m}; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t))\\
= & \lambda_{ml}(dt_{m})  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}},
\end{align}

for person $k = 1,\ldots,N$, where $\mathbf{z}(t)$ is time-varying covariates, $\lambda_{kml}(t)$ is a baseline intensity function, \(D_{ml} \in [-1,1]\) denotes the cosine similarity between actions $m$ and $l$. The cosine similarity is obtained using ~word2vec~ on action sequences of an item. The closer the cosine value to 1, the greater the similarity between actions. The closer the cosine value to -1, the greater the dis-similarity between actions. This mean there are $n_{m}$ corresponding intensity functions for state $m$, and overall $\sum_{m in S} n_m$ intensity functions.

We use the constant baseline hazard based on out-of-state transition speed and person's transition speed:
\[
  \lambda_{ml}(dt) = \kappa_{m} \tau_{k} \text{ for } l \in A_{m}.
\]

A running model has no coviarate terms:
\[
q_{ml}\left(t ; \mathcal{F}_{t}\right) = q_{ml}(dt) = \kappa_{m} \tau_{k} e^{\theta_{k} D_{ml} }.
\]

- larger $\kappa_{m}$ shorter time staying on action $m$ (faster out-of-state transition)
- larger $\tau_{k}$, faster transition speed
- larger $\theta_{k}$, larger trasition rate towards a similar action. A person with large $\theta_{k}$ tends to choose more coherent actions

*** likelihood

\begin{align*}
    q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}}\\
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S
\end{align*}


The survival function is
\[
  S_{ml}(dt) = e^{-\int_{0}^{dt_{m}} q_{ml}(x) \dd x}.
\]
Let $\nu_{mlk}(t) = 1$ if person $k$ jump from actions $m$ to $l$ at time $t$; 0 otherwise.
\[
  f_{ml}(t) = q_{ml}(t) S_{ml}(t)
\]
\[
  likelihood =\prod_{k} f_{ml}(dt) \prod_{g \in A_{m}} S_{mg}(t),
\]
\[
  f_{ml} = q_{ml}(t) S_{ml}(t),
  S_{ml}(t) = e^{-\int_{0}^{t^{stop} - t^{start}} q_{ml}(t)\dd t}
\]

\[
  S_{ml}(dt) =  e^{-dt \kappa_{m} \omega_{l} \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }}
\]

\(n = 1,\ldots,M_{k}\): n-th action of k-th person, $M_k$: sequence length

\(  \delta_{k,n,m} = 1 \) if person k's n-th action is m.

\( \delta_{k,n,m}  \delta_{k,n+1,l} = 1 \) for $n < M_{k}$ if person k's n-th transition is m to l.

time at starting state (one after START) is set to the first action (n=1), and the corresponding time is set to 0.
*** prior
The proposed method use a fully Bayesian approach for estimating the proposed latent space model, using MCMC methods. Our prior specification is as follows:

\begin{align*}
\pi\left(\kappa_{m}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\tau_{k}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\theta_{k} | \sigma^{2}\right) & \sim \mathrm{N}\left(0, \sigma^{2}\right); \\
\pi\left(\sigma^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma}, b_{\sigma}\right); \\
\end{align*}

# \pi\left(\beta_{k} |\mu_{\beta}, \sigma_{\beta}^{2}\right) & \sim \mathrm{N}\left(\mu_{\beta}, \sigma_{\beta}^{2}\right); \\
# \pi\left(\mu_{\beta}|\sigma_{\beta}) & \sim \mathrm{N}\left(0, \sigma^{2}_{\mu_{\beta}}\right);\\
# \pi\left(\sigma_{\beta}^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma_{\beta}}, b_{\sigma_{\beta}}\right),\\

# \begin{align*}
# \pi\left(\sigma^{2}\right) & \propto \sigma^{-2}\\
# \pi\left(\mu_{\beta}, \sigma_{\beta}^{2}\right) & \propto \sigma_{\beta}^{-2},
# \end{align*}

inv-Gamma(\theta|\alpha,\beta)
\[
p(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha+1)} e^{-\beta / \theta}, \quad \theta>0
\]

where hyperparameters are chosen as
\[a_{\sigma}=0.0001, b_{\sigma}=0.0001, \mu_{\theta}=0, \text { and } ....\]

Based on our experience, the inference of $\mathbf{\Theta}$ is highly sensitive to its variance $\sigma^2$. Also, the configuration of latent embeddings highly depends on the scale parameter $\gamma$ of the latent space. Rather than choosing sub-optimal tuning parameters, we use a layer of hyper-priors to learn optimal values of these parameters from data. We choose hyperparameters such that priors are minimally informative to facilitate the flexible Bayesian learning.
*** pseudo code
:PROPERTIES:
:ID:       5c29e214-f86d-41d5-89a0-e164602bf6b8
:END:
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::79;;1][3.2 Normal data with a noninformative prior distribution org-id:{ce3939d9-fb55-4b01-8747-0f486c98c9e7}:org-id]]
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::591;;1][Continuous distributions org-id:{5c29e214-f86d-41d5-89a0-e164602bf6b8}:org-id]]
**** update \(\kappa_{m}\):
- For each $k,c$, a symmetric MH jumping $J(\kappa_{m}^{(l-1)} \rightarrow \kappa_{m}^{* })$ is used to propose a new sample.
- all $k$ person's having action m, all l \in A_m (all possible actions that can jump from m)
- transition start and stop time $dt_{k,n}$ for all $\delta_{k,n,m} = 1$
- We accept $\kappa_{m}^{(l)} = \kappa_{m}^{* }$ with probability $\min(1, r_{{\kappa_{m}}^{* }})$ where
\begin{align*}
\log r_{{\kappa_{m}}^{* }} =&
\sum \delta_{k,n,m} (\log \kappa_{m}^{* } - \log \kappa_{m}^{(l-1)})\\
&-\sum dt (\kappa_{m}^{* } - \kappa_{m}^{(l-1)}) \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }
+ \log \frac{\pi(\kappa_{m}^{* })}{\pi(\kappa_{m}^{t})}.
\end{align*}
**** update \(\tau_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
\begin{align*}
\log r_{{\tau_{k}}^*} =&
\sum \delta_{k,n,m} (\log \tau_{k}^* - \log \tau_{k}^{(l-1)})\\
&-\sum dt \kappa_{m}e^{(\theta_{k} + \beta) D_{ml}} ( \tau_{k}^* -  \tau_{k}^{(l-1)} )
+ \log \frac{\pi(\tau_{k}^*)}{\pi(\theta_{k}^{t})}.
\end{align*}
**** update \(\theta_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
- For each $k$, a symmetric MH jumping $J(\theta_{k}^{(l-1)} \rightarrow \theta_{k}^{* }$ is used to propose a new sample.
- We accept $\theta_{k}^{(l)} = \theta_{k}^{* }$ with probability $\min(1, r_{{\theta_{k}}^{* )}})$ where

\begin{align*}
\log r_{{\theta_{k}}^{* }} =& \sum \delta_{k,n,m} (\theta_{k}^{* } - \theta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \beta D_{ml} }(e^{\theta_{k}^{* }D_{ml}} -  e^{\theta_{k}^{(l-1)} D_{ml} })
+ \log \frac{\pi(\theta_{k}^{* })}{\pi(\theta_{k}^{(l-1)})}.\\
\end{align*}
**** update \(\sigma\)
\[
 p( \sigma^2|e.e.) \propto invGamma(\sigma^{2}|a,b) \prod N(\theta_{k} | \mu, \sigma^2)
\]
\(\sigma^{2} \sim inv-gamma(a + 0.5 * N, b + 0.5 + \sum \theta_{k}^2)\)
with flat prior:
\(\sigma^{2} \sim inv-gamma(0.5 * N, 0.5 + \sum \theta_{k}^2)\)
**** update \(\omega_{l}\) :noexport:ARCHIVE:
- all $k$ person's having action l, all m \in B_l (all possible actions that can jump to l)
- transition start and stop time $dt_{k,n-1}$ for all $\delta_{k,n,l} = 1$
**** update \(\beta\) :noexport:ARCHIVE:
- a symmetric MH jumping $J(\beta_{k}^{(l-1)} \rightarrow \beta_{k}^{* })$ is used to propose a new sample.
- We accept $\beta_{k}^{(l)} = \beta_{k}^{* }$ with probability $\min(1, r_{{\beta_{k}}^{* )}})$ where
\begin{align*}
\log r_{{\beta_{k}}^{* }} =&
\sum \delta_{k,n,m} (\beta_{k}^{* } - \beta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \theta_k D_{ml} }(e^{\beta^{* }D_{ml}} -  e^{\beta^{(l-1)} D_{ml} })
+ \log \frac{\pi(\beta_{k}^{* })}{\pi(\beta_{k}^{(l-1)})}.
\end{align*}
**** update \(\mu_{\beta}, \sigma_{\beta}\) :noexport:ARCHIVE:
\begin{align*}
  \rho &= 1/\sigma_{\beta}^{2} + 1/\sigma_{\mu_{\beta}}^2 \\
  p(\mu_{\beta}|..)&= N(\frac{1/\sigma_{\beta}^2 \times \beta}{\rho}, 1/\rho )
\end{align*}
\[
  p(\sigma_{\beta}^{2}|ee) = inv-Gamma(a + 0.5, b + 0.5 (\beta - \mu_{\beta})^2)
\]
*** data structure
**** mstate R package :noexport:
- how to format data to follow the below?
| person | entry time | exit time | from | to | observed | covariate1 | covariate2 | covariate3    | time dependent covaraite |
|      1 |          0 |        10 |    1 |  2 |        1 | D_{12}     | \theta D_{12}   | total actions | none                     |
|      1 |          0 |        10 |    1 |  3 |        0 | D_{12}     | \theta D_{12}   | total actions | none                     |
|        |            |           |      |    |          |            |            |               |                          |
|        |            |           |      |    |          |            |            |               |                          |
- mstate: cannot handle recurrent states
#+begin_src R
library(mstate)
# Transition matrix for illness-death model
tmat <- trans.illdeath()
# Data in wide format, for transition 1 this is dataset E1 of
# Therneau & Grambsch (T&G)
tg <- data.frame(id=1:6,illt=c(1,1,6,6,8,9),ills=c(1,0,1,1,0,1),
                 dt=c(5,1,9,7,8,12),ds=c(1,1,1,1,1,1),
                 x1=c(1,1,1,0,0,0),x2=c(6:1))
# Data in long format using msprep
tglong <- msprep(time=c(NA,"illt","dt"),status=c(NA,"ills","ds"),
                 data=tg,keep=c("x1","x2"),trans=tmat, id="id")
#+end_src

#+RESULTS:
: Loading required package: survival

#+begin_src R
# Same thing in etm format
tra <- trans2tra(tmat)
tgetm <- msdata2etm(tglong, id="id")
tgetm <- msdata2etm(tglong, id="id", covs=c("x1", "x2")) # with covariates
# And back
etm2msdata(tgetm, id="id", tra=tra)
etm2msdata(tgetm, id="id", tra=tra, covs=c("x1", "x2")) # with covariates
#+end_src

#+RESULTS:
: Error in trans2tra(tmat) : could not find function "trans2tra"
: Error in msdata2etm(tglong, id = "id") :
:   could not find function "msdata2etm"
: Error in msdata2etm(tglong, id = "id", covs = c("x1", "x2")) :
:   could not find function "msdata2etm"
: Error in etm2msdata(tgetm, id = "id", tra = tra) :
:   could not find function "etm2msdata"
: Error in etm2msdata(tgetm, id = "id", tra = tra, covs = c("x1", "x2")) :
:   could not find function "etm2msdata"

**** msm R package
https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv
#+begin_src R
library(msm)
msmdat <- data.frame(
 subj = c(1, 1, 1, 1, 1, 2, 2, 2),
 days = c(0, 27, 75, 97, 1106, 0, 90, 1037),
 status = c(1, 2, 3, 4, 4, 1, 2, 2),
 age = c(66, 66, 66, 66, 69, 49, 49, 51),
 treat = c(1, 1, 1, 1, 1, 0, 0, 0)
)
# transitions only allowed to next state up or state 4
Q <- rbind(c(1, 1, 0, 1),
           c(0, 1, 1, 1),
           c(0, 0, 1, 1),
           c(0, 0, 0, 0))
dat <- msm2Surv(data=msmdat, subject="subj", time="days", state="status",
         Q=Q)
dat
attr(dat, "trans")
#+end_src
* Draft
:PROPERTIES:
:header-args:R: :exports none :eval never-export
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: draft
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 3
:END:
[[file:~/Dropbox/research/procmod/ies_naep/main.tex::\section{Significance}][NEAP proposal]]

** Introduction
%%% stolen from the neap proposal

*** what's process data
%Advances in technology have expanded opportunities for educational measurement through changes to item design, item delivery and data collection. Some examples include simulation-, scenario-, and game-based assessment and learning environments.

The NAEP computerized testing format provides an interactive environment for students.
Students can choose among a set of available actions and take one or more steps to finish a task. All student actions are automatically recorded in system log (Kerr, Chung, \& Iseli, 2011), which can be used %immediately for providing instant feedback to students
for diagnostic and scoring purposes (DiCerbo \& Behrens, 2014).

*** Benefits of process data
The availability of process data open new research opportunities including to better understand test-takers' behavior patterns, ..., and ....

*** Challenges
While the availability of rich response process data during problem solving comes the great challenge of building appropriate psychometric models to analyze these data.
The raw process data are usually formatted as lines of coded and time-stamped strings.
The vast amount of data on students' potential trial-and-error process makes it less than straightforward to detect patterns in problem solving.

*** Exisitng methods and limitations
Several data analysis techniques and models have been explored to uncover problem-solving patterns. For example, researchers used methods such as cluster analysis (Bergner, Shu, \& von Davier, 2014) and editing distance (Hao, Shu, Bergner, Zhu, \& von Davier, 2014).
Other researchers explored the method of combining Markov movesl and item response theory (IRT) framework. Process mining techniques such as Petri net were also used to study behavioral patterns (Howard, Johnson, \& Neitzel, 2010).
In addition, researchers used digraphs to visualize and analyze sequential process data collected from assessment.
Zhu, Shu, and von Davier (2016) used network visualization and analysis for understanding process data.

cite:&chen_statistical_2019
[[cite:&chen_continuous-time_2020]]
[[cite:&tang_latent_2019]]
cite:&he_identifying_2015
cite:&ulitzsch_combining_2021
cite:&qiao_data_2018
[[cite:&wang_subtask_2020]]

RNN
[[cite:&tang_exploratory_2019]]

n-gram
cite:&van_der_ark_identifying_2015
*** Motivation and our Aim
Students' response outcomes are a result of a sequence of actions that they take.
The quality as well as quantity of actions vary across individuals as well as across items.
Understanding the action sequence and its relation to response outcomes will help us better understand the nature of response process and individual differences in the process.
Models that relate process data to process outcomes are rare in the current literature.

We propose to develop a new, network modeling framework for analyzing time-stamped sequences of actions taken by NAEP test takers. The innovative aspect of our proposal is that we view test takers’ sequences of actions collected in the computer-assisted NAEP assessment system as directed paths between actions in a network of possible actions.  With our framework, researchers and policymakers can quantify and better understand how learners with disabilities process mathematics test items.

We have successfully collaborated to develop novel network-based modeling approaches for analyzing conventional assessment data on two papers (Jin \& Jeon, 2018; Jeon, Jin, Schweinberger, \& Baugh, 2020), with more papers in the pipeline. We will extend this model-based framework for analyzing NAEP process data. Since the number of possible actions is large and many test takers will choose a small subset of the possible actions, the data is sparse. To deal with the sparsity of the data, we use machine learning techniques. These machine learning techniques penalize models that are more complex than warranted by the data.

*** Advantages of the proposed method
Advantage I. An important advantage of our network-based approach is the introduction of a virtual, two-dimensional Euclidean map of the interplay between actions for different test takers. This interactive map could offer substantially enhanced insights into how and why learners with and without disabilities are different in their response behavior on the current NAEP mathematics assessment.

Advantage II. A second advantage of our network-based approach is that we can easily link the network of actions with test takers’ mathematics performance outcomes, their background information, as well as any technical accommodations they utilized during the test, which allows educators to identify which accommodations might be more effective than others in helping learners with disabilities to display their full ability within the digitalized NAEP assessment environment.

*** paper org.
We first develop xxx. We further develop xxx. The
remainder of this article is organized as follows.
In Section, we introduce . In Section , we present. Applications are given in
Section, followed by conclusions given in
Section.
x
** Motivating example
*** Problem Solving in Technology-Rich Environments

% stolen from cite:&chen_statistical_2019
We introduce a specific item, CLIMATE CONTROL (CC), to demonstrate the data structure and to motivate our research questions. It is part of a CPS unit in PISA 2012 that was designed under the “MicroDYN” framework (Greiff et al., 2012; Wüstenberg et al., 2012), a framework for the development of small dynamic systems of causal relationships for assessing CPS.


Interactive tasks as implemented in the problem solving in a technology-rich
environment (PSTRE) domain in the Programme for the International Assessment of
Adult Competencies (PIAAC, citealt:&oecd_technical_2019) and the problem solving domain in the
Programme for International Student Assessment (PISA, OECD, 2014) aim at mirroring
real-life problem-solving behavior (Goldhammer, Naumann, & Keßel, 2013). While correct
responses to such tasks can be assumed to stem from examinees having the skill set and
the motivation required to solve the task, incorrect responses can occur for a variety of
different reasons, ranging from lack of different subskills and/or metacompetencies required
to solve the task through misinterpreting instructions to examinees not exerting their best
Fo
effort and interacting quickly and superficially with the task at hand.

As a motivating example, we introduce problem solving in technology-rich environments (PSTRE) We introduce an example of pro

OECD Survey of Adult Skills (PIAAC) Log Data
Downloaded from https://piaac-logdata.tba-hosting.de/
Problem Solving Items:

The Programme for the International Assessment of Adult Competencies (PIAAC) is a programme of assessment and analysis of adult skills. The major survey conducted as part of PIAAC is the Survey of Adult Skills. The Survey measures adults’ proficiency in key information-processing skills - literacy, numeracy and problem solving - and gathers information and data on how adults use their skills at home, at work and in the wider community.

This international survey is conducted in over 40 countries/economies and measures the key cognitive and workplace skills needed for individuals to participate in society and for economies to prosper.

The OECD Survey of Adult Skills (PIAAC) assesses the proficiency of adults in information processing skills. During the PIAAC assessement, user interactions were logged automatically. This means that most of the users’ actions within the assessment tool were recorded and stored with time stamps in separate files called log files.

#+begin_quote
This refers to the ability to use technology to solve problems and accomplish complex tasks. It is not a measurement of “computer literacy”, but rather of the cognitive skills required in the information age – an age in which the accessibility of boundless information has made it essential for people to be able to decide what information they need, to evaluate it critically, and to use it to solve problems. In this survey, higher-order skills are identified along with basic proficiency.
#+end_quote

**** [[id:32a1c1a4-f67b-428e-8458-16f4691b77a3][Questions we like to answer (from Dr. Jeon's proposal)]]
- which sequences or actions are effective? given the person's ability and item difficulty
- is the same sequence (strategy) effective for all items or not?
- is the same sequence effective for all people?
- if effective sequences are not the same across all items, can we extract some common features of effective sequences ?
- which sequences or actions are more or less effective for students with disability?
- any other person covariates? ability? that is, does the effectiveness of sequences depend on person abilities? (interaction between sequence and ability)
- does the effect of the sequence change depending on how long it took? for instance,  when it was taken in a shorter time, a sequence might have a positive effect, while it might have a negative effect when it was taken in a longer time.
- instead of using the log time (continuous), it may be better or useful to use a categorical variable?
- the effect of sequences on the success probability may be a function of item difficulty or other item features, for instance, item position, item types (e.g., multiple-choice vs. open-ended), item contents (algebra, geometry) ?

**** Illustrate a ticket example:

#+attr_latex: :placement [ht] :width 0.5\textwidth
#+label: fig:tickets_home
#+caption: An example of PS-TRE items. In this simulated web environment, respondents can access information required for ticket reservation.
[[file:tickets_demo.png]]

This  item  involves  a  scenario  in  which  the respondent  is asked to reserve all fooball game tickets that an entire group can attend. A group of friend provides thier availabilities via an online calendar. Respondents  access  and  evaluate  information from ticket-reservation web pages and online calendars in simulated web environment. Respondents are able to:
- Click on tabs for ticket reservation web pages and online calendar;
- Click on checkboxes to choose game dates;
- Manipulate drop-down menus for events, locations, and number of tikcets; and
- Click on menu items or navigation icons.

#+begin_src R :results silent
sub_str = rbind(
  ## c("(.*)\\*\\$target=u021_(.*)","\\1\\2"),
  c("TEST_TIME=(.+)",""),
  c("id=u021_", ""),
  c("id=",""),
  c("\\*\\$target=_", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", "index="),
  c("\\*\\$href=", "href="),
  c("\\|","\\.")
  )
ignore_desc = c(
"START",
"END",
"KEYPRESS")
piacc_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata"

item_code = "U21x000S"

item = read_piacc(piacc_path, item_code, sub_str, ignore_str)
unique(item$event_description)
length(unique(item$word))
#+end_src

#+begin_src R
library(kableExtra)
dd = item %>% filter(response == 1) %>% select(SEQID, word, timestamp)
aa = dd %>% filter(SEQID == 4016) %>% mutate(timestamp = round(timestamp / 1000,1))
aa %>% kbl(booktab = T, format = "latex")
#+end_src

#+attr_latex: :placement [H] :center t :font \sffamily
#+label: tab:df_action
#+caption: An example action sequence of one respondent. A sequence of actions (2nd column) taken is recorded with timestamp (3rd column).
|   ID | Action                         | Time (sec) |
|------+--------------------------------+------------|
| 4016 | START                          |        0.0 |
| 4016 | COMBOBOX-default_menu1.index=7 |       47.3 |
| 4016 | COMBOBOX-default_menu2.index=2 |       51.8 |
| 4016 | BUTTON_search-default_txt23    |       65.0 |
| 4016 | CHECKBOX-check2                |       93.2 |
| 4016 | BUTTON_available-pg1_txt47     |       96.0 |
| 4016 | BUTTON_available-pg7_txt47     |      108.2 |
| 4016 | COMBOBOX-pg2_menu1.index=19    |      136.7 |
| 4016 | COMBOBOX-pg2_menu6.index=19    |      144.5 |
| 4016 | BUTTON_submit-pg2_txt33        |      146.1 |
| 4016 | BUTTON_submit_ok-u21p2pu5_txt2 |      148.9 |
| 4016 | NEXT_INQUIRY-REQUEST           |      155.8 |
| 4016 | END                            |      157.3 |


#+begin_src R
item %>% group_by(SEQID) %>% summarise(mnum = max(event_num), time = max(timestamp)/1000) %>% summary %>%
#+end_src

#+RESULTS:
: `summarise()` ungrouping output (override with `.groups` argument)
:      SEQID           mnum         time
:  Min.   :   2   Min.   : 3   Min.   :  5.579
:  1st Qu.:1186   1st Qu.:18   1st Qu.:134.172
:  Median :2511   Median :23   Median :182.487
:  Mean   :2490   Mean   :23   Mean   :192.281
:  3rd Qu.:3778   3rd Qu.:28   3rd Qu.:241.393
:  Max.   :5010   Max.   :80   Max.   :833.395

There are 172 unique observed actions.
On average, repondents spend 182 (IQR: 107) seconds on this time, and take 23 (IQR: 10) actions.

**** address challenges in details
The process data consists of pairs of actions and time stamps of each respondents.
Major challenges to establish a statistcal model taking the process data as an input are 1) unequal length of respondents' actions sequences; 2) large number of distinct actions transitions; and 3) ...

Thanks to the recent development of natual language processing

** Methods
%% stolen from neap proposal
We propose to develop a new modeling framework for analyzing time-stamped sequences of actions. The innovative aspect of our proposed model is that we view users' sequences of actions as Markov processes of possible actions mapped to Euclidean space.

With our framework, researchers and policymakers can quantify and better understand learners' problem solving processes.

*** notations
Let $S$ denotes a set of all possible actions. For each action $m \in S$, $A_{m}$
denotes a set of competing actions $\{l_1, \ldots, l_{n_m}\}$ that can be taken
directly after $m$. Let \(t_{k,n}\) denote entry time that the \(k\)-th
respondent starts his/her $n$-th action. So, his/her sojourn time in the $n$-th
action is denoted by $\dd t_{k,n} = t_{k,n+1} - t_{k,n}$ for $n < M_{k} - 1$.
Respondents are assumed to begin problem solving processes at time $t=0$. Let
\(Y_k(t)\) denote an action being taken by the $k$-th respondent at time $t$.
Then, a sequence of the $k$-th respondent's actions is \(S_{k} =
\{y_{k}(t_{k,1}), y_{k}(t_{k,1}),y_{k}(t_{k,2}), \ldots, y_{k}(t_{k,M_{k}})\}\) whose
length is $M_{k}$. We define \( \delta_{k,n,m} = 1 \) if respondent $k$'s $n$-th
action is $m$; $0$ otherwise. Thus, \( \delta_{k,n,m} \delta_{k,n+1,l} = 1 \) means
respondent $k$'s \(n\)-th transition ($n < M_{k}$) is from action $m$ to action
$l$.

*** Action embedding
Instead of using the action symbol as an input in the model, ...

A goal for action embedding is to substitute a symbolic representation with a vectoric representation of actions. Similar procedures in natual language processing context is well established, and we will adapot a skip-gram model for the action embedding purpose.

A skip-gram model which predict actions within a certain range before and after the current action in the same sentence. This model learns parameters that lead to a high-valued cosine similarity for embeddings of frequently co-occuring actions, where the cosine similarity between two vectors $u_{i}, v_i \in \mathbb{R}^{d}$ is calculated as
\[
  \frac{\sum_{i=1}^{d} u_{i} v_{i}}{\sqrt{\sum_{i=1}^{d} u_{i}^{2}} \cdot \sqrt{\sum_{i=1}^{d} v_{i}^{2}}}.
\]
Actions that tend to “behave similarly” end up close to one another in the embedding space. The notion of "behavior" could refer to syntactic categorization or semantic association.

The training objective of the skip-gram model is to maximize the probability of predicting neighboring actions given the target action. The objective can be written as the average log probability
\[
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(m_{t+j} \mid m_{t}\right)
\]
where ~c~ is the window size of neighboring actions. The skip-gram formulation defines this probability using the softmax function.
\begin{equation}
\label{eq:skip-gram}
  p\left(m_{j} \mid m_{0}, u, v\right)=\frac{\exp(u\left(m_{0}\right)' v\left(m_{j}\right))}{\sum_{m \in S} \exp(u\left(m_{0}\right)' v(m))}
\end{equation}
where \(u: S \rightarrow \mathbb{R}^{d} \) and \(v: S \rightarrow \mathbb{R}^{d}\) are functions which map actions to a action embedding.

The negative sampling cite:&mikolov_context_2012 is a computational technique proposed by to resolve the intractable denominator in eqref:eq:skip-gram.
Skip-gram modeling of the above form coupled with negative sampling is often referred to as a =word2vec= model cite:&mikolov_distributed_2013.

The closer the cosine value to 1, the greater the similarity between actions. The closer the cosine value to -1, the greater the dis-similarity between actions.

Online visualization tool
Embedding projector
RegExp (regular expression) for metadata
https://projector.tensorflow.org/

How to convert an action sequence to a sequence??
**** leftover :noexport:
The similarity between word embedding vectors is often measured through such measures as the cosine similarity.

The model predicts the neighboring context of an action, given the action itself.

Therefore, through the contexts, words that are similar to each other in their co-occurrence patterns map to vectors that are close to each other in the Euclidean space.

The skip-gram model is to maximize the probability of predicting context words given a target word. The probability is defined by the cosine similarity (softmax function) based on word embeddings.

Words close in the Euclidean space are words 1) with similar meanings, 2) associated with the same part of a sentence, 3) with semantic association. The similarity can be learned from a large corpus. Unseen words in the training sample are embedded, so one can exploit the similarity information.


[[id:1b2724b9-324b-463b-967a-3b064a0f115e][Word2Vec | Skip-grams | TensorFlow Core]]

negative sampling cite:&mikolov_context_2012: to reduce computational cost of the cosine similarity (the denominator is summation of all words in vocabulary).

a bag-of-words (CBOW): to predict a target word given context words (neighbor of a target word)


An action embedding space captures similarity in that space, be it meaning, morphology, context, or some other kind of relationship.

Actions close in the space can be actions 1) with similar functions, 2) with semantic association, or 3) associated with the same part of a sequence,

The notion of “behavior” in this case usually remains underspecified, but could refer to syntactic categorization (i.e., words most often associated with the same part of speech will cluster together) or semantic association (words that are semantically related cluster together). The similarity between word embedding vectors is often measured through such measures as the dot product or cosine similarity.

The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a ~target_word~ that can be considered context word. Take a look at this table of skip-grams for ~target_words~ based on different window sizes.

A second proposed model of word2vec is the continuous bag-of-words model (CBOW), which predicts a word from the context—in reverse from the skip-gram model.

While a bag-of-words model predicts a word given the neighboring context,

The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (n-gram is a contiguous sequence of n items from a given sample of text or speech). The context of a word can be represented through a set of skip-gram pairs of ~(target_word, context_word)~ where ~context_word~ appears in the neighboring context of ~target_word~.
*** data structure
https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv
Given a configured transition matrix, we use \texttt{msm} [[cite:&jackson_multi-state_2011]] to transform data to a desired "long" format:
| person | entry | exit | from | to | observed | cov1   | cov2     | time cov |
|      1 |     0 |   10 |    1 |  2 |        1 | D_{12} | \theta D_{12} |          |
|      1 |     0 |   10 |    1 |  3 |        0 | D_{12} | \theta D_{12} |          |

*** Multistate model (general)
The intensity function $q_{ml}(t)$ represents the instantaneous rate of jumping from action $m$ to $l$ at time \(t\):
\begin{align*}
q_{ml}\left(t ; Y_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t},
\end{align*}
where $m \neq l$, $m, l \in S$, and $\mathcal{Y}_t$ denotes the process up to time $t$.

Action transition is assumed to follow Semi-Markovian, which means the intensity depends on the sojourn time (\(t - t_{m}\) ; time spent on the current action). This is often called "clock reset" approach as opposed to "clock forward" approach. Let $dt_{m}$ denote the sojourn time.

Cox model
\begin{align*}
q_{ml}\left(t ; \mathcal{F}_{t}\right) = & q_{ml} (t - t_{m}; \bm{\lambda}, \bm{\beta}, \mathbf{z}(t))\\
= & \lambda_{ml}(dt_{m})  e^{(\bm{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}},
\end{align*}
for person $k = 1,\ldots,N$, where $\mathbf{z}(t)$ is time-varying covariates, $\lambda_{kml}(t)$ is a baseline intensity function, \(D_{ml} \in [-1,1]\) denotes the cosine similarity between actions $m$ and $l$. The cosine similarity is obtained using ~word2vec~ on action sequences of an item. The closer the cosine value to 1, the greater the similarity between actions. The closer the cosine value to -1, the greater the dis-similarity between actions. This mean there are $n_{m}$ corresponding intensity functions for state $m$, and overall $\sum_{m in S} n_m$ intensity functions.

We use the constant baseline hazard based on out-of-state transition speed and person's transition speed:
\[
  \lambda_{ml}(dt) = \kappa_{m} \tau_{k} \text{ for } l \in A_{m}.
\]

A running model has no coviarate terms:
\[
q_{ml}\left(t ; \mathcal{F}_{t}\right) = q_{ml}(dt) = \kappa_{m} \tau_{k} e^{\theta_{k} D_{ml} }.
\]

- larger $\kappa_{m}$ shorter time staying on action $m$ (faster out-of-state transition)
- larger $\tau_{k}$, faster transition speed
- larger $\theta_{k}$, larger trasition rate towards a similar action. A person with large $\theta_{k}$ tends to choose more coherent actions

  %% stolen from the neap proposal
  [multi-state survival model]
%We take one-partite network view.
We take a network view on action sequences, where nodes are a set of predefined action and links represent action transitions.
Given that item $k$ is chosen,
the action network of student $i$ is represented by $L \times L$ adjacency matrix.
Suppose student $i$ at item $k$ has chosen action $A_{i,k,l}$. The transition probability of moving from $A_{i,k,l}$ to some other action $A_{i,k,m}$ among $L$ actions is modeled with a multinomial logistic model
\begin{equation}\label{eq:action}
     \mathbb{P} ( A_{i,k,m}  |  A_{i,k,l} )
= \frac{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ) }{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,1} ) + \cdots + \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,L} )},
\end{equation}
\noindent
where $ \alpha^{(A)}_m$ and $\alpha^{(A)}_l$  are the main effects of the current and previous actions $m$ and $l$, and  $\alpha^{(A)}_{m,l}$ is the interaction effect of the two actions.
$\beta_{m,l}^{(A)}$ represents the effect of moving from action $A_{i,k,l}$ to $A_{i,k,m}$, while
$ z_{i,k,l,m}$ indicate observed or unobserved covariates that capture the movement.
For example, $z_{i,k,l,m}$ can represent a distance between the two actions as in a latent space modeling approach (reference).
Figure xxx illustrates the direct paths for the sequences of actions taken by
two students, one represented with dashed paths  and the other with solid paths.

\textcolor{red}{MJ: can we handle directions? choosing the same actions? }

\textcolor{cyan}{JY: incorporating action times in the transition probability...} \\
We assume symmetric transition probabilities between actions.
We define a function describing transition intensity (hazard) between actions $m$ and $l$ ($m \neq l$):
\begin{align*}
  h (t ;  A_{i,k,l} \rightarrow  A_{i,k,m}  ) = & \lim_{\delta t \to 0}  \frac{P(A_{i,k}(t + \delta t) = m | A_{i,k}(t) = l)}{\delta t} \\
  = & \lambda_{k,l\rightarrow m}(t) \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ),
\end{align*}
where $\lambda_{k,l\rightarrow m}(t)$ is a baseline intensity function and $A_{i,k}(t)$ is an action taken by person $i$ at $t$ for item $k$.
The non-transition intensity of action $m$ is
\[
  h (t ;  A_{i,k,m} \rightarrow  A_{i,k,m}  ) =   \lambda_{k,m\rightarrow m}(t) \exp( \alpha^{(A)}_m).
\]

Then, the corresponding transition probability can be defined as
\begin{align*}
  \mathbb{P} (t ;  A_{i,k,l} \rightarrow  A_{i,k,m} ) = & \frac{h(t; A_{i,k,l} \rightarrow A_{i,k,m})}{\sum_{l=1}^{L}  h(t; A_{i,k,l} \rightarrow A_{i,k,m})}
\end{align*}

It is possible to include the outcome in this multi-state survival modeling framework. In such case, however, identifying meaningful ``subsequence of actions'' would not be straightforward as appeared in \eqref{eq:no-response1}. Perhaps, we can use this model for parsing action sequence, and use the subsequence for \eqref{eq:no-response1}?

*** Multistate model (intercept only)
The intensity function $q_{ml}(t)$ represents the instantaneous rate of transition from action $m$ to $l$ at time \(t\):
\begin{align*}
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t},
\end{align*}
where $m \neq l$, $m, l \in S$, and $\mathcal{F}_t$ denotes the process up to time $t$.
Action transition is assumed to follow Semi-Markovian, which means the intensity depends olny on $\mathcal{F}_{t}$ through time since the current action
is started. The intensity is assumed to follow a Cox model. We assume the exponential baseline hazard function of a product of out-of-state and respondent's transition speed. The intensity can be written for all possible transitions as
\begin{equation}
\label{eq:intensity}
\begin{split}
q_{ml}\left(t ; \mathcal{F}_{t}\right)
= & q_{ml} (t - t_{Y_{(t)}})\\
= & \kappa_{m} \tau_{k} \exp(\theta_{k} D_{ml}),
\end{split}
\end{equation}
for all $m \in S$, $l \in A_{m}$, respondents $k = 1,\ldots,N$, and \(D_{ml} \in [-1,1]\) denotes the cosine similarity between actions $m$ and $l$. There are $N_m = #\{A_{m}\}$ intensity functions for each action $m$, which leads to $\sum_{m in S} N_m$ intensity functions.

- out-of-action speed $\kappa_{m}$ measures speed of action $m$. The parameter is related to the time to finish action $m$ and move forward to another action.
  - The larger $\kappa_{m}$, The shorter time to finish the action $m \in S$.(faster out-of-state transition)
- action transition speed $\tau_{k}$ measures respondents' speed of problem solving after considering average durations of action s/he took.
  - With large $\tau_{k}$, the respondent is likely to quickly finish each action.
- $\theta_{k}$ measures respondents' tendency to choose actions "behaving similarly".
  - With large $\theta_{k}$, the respondent is likely to choose the next action coherently.

*** TODO NEED REVIEW! stolen from the neap proposal :noexport:

We take a network view on action sequences, where nodes are a set of predefined action and links represent action transitions.
Given that item $k$ is chosen,
the action network of student $i$ is represented by $L \times L$ adjacency matrix.
Suppose student $i$ at item $k$ has chosen action $A_{i,k,l}$. The transition probability of moving from $A_{i,k,l}$ to some other action $A_{i,k,m}$ among $L$ actions is modeled with a multinomial logistic model
\begin{equation}\label{eq:action}
     \mathbb{P} ( A_{i,k,m}  |  A_{i,k,l} )
= \frac{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ) }{ \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,1} ) + \cdots + \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l}+ \beta_{m,l}^{(A)} z_{i,k,l,L} )},
\end{equation}
\noindent
where $ \alpha^{(A)}_m$ and $\alpha^{(A)}_l$  are the main effects of the current and previous actions $m$ and $l$, and  $\alpha^{(A)}_{m,l}$ is the interaction effect of the two actions.
$\beta_{m,l}^{(A)}$ represents the effect of moving from action $A_{i,k,l}$ to $A_{i,k,m}$, while
$ z_{i,k,l,m}$ indicate observed or unobserved covariates that capture the movement.
For example, $z_{i,k,l,m}$ can represent a distance between the two actions as in a latent space modeling approach (reference).
Figure xxx illustrates the direct paths for the sequences of actions taken by
two students, one represented with dashed paths  and the other with solid paths.

We assume symmetric transition probabilities between actions.
We define a function describing transition intensity (hazard) between actions $m$ and $l$ ($m \neq l$):
\begin{align*}
  h (t ;  A_{i,k,l} \rightarrow  A_{i,k,m}  ) = & \lim_{\delta t \to 0}  \frac{P(A_{i,k}(t + \delta t) = m | A_{i,k}(t) = l)}{\delta t} \\
  = & \lambda_{k,l\rightarrow m}(t) \exp( \alpha^{(A)}_m + \alpha^{(A)}_l + \alpha^{(A)}_{m,l} + \beta_{m,l}^{(A)} z_{i,k,l,m} ),
\end{align*}
where $\lambda_{k,l\rightarrow m}(t)$ is a baseline intensity function and $A_{i,k}(t)$ is an action taken by person $i$ at $t$ for item $k$.
The non-transition intensity of action $m$ is
\[
  h (t ;  A_{i,k,m} \rightarrow  A_{i,k,m}  ) =   \lambda_{k,m\rightarrow m}(t) \exp( \alpha^{(A)}_m).
\]

Then, the corresponding transition probability can be defined as
\begin{align*}
  \mathbb{P} (t ;  A_{i,k,l} \rightarrow  A_{i,k,m} ) = & \frac{h(t; A_{i,k,l} \rightarrow A_{i,k,m})}{\sum_{l=1}^{L}  h(t; A_{i,k,l} \rightarrow A_{i,k,m})}
\end{align*}

It is possible to include the outcome in this multi-state survival modeling framework. In such case, however, identifying meaningful ``subsequence of actions'' would not be straightforward as appeared in \eqref{eq:no-response1}. Perhaps, we can use this model for parsing action sequence, and use the subsequence for \eqref{eq:no-response1}?

** Estimation
:PROPERTIES:
:ID:       5c29e214-f86d-41d5-89a0-e164602bf6b8
:END:
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::79;;1][3.2 Normal data with a noninformative prior distribution org-id:{ce3939d9-fb55-4b01-8747-0f486c98c9e7}:org-id]]
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::591;;1][Continuous distributions org-id:{5c29e214-f86d-41d5-89a0-e164602bf6b8}:org-id]]
*** software
- word2vec
- multistate model + visualization

The action embedding algorithm is written using =TensorFlow= cite:tensorflow2015-whitepaper library in =Python= cite:10.5555/1593511.
The MCMC algorithm was written in =R= cite:r_core_team_r_2020 and =C++14= [[cite:&ISO:2014:IIIb]] with =Stan= math library  cite:carpenter_stan_2015. The code and documentations, along with example data sets, are found
in \url{https://jonghyun-yun.github.io/procmod/}.

=tidyLPA= cite:&rosenberg_tidylpa_2018
=mclust= cite:&scrucca_mclust_2016

*** likelihood
\(\bm{\tau} = (\tau_{1},\ldots,\tau_{N})'\)
\(\bm{\theta} = (\theta_{1},\ldots,\theta_{N})'\)
\(\bm{\kappa} = (\kappa_{1},\ldots,\kappa_{M})'\)

\begin{align*}
    q_{ml} (t ; \bm{\kappa, \theta, \tau}, \bm{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{(\bm{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}}\\
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S
\end{align*}


The survival function is
\[
  S_{ml}(dt) = e^{-\int_{0}^{dt_{m}} q_{ml}(x) \dd x}.
\]
Let $\nu_{mlk}(t) = 1$ if person $k$ jump from actions $m$ to $l$ at time $t$; 0 otherwise.
\[
  f_{ml}(t) = q_{ml}(t) S_{ml}(t)
\]
\[
  likelihood =\prod_{k} f_{ml}(dt) \prod_{g \in A_{m}} S_{mg}(t),
\]
\[
  f_{ml} = q_{ml}(t) S_{ml}(t),
  S_{ml}(t) = e^{-\int_{0}^{t^{stop} - t^{start}} q_{ml}(t)\dd t}
\]

\[
  S_{ml}(dt) =  e^{-dt \kappa_{m} \omega_{l} \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }}
\]

\(n = 1,\ldots,M_{k}\): n-th action of k-th person, $M_k$: sequence length

\(  \delta_{k,n,m} = 1 \) if person k's n-th action is m.

\( \delta_{k,n,m}  \delta_{k,n+1,l} = 1 \) for $n < M_{k}$ if person k's n-th transition is m to l.

time at starting state (one after START) is set to the first action (n=1), and the corresponding time is set to 0.
We present a fully Bayesian approach for estimating the proposed model.
*** prior
For each $m$, $k$, we specify independent priors as follows:
\begin{align*}
\pi\left(\kappa_{m}\right) & \sim \operatorname{Gamma}(a_{\kappa}, b_{\kappa}); \\
\pi\left(\tau_{k}\right) & \sim \operatorname{Gamma}(a_{\tau}, b_{\tau}); \\
\pi\left(\theta_{k} | \sigma^{2}\right) & \sim \operatorname{N}(\mu_{\theta}, \sigma^{2}); \\
\pi\left(\sigma^{2}\right) & \sim \operatorname{Inv-Gamma}(a_{\sigma}, b_{\sigma}),\\
\end{align*}
where $\mbox{Inv-Gamma}(\alpha,\beta)$ denotes the inverse gamma distribution with shape $\alpha >0$ and scale $\beta >0$.
# whose density is
# \[
# \operatorname{Inv-Gamma}(y|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-(\alpha+1)} \exp( -\beta \frac{1}{y} ).
# \]

# \pi\left(\beta_{k} |\mu_{\beta}, \sigma_{\beta}^{2}\right) & \sim \mathrm{N}\left(\mu_{\beta}, \sigma_{\beta}^{2}\right); \\
# \pi\left(\mu_{\beta}|\sigma_{\beta}) & \sim \mathrm{N}\left(0, \sigma^{2}_{\mu_{\beta}}\right);\\
# \pi\left(\sigma_{\beta}^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma_{\beta}}, b_{\sigma_{\beta}}\right),\\

# \begin{align*}
# \pi\left(\sigma^{2}\right) & \propto \sigma^{-2}\\
# \pi\left(\mu_{\beta}, \sigma_{\beta}^{2}\right) & \propto \sigma_{\beta}^{-2},
# \end{align*}

The hyperparameters are chosen as
\[a_{\kappa} = a_{\tau} = 0.1, b_{\kappa} = b_{\tau} = 0.1, a_{\sigma}=1.0, b_{\sigma}=1.0, \mu_{\theta}=0, \text { and } ....\]
Based on our experience, the inference of $\mathbf{\Theta}$ is highly sensitive to its variance $\sigma^2$. Also, the configuration of latent embeddings highly depends on the scale parameter $\gamma$ of the latent space. Rather than choosing sub-optimal tuning parameters, we use a layer of hyper-priors to learn optimal values of these parameters from data. We choose hyperparameters such that priors are minimally informative to facilitate the flexible Bayesian learning.
*** update \(\kappa_{m}\):
:PROPERTIES:
:ID:       bffeec1d-053a-4e74-8d30-40278b23d60c
:END:
For each $m$, we draw \(\kappa_m^{(t)}\) from
$\mbox{Gamma}\left(  a_{\tau} + \sum_{n=1}^{M_{k}} \sum_{k=1}^N \mbox{I}(\delta_{k,n,m} = 1) ,b_{\tau} + \sum_{n=1}^{M_{k}-1}\sum_{k=1}^{N} \sum_{ l \in A_m } dt_{k,n} \tau_{k}e^{(\theta_{k} + \beta) D_{ml}}\right)$

#+BEGIN_SRC cpp
  double post_a = a_kappa + 1.0;
  double post_b = b_kappa;
  for (auto &ii : oos_m) {
    if (status.at(ii)==1) post_a += 1.0;
    post_b += tdiff(ii) * tau(sid.at(ii)) * std::exp((theta(sid.at(ii)) + beta) * dist(ii));
  }
  kappa = stan::math::gamma_rng(post_a, post_b, rng);
#+END_SRC
*** update \(\tau_{k}\)
For each $k$, we draw \(\tau_k^{(t)}\) from
$\mbox{Gamma}\left(  a_{\tau} + M_k, b_{\tau} + \sum_{n=1}^{M_{k}} \sum_{m \in S, l \in A_m } dt_{k,n} \kappa_{m}e^{(\theta_{k} + \beta) D_{ml}}\right)$

#+BEGIN_SRC cpp
double post_a = a_tau + 1.0;
double post_b = b_tau;
for (auto &ii : person_k) {
    if (status.at(ii) == 1)
        post_a += 1.0;
    post_b +=
        tdiff(ii) * kappa(acfrom.at(ii)) * std::exp((theta_k + beta) * dist(ii));
}
tau = stan::math::gamma_rng(post_a, post_b, rng);
#+END_SRC
*** update \(\theta_{k}\)
For each $k$, we draw \(\theta_k^{* }\)$ from a symmetric MH jumping distribution, and accept $\theta_{m}^{(l)} = \theta_{m}^{* }$ with probability $\min(1, r_{{\theta_{m}}^{* }})$ where
\begin{align*}
\log r_{{\theta_{k}}^{* }} =& \sum_{m \in S} \sum_{n=1}^{M_{k}} \left[ \delta_{k,n,m} (\theta_{k}^{* } - \theta_{k}^{(l-1)})D_{ml} -\sum_{l \in A_m} dt_{k,n} \kappa_{m} \tau_{k} e^{ \beta D_{ml} }(e^{\theta_{k}^{* }D_{ml}} -  e^{\theta_{k}^{(l-1)} D_{ml} }) + \log \frac{\pi(\theta_{k}^{* })}{\pi(\theta_{k}^{(l-1)})} \right].\\
\end{align*}

*** update \(\sigma^{2}\)
We draw \((\sigma^{2})^{(t)}\) from
\[
 p( \sigma^2|e.e.) \propto \mbox{Inv-Gamma}(\sigma^{2}|a,b) \prod N(\theta_{k} | \mu, \sigma^2)
\]
\(\sigma^{2} \sim \mbox{Inv-Gamma}(a + 0.5 * N, b + 0.5 + \sum \theta_{k}^2)\)
c.f) with flat prior:
\(\sigma^{2} \sim inv-gamma(0.5 * N, 0.5 + \sum \theta_{k}^2)\)

$\mbox{Inv-Gamma}(\alpha,\beta)$ denotes the inverse gamma distribution with a density
\[
\mbox{Inv-Gamma}(y|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} y^{-(\alpha+1)} \exp \left(-\beta \frac{1}{y}\right).
\]

*** update \(\beta\) :noexport:ARCHIVE:
- a symmetric MH jumping $J(\beta_{k}^{(l-1)} \rightarrow \beta_{k}^{* })$ is used to propose a new sample.
- We accept $\beta_{k}^{(l)} = \beta_{k}^{* }$ with probability $\min(1, r_{{\beta_{k}}^{* )}})$ where
\begin{align*}
\log r_{{\beta_{k}}^{* }} =&
\sum \delta_{k,n,m} (\beta_{k}^{* } - \beta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \theta_k D_{ml} }(e^{\beta^{* }D_{ml}} -  e^{\beta^{(l-1)} D_{ml} })
+ \log \frac{\pi(\beta_{k}^{* })}{\pi(\beta_{k}^{(l-1)})}.
\end{align*}
*** update \(\mu_{\beta}, \sigma_{\beta}\) :noexport:ARCHIVE:
\begin{align*}
  \rho &= 1/\sigma_{\beta}^{2} + 1/\sigma_{\mu_{\beta}}^2 \\
  p(\mu_{\beta}|..)&= N(\frac{1/\sigma_{\beta}^2 \times \beta}{\rho}, 1/\rho )
\end{align*}
\[
  p(\sigma_{\beta}^{2}|ee) = inv-Gamma(a + 0.5, b + 0.5 (\beta - \mu_{\beta})^2)
\]
** Applications
*** Tickets
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: tickets
:END:

We implement the prposed method to our motivating example.

- ~ftime~: time until the first action taken
- ~time~: total time of a person's process

We introduce two rudimentary statistcs.
- ~naction~ or ~#action~: the action sequence length $M_{k}$. The total number of actions taken by respondent $k$.
- ~fastness~: ~#action~ divided by the total time elaped since $t_{k,1}$.

Multivariate Gaussian mixture for clustering

- Varying variances and varying covariances (Model 6)

  AIC and

#+begin_src emacs-lisp :var out_dir="tickets"
(insert-file (concat out_dir "/lpa_mods.txt"))
#+end_src

#+begin_export html
--------------------------------------------------------------
tau and theta + naction, spd
--------------------------------------------------------------
Compare tidyLPA solutions:

 Model Classes AIC      AWE      BIC      CLC      KIC
 6     1       9741.434 9945.767 9809.601 9715.434 9758.434
 6     2       6976.934 7402.489 7118.135 6920.782 7008.934
 6     3       5955.984 6602.815 6170.221 5869.626 6002.984
 6     4       5589.996 6457.851 5877.267 5473.684 5651.996

Best model according to AIC is Model 6 with 4 classes.
Best model according to AWE is Model 6 with 4 classes.
Best model according to BIC is Model 6 with 4 classes.
Best model according to CLC is Model 6 with 4 classes.
Best model according to KIC is Model 6 with 4 classes.

An analytic hierarchy process, based on the fit indices AIC, AWE, BIC, CLC, and KIC (Akogul & Erisoglu, 2017), suggests the best solution is Model 6 with 4 classes.

--------------------------------------------------------------
naction, spd
--------------------------------------------------------------
Compare tidyLPA solutions:

 Model Classes AIC      AWE      BIC      CLC      KIC
 6     1       5391.087 5462.777 5415.432 5383.087 5399.087
 6     2       3693.159 3853.480 3746.718 3672.956 3707.159
 6     3       3370.270 3619.131 3453.043 3337.955 3390.270
 6     4       3506.481 3844.076 3618.469 3461.861 3532.481

Best model according to AIC is Model 6 with 3 classes.
Best model according to AWE is Model 6 with 3 classes.
Best model according to BIC is Model 6 with 3 classes.
Best model according to CLC is Model 6 with 3 classes.
Best model according to KIC is Model 6 with 3 classes.

An analytic hierarchy process, based on the fit indices AIC, AWE, BIC, CLC, and KIC (Akogul & Erisoglu, 2017), suggests the best solution is Model 6 with 3 classes.
#+end_export

#+BEGIN_SRC sh
out_dir="tickets/"
cd $out_dir
cd figure
convert -density 300 tau_action.pdf tau_action.png
convert -density 300 theta_tau_res.pdf theta_tau_res.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+END_SRC

#+begin_export html
|Name        |Label                                                                               |Value scheme                                          |
|:-----------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------|
|AGEG5LFS    |Age groups in 5-year intervals based on LFS groupings (derived)                     |Derived - Age groups in equal 5 year intervals (1-10) |
|NFEHRS      |Number of hours of participation in non-formal education (derived)                  |NA                                                    |
|EARNHRDCL   |Hourly earnings excluding bonuses for wage and salary earners, in deciles (derived) |Derived - Decile                                      |
|LEARNATWORK |Index of learning at work (derived)                                                 |NA                                                    |
|ICTHOME     |Index of use of ICT skills at home (derived)                                        |NA                                                    |
|ICTWORK     |Index of use of ICT skills at work (derived)                                        |NA                                                    |
|INFLUENCE   |Index of use of influencing skills at work (derived)                                |NA                                                    |
|NUMHOME     |Index of use of numeracy skills at home (basic and advanced - derived)              |NA                                                    |
|NUMWORK     |Index of use of numeracy skills at work (basic and advanced - derived)              |NA                                                    |
|READHOME    |Index of use of reading skills at home (prose and document texts - derived)         |NA                                                    |
|READWORK    |Index of use of reading skills at work (prose and document texts - derived)         |NA                                                    |
|TASKDISC    |Index of use of task discretion at work (derived)                                   |NA                                                    |
|WRITHOME    |Index of use of writing skills at home (derived)                                    |NA                                                    |
|WRITWORK    |Index of use of writing skills at work (derived)                                    |NA                                                    |
#+end_export

|---------------------------------------+------------------------------------|
| [[file:tickets/figure/lpa_plot-0.png]]    | [[file:tickets/figure/lpa_plot-1.png]] |
| [[file:tickets/figure/lpa_back_line.png]] |                                    |

Response: the smaller, the better

#+RESULTS:
#+begin_export html
## mean


|        tau|      theta|    naction|        spd|      res|
|----------:|----------:|----------:|----------:|--------:|
| -0.4123013|  0.1939430|  0.0105633|  0.2949513| 3.710588|
|  0.9344900| -0.6655596|  0.3971435|  0.1263112| 5.219081|
|  1.1296467| -1.7866636| -1.9230219| -1.9932064| 7.000000|
| -0.9718218|  1.3403763|  0.1527688| -0.0648598| 3.145251|

## sd


|       tau|    theta|   naction|       spd|      res|
|---------:|--------:|---------:|---------:|--------:|
| 0.4111353| 0.384638| 0.6375695| 0.2628682| 2.989527|
| 0.9065238| 0.553052| 1.2095532| 0.3520752| 2.745995|
| 0.9047816| 1.085768| 0.1671354| 2.6734464| 0.000000|
| 0.2688267| 0.354953| 0.5181867| 0.3940398| 2.883724|

## n


| tau| theta| naction| spd| res|
|---:|-----:|-------:|---:|---:|
| 425|   425|     425| 425| 425|
| 283|   283|     283| 283| 283|
|  75|    75|      75|  75|  75|
| 179|   179|     179| 179| 179|
#+end_export

\tau's covaritates:
\theta's covaritates:
| [[file:tickets/figure/theta_tau_res.png]]       | [[file:tickets/figure/tau_action.png]]          |
| [[file:tickets/figure/time_action-3.png]]       | [[file:tickets/figure/time_action_more-2.png]]  |
| [[file:tickets/figure/time_action_more-5.png]]  | [[file:tickets/figure/time_action_more-7.png]]  |
| [[file:tickets/figure/time_action_more-8.png]]  | [[file:tickets/figure/time_action_more-9.png]]  |
| [[file:tickets/figure/time_action_more-10.png]] | [[file:tickets/figure/time_action_more-11.png]] |
| [[file:tickets/figure/time_action_more-13.png]] |                                             |

*** party_invitations-1
[[file:party_invitations-1/figure/lpa_plot-0.png]]
[[file:party_invitations-1/figure/lpa_plot-1.png]]
# [[file:party_invitations-1/figure/lpa_back.png]]
[[file:party_invitations-1/figure/lpa_back_line.png]]

Response: the larger, the better

#+RESULTS:
#+begin_export html

### clustering w/ tau and theta


|          tau|        theta|      naction|           spd|         res|             n|
|------------:|------------:|------------:|-------------:|-----------:|-------------:|
|  2.06 (1.10)| -1.49 (0.96)|  0.08 (3.72)| -4.43 (11.77)| 0.43 (1.13)|   7.00 (0.00)|
|  0.02 (0.65)|  0.63 (0.43)| -0.21 (0.38)|   0.03 (0.00)| 2.77 (0.56)| 443.00 (0.00)|
| -0.65 (0.57)| -0.58 (1.11)| -0.28 (0.70)|   0.03 (0.00)| 1.18 (1.32)| 309.00 (0.00)|
|  0.83 (1.33)| -0.43 (0.89)|  0.84 (1.51)|   0.03 (0.00)| 1.90 (1.24)| 211.00 (0.00)|

### clustering w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
|  1.82 (4.16)| -3.09 (9.85)| 0.98 (0.05)| 0.02 (0.05)| 0.70 (1.25)|  10.00 (0.00)|
| -0.02 (0.90)|  0.03 (0.00)| 0.00 (0.01)| 1.00 (0.01)| 2.07 (1.23)| 960.00 (0.00)|
#+end_export

*** book_order

[[file:book_order/figure/lpa_plot-0.png]]
[[file:book_order/figure/lpa_plot-1.png]]
# [[file:book_order/figure/lpa_back.png]]
[[file:book_order/figure/lpa_back_line.png]]

Response: the larger, the better
#+RESULTS:
#+begin_export html

### w/ tau and theta


|          tau|        theta|      naction|          spd|         res|             n|
|------------:|------------:|------------:|------------:|-----------:|-------------:|
| -0.18 (0.88)|  0.37 (0.49)|  0.21 (0.68)|  0.11 (0.42)| 2.27 (2.45)| 450.00 (0.00)|
|  0.83 (1.06)| -1.67 (0.83)| -1.34 (0.12)| -0.25 (0.80)| 6.32 (1.92)|  88.00 (0.00)|
|  0.31 (1.32)| -0.79 (1.73)|  0.95 (2.48)| -0.96 (3.97)| 4.23 (3.05)|  26.00 (0.00)|

### w/o tau and theta


|      naction|          spd|      CPROB1|      CPROB2|         res|             n|
|------------:|------------:|-----------:|-----------:|-----------:|-------------:|
| -0.04 (0.83)|  0.08 (0.45)| 0.99 (0.03)| 0.01 (0.03)| 2.91 (2.80)| 535.00 (0.00)|
|  0.73 (2.54)| -1.45 (3.73)| 0.11 (0.16)| 0.89 (0.16)| 4.52 (3.01)|  29.00 (0.00)|
#+end_export

** References
#+csl_style: ~/Zotero/styles/chicago-author-date.csl
#+latex: \printbibliography
# for html export with bib

#+bibliography: ~/Zotero/myref.bib
bibliography:~/Zotero/myref.bib
bibliographystyle:unsrtnat

* COMMENT Local Variables
# Local Variables:
# eval: (jyun/set-org-babel-default-header-args:R)
# eval: (flyspell-mode -1)
# eval: (spell-fu-mode -1)
# End:
