#+TITLE: PIACC data processing

# https://orgmode.org/manual/Export-Settings.html#Export-Settings
#+options:   H:10 num:nil toc:t \n:nil @:t ::t |:t ^:nil ^:{} -:t f:t *:t <:t ':nil -:nil pri:t
#+options:   TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:nil

#+startup: overview inlineimages logdone indent

#+latex_class: article
#+latex_class_options: [letterpaper,11pt]

#+latex_compiler: pdflatex

# comment out for reveal.js
#+setupfile: ~/setup/my-theme-readtheorg.setup
#+setupfile: ~/org/latex_header.setup
#+setupfile: ~/org/orgmode_header.setup
# https://orgmode.org/manual/Export-Settings.html#Export-Settings

#+PROPERTY: header-args :tangle
#+PROPERTY: header-args :eval never-export
#+PROPERTY: header-args:ein :session localhost
#+PROPERTY: header-args:jupyter-python :session *jupyter-piacc* :kernel tf

#+begin_src emacs-lisp
  ;; fast, no unicode-math
  (setq org-preview-latex-default-process 'dvipng)
  ;; slow, supprts unicode-math
  (setq org-preview-latex-default-process 'dvisvgm)
#+end_src

#+RESULTS:
: dvisvgm

* discussion
- [2021-05-09 Sun] note
  I drop event description and tryed a few more embeddings.
it seems incorrect process actions have low cosine similarity. This means they are difficult to form a semantic cluster(s).
actions associated to the task (or problem solving) has high similarity. so as long as event description reoains the same, I can set the subsequence as a new action. embedding of a new action is determined by the mean (center) embeddings of its members.
** party invitation I :noexport:ATTACH:
:PROPERTIES:
:ID:       468ae777-6949-4e3f-9521-fa63037fdf73
:END:
#+caption: Email invitation
#+attr_org: :width 500
[[attachment:_20210426_142403screenshot.png]]
[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/PartyInvitations1/pages/pi-start.html][Party Invitations Part 1]]

#+begin_src R
source("email_word2vec_preproc.r")
email %>% filter(SEQID == 101) %>% select(event_type,  event_description, timestamp, diff)
#+end_src

#+RESULTS:
#+begin_example
       event_type        event_description timestamp   diff
 1:         START                                  0  32679
 2:     MAIL_DRAG                  item101     32679   2784
 3:   MAIL_VIEWED                  item104     35463      4
 4: FOLDER_VIEWED            CanComeFolder     35467     70
 5:    MAIL_MOVED    item101|CanComeFolder     35537   5294
 6:   MAIL_VIEWED                  item102     40831   4551
 7:     MAIL_DRAG                  item102     45382   1912
 8:   MAIL_VIEWED                  item105     47294      4
 9: FOLDER_VIEWED         CannotComeFolder     47298     56
10:    MAIL_MOVED item102|CannotComeFolder     47354   5049
11:   MAIL_VIEWED                  item104     52403   3589
12:     MAIL_DRAG                  item104     55992   1542
13:   MAIL_VIEWED                  item105     57534      4
14: FOLDER_VIEWED            CanComeFolder     57538     50
15:    MAIL_MOVED    item104|CanComeFolder     57588   4722
16:   MAIL_VIEWED                  item105     62310   6690
17:   MAIL_VIEWED                  item103     69000   4563
18: FOLDER_VIEWED            CanComeFolder     73563   3468
19:   MAIL_VIEWED                  item101     77031   9667
20:  NEXT_INQUIRY                              86698   5229
21:           END                              91927 -91927
       event_type        event_description timestamp   diff
#+end_example
** party invitation II :ATTACH:
:PROPERTIES:
:ID:       b782e36e-45f5-40c7-947c-a3f668b53610
:END:

#+attr_org: :width 500
[[attachment:_20210903_105241screenshot.png]]

** [[mu4e:msgid:CAC72WzTzKpZBEHrobD4TFkHZrUHqVu6NR24qsHOYcDZ-dHFP=g@mail.gmail.com][Re: Questions about Data]] :noexport:
- initial location
  + inbox: 100s
  + can come: 200s
  + cannot come: 301

- response = (0 - 3)
  + can come: 101, 104
  + cannot come: 102
  + can come: 201, 202 (don't move, or move but put back(e.g ~TOOLBAR_trash~))
  + anywhere: 103, 105
** to sequence
- discard actions being taken for very short time (100ms)
- otherwise embeddings doesn't help much
#+begin_src R
seqs[4]
#+end_src

#+RESULTS:
: [1] "MAIL_DRAG-item101 MAIL_MOVED-item101|CanComeFolder MAIL_VIEWED-item102 MAIL_DRAG-item102 MAIL_MOVED-item102|CannotComeFolder MAIL_VIEWED-item104 MAIL_DRAG-item104 MAIL_MOVED-item104|CanComeFolder MAIL_VIEWED-item105 MAIL_VIEWED-item103 FOLDER_VIEWED-CanComeFolder MAIL_VIEWED-item101 NEXT_INQUIRY"
** [[id:02fc4776-ea2d-4770-91ed-92d35c08a81d][word2vec embedding]] :noexport:
https://www.tensorflow.org/tutorials/text/word2vec
- Words that tend to “behave similarly” end up close to one another in the embedding space. Instead of using the word symbol as a feature in the model, we can use its vector, which exploits such similarities.

\[
  p\left(w_{j} \mid w_{0}, u, v\right)=\frac{\exp \left(u\left(w_{0}\right)^{\top} v\left(w_{j}\right)\right)}{\sum_{w \in V} \exp \left(u\left(w_{0}\right)^{\top} v(w)\right)}
\]
where \(u: V \rightarrow \mathbb{R}^{k} \) and \(v: V \rightarrow \mathbb{R}^{k}\) are functions which map words to a word embedding—one for the pivot words, and the other for context.
** transition models
- action based transition: >= 500
- embeddings based transition (~embedding_dim~)

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$. It may depend on covariates $\mathbf{z}(t)$, the time t itself, and possibly also the “history” of the process up to that time, $\mathbf{F}_t$: the states previously visited or the length of time spent in them.
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta} d_{i,m,l} ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.
** ticket :noexport:ATTACH:
:PROPERTIES:
:ID:       a183bf8a-10f9-4ccb-861b-9658d5b2b9f5
:header-args:R: :results silent :session *R-PIACC* :exports both :noweb yes :eval never-export
:END:
#+attr_org: :width 300
[[attachment:_20210426_040805screenshot.png]]

[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/FootballTickets/pages/ft-home.html][Football Tickets]]
- calendar: TAB-id=tabbutton2
- ticketing: TAB-id=tabbutton1
- event type: COMBOBOX-id=u021_default_menu1|*$index=7 (football)
- location: COMBOBOX-id=u021_default_menu2|*$index=2 (Bakerton)
- response = 1 (correct) 7 (incorrect) 0 missing
  menu1 and 6 = 9 (8 seems ok too)
- N = 1344
  src_R[:session *R-PIACC* :exports results]{length(unique(df$SEQID))}

#+transclude: t
[[id:673df774-623f-44b5-a262-b22739c9a506][CD Tally]]

* multi-state model
- https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2017/10/multistate_enar_webinar.pdf
- https://cran.r-project.org/web/packages/msm/msm.pdf
- [[id:57b08111-9c48-4fa9-b289-10f01fc7a0d6][cite:hill_relaxing_2021]]
* running model and action space
See cite:jackson_flexsurv_2016 for available baseline functions.
Proportional baseline:
\[
  \lambda_{ml}(dt_{k,n}) = \lambda_{m0}(t) \lambda_{l} \tau_{k} \text{ for } l \in S_{m} \text{and} \lambda_{s_{m,1}}=1.
\]
\(dt_{k,n}\) denotes t_{k,n}^{stop} - t_{k,n}^{start}

Proportional hazard term:
\[
  e^{(\theta_{k} + \beta) D_{ml} }
\]
- add covariate later.
out-of-state, item, person parameters.
- no incercept term in prop. hazard if baseline contains constant in the same level.
- action m leads to more/less coherent action
- \(D_{ml}\) is bi-directional similarity mapping.
- including \(\beta_m\) doesn't make it directional.
- is \(\beta_k\) meaningful for item-specific action space? certainly not! this opens up the question about how actions should be defined. loosely defined without event_desciption or not.
** option1: similar items share the same action space
no event description should be used.
** option2: each item has its own action space (item-specific action space)

- use multi-state modeling framework to explain?
- target journal:
- grant application (check deadline)
- meeting at 4pm (CST)
- online learning platform: interaction with online resources, with instructors, with other people (communication length, contents) - team collaboration.
  - data will be available on Aug.
  + team science program (NIH)


The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$.

\begin{align*}
  q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}},
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated
Common out of state transition: \(\beta_{ml}=\beta_{m}\).

Baseline hazard:
\[
  \lambda_{ml}(t) = \alpha_{m1}(t) \alpha_{l} + \theta_{\lambda} \text{ for } l \neq 1.
\]
Proportional hazard term:
\[
  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}}
\]
- \(D_{ml}\) is bi-directional similarity embedding between actions $m$ and $l$.

The piecewise-constant baseline hazard is used:
\begin{equation}
\label{eq:1}
\lambda(t) = \lambda_j \text{ if } s_{j-1} \le t < s_{j},
\end{equation}
for $j = 1,\ldots,J$. $\lambda_{j}$ could be a function of the similarity. This would be similar to have a piecewise constant transition matrix (time-inhomogeneous Markov chain), but much simpler as you have a parametric model for constants. The cosine similiarity should be normalized before used.
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t) \exp( \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta} d_{i,m,l} ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.
* simplified one (it's probably ok)
Proportional baseline (based on current and next action)
\[
  \lambda_{ml}(dt_{k,n}) = \kappa_{m} \omega_{l} \tau_{k} \text{ for } l \in A_{m} \text{and} \omega_{s_{m,1}}=1.
\]
\(dt_{k,n}\) denotes t_{k,n}^{stop} - t_{k,n}^{start}
- large \kappa -> fast out-of-state action
- how about \(\kappa_{}_{}_{}_{ml}\)?
- \(\omega\) should be dropped.

Proportional hazard term:
\[
  e^{(\theta_{k} + \beta) D_{ml} }
\]
- \(D_{ml}\): cosine similarity([0,1])
- \beta should be dropped. (no keep it, and $\Theta$ has a mean zero).
- large \theta: do similar action
- small (negative) \theta: do dis-similar actions
- fit the model for each response group
- add covariate terms later.
- no incercept term in prop. hazard if baseline contains constant in the same level.
- action m leads to more/less coherent action
- determine absorbing state(s): END? NEXT?
** likelihood
\[
  f_{ml}(t) = q_{ml}(t) S_{ml}(t)
\]
\[
  likelihood =\prod_{k} f_{ml}(dt) \prod_{g \in A_{m}} S_{mg}(t),
\]
\[
  f_{ml} = q_{ml}(t) S_{ml}(t),
  S_{ml}(t) = e^{-\int_{0}^{t^{stop} - t^{start}} q_{ml}(t)\dd t}
\]

\[
  S_{ml}(dt) =  e^{-dt \kappa_{m} \omega_{l} \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }}
\]

\(n = 1,\ldots,M_{k}\): n-th action of k-th person, $M_k$: sequence length

\(  \delta_{k,n,m} = 1 \) if person k's n-th action is m.

\( \delta_{k,n,m}  \delta_{k,n+1,l} = 1 \) for $n < M_{k}$ if person k's n-th transition is m to l.

time at starting state (one after START) is set to the first action (n=1), and the corresponding time is set to 0.
** prior
The proposed method use a fully Bayesian approach for estimating the proposed latent space model, using MCMC methods. Our prior specification is as follows:

\begin{align*}
\pi\left(\kappa_{m}\right) & \sim \operatorname{Gamma}\left(\text{mean} = \tilde \kappa_{m}, \text{var} = 2 \cdot \tilde \kappa_{m})\right); \\
\pi\left(\tau_{k}\right) & \sim \operatorname{Gamma}\left(\text{mean} = \tilde \tau_{k}, \text{var} = 2 \cdot \tilde \tau_{k})\right); \\
\pi\left(\theta_{k} | \sigma^{2}\right) & \sim \mathrm{N}\left(0, \sigma^{2}\right); \\
\pi\left(\beta_{k} |\mu_{\beta}, \sigma_{\beta}^{2}\right) & \sim \mathrm{N}\left(\mu_{\beta}, \sigma_{\beta}^{2}\right); \\
\pi\left(\mu_{\beta}|\sigma_{\beta}) & \sim \mathrm{N}\left(0, \sigma^{2}_{\mu_{\beta}}\right);\\
\pi\left(\sigma^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma}, b_{\sigma}\right); \\
\pi\left(\sigma_{\beta}^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma_{\beta}}, b_{\sigma_{\beta}}\right),\\
\end{align*}

\begin{align*}
\pi\left(\sigma^{2}\right) & \propto \sigma^{-2}\\
\pi\left(\mu_{\beta}, \sigma_{\beta}^{2}\right) & \propto \sigma_{\beta}^{-2},
\end{align*}

inv-Gamma(\theta|\alpha,\beta)
\[
p(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha+1)} e^{-\beta / \theta}, \quad \theta>0
\]


where $\mathrm{MVN}_{d}$ denotes a $d$ dimensional normal distribution,   $\mathbf{I}_{d}$ is a $d \times d$ identity matrix, $\tilde{\lambda}_{j}=G / \left\{U \left(G-j+0.5\right)\right\}$. Other hyperparameters are chosen as
\[a_{\sigma}=0.0001, b_{\sigma}=0.0001, \mu_{\gamma}=0, \text { and } \tau_{\gamma}^{}=2.\]
A similar vague prior for $\lambda_{ic,j}$ has been used in cite:jin_using_2014.

Based on our experience, the inference of $\mathbf{\Theta}$ is highly sensitive to its variance $\sigma^2$. Also, the configuration of latent embeddings highly depends on the scale parameter $\gamma$ of the latent space. Rather than choosing sub-optimal tuning parameters, we use a layer of hyper-priors to learn optimal values of these parameters from data. We choose hyperparameters such that priors are minimally informative to facilitate the flexible Bayesian learning.
** pseudo code
:PROPERTIES:
:ID:       5c29e214-f86d-41d5-89a0-e164602bf6b8
:END:
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::79;;1][3.2 Normal data with a noninformative prior distribution org-id:{ce3939d9-fb55-4b01-8747-0f486c98c9e7}:org-id]]
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::591;;1][Continuous distributions org-id:{5c29e214-f86d-41d5-89a0-e164602bf6b8}:org-id]]
*** update \(\kappa_{m}\)
- all $k$ person's having action m, all l \in A_m (all possible actions that can jump from m)
- transition start and stop time $dt_{k,n}$ for all $\delta_{k,n,m} = 1$
- For each $k,c$, a symmetric MH jumping $J(\kappa_{m}^{(l-1)} \rightarrow \kappa_{m}^{* }$ is used to propose a new sample.
- We accept $\kappa_{m}^{(l)} = \kappa_{m}^{* }$ with probability $\min(1, r_{{\kappa_{m}}^{* )}})$ where
\begin{align*}
\log r_{{\kappa_{m}}^{* }} =&
\sum \delta_{k,n,m} (\log \kappa_{m}^{* } - \log \kappa_{m}^{(l-1)})\\
&-\sum dt (\kappa_{m}^{* } - \kappa_{m}^{(l-1)}) \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }
+ \log \frac{\pi(\kappa_{m}^{* })}{\pi(\kappa_{m}^{t})}.
\end{align*}

*** update \(\sigma\)
\[
 p( \sigma^2|e.e.) \propto invGamma(\sigma^{2}|a,b) \prod N(\theta_{k} | \mu, \sigma^2)
\]
\(\sigma^{2} \sim inv-gamma(a + 0.5 * N, b + 0.5 + \sum \theta_{k}^2)\)
with flat prior:
\(\sigma^{2} \sim inv-gamma(0.5 * N, 0.5 + \sum \theta_{k}^2)\)
*** update \(\omega_{l}\)
- all $k$ person's having action l, all m \in B_l (all possible actions that can jump to l)
- transition start and stop time $dt_{k,n-1}$ for all $\delta_{k,n,l} = 1$
*** update \(\tau_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
\begin{align*}
\log r_{{\tau_{k}}^{* }} =&
\sum \delta_{k,n,m} (\log \tau_{k}^{* } - \log \tau_{k}^{(l-1)})\\
&-\sum dt \kappa_{m}e^{(\theta_{k} + \beta) D_{ml}} ( \tau_{k}^{* } -  \tau_{k}^{(l-1)} )
+ \log \frac{\pi(\tau_{k}^{* })}{\pi(\theta_{k}^{t})}.
\end{align*}
*** update \(\theta_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
- For each $k$, a symmetric MH jumping $J(\theta_{k}^{(l-1)} \rightarrow \theta_{k}^{* }$ is used to propose a new sample.
- We accept $\theta_{k}^{(l)} = \theta_{k}^{* }$ with probability $\min(1, r_{{\theta_{k}}^{* )}})$ where
\begin{align*}
\log r_{{\theta_{k}}^{* }} =&
\sum \delta_{k,n,m} (\theta_{k}^{* } - \theta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \beta D_{ml} }(e^{\theta_{k}^{* }D_{ml}} -  e^{\theta_{k}^{(l-1)} D_{ml} })
+ \log \frac{\pi(\theta_{k}^{* })}{\pi(\theta_{k}^{(l-1)})}.
\end{align*}

*** update \(\beta\)
- a symmetric MH jumping $J(\beta_{k}^{(l-1)} \rightarrow \beta_{k}^{* })$ is used to propose a new sample.
- We accept $\beta_{k}^{(l)} = \beta_{k}^{* }$ with probability $\min(1, r_{{\beta_{k}}^{* )}})$ where
\begin{align*}
\log r_{{\beta_{k}}^{* }} =&
\sum \delta_{k,n,m} (\beta_{k}^{* } - \beta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \theta_k D_{ml} }(e^{\beta^{* }D_{ml}} -  e^{\beta^{(l-1)} D_{ml} })
+ \log \frac{\pi(\beta_{k}^{* })}{\pi(\beta_{k}^{(l-1)})}.
\end{align*}
*** update \(\mu_{\beta}, \sigma_{\beta}\)
\begin{align*}
  \rho &= 1/\sigma_{\beta}^{2} + 1/\sigma_{\mu_{\beta}}^2 \\
  p(\mu_{\beta}|..)&= N(\frac{1/\sigma_{\beta}^2 \times \beta}{\rho}, 1/\rho )
\end{align*}
\[
  p(\sigma_{\beta}^{2}|ee) = inv-Gamma(a + 0.5, b + 0.5 (\beta - \mu_{\beta})^2)
\]


** data structure
*** mstate R package
- how to format data to follow the below?
| person | entry time | exit time | from | to | observed | covariate1 | covariate2 | covariate3    | time dependent covaraite |
|      1 |          0 |        10 |    1 |  2 |        1 | D_{12}        | \theta D_{12}      | total actions | none                     |
|      1 |          0 |        10 |    1 |  3 |        0 | D_{12}        | \theta D_{12}      | total actions | none                     |
|        |            |           |      |    |          |            |            |               |                          |
|        |            |           |      |    |          |            |            |               |                          |
- mstate: cannot handle recurrent states
#+begin_src R
library(mstate)
# Transition matrix for illness-death model
tmat <- trans.illdeath()
# Data in wide format, for transition 1 this is dataset E1 of
# Therneau & Grambsch (T&G)
tg <- data.frame(id=1:6,illt=c(1,1,6,6,8,9),ills=c(1,0,1,1,0,1),
                 dt=c(5,1,9,7,8,12),ds=c(1,1,1,1,1,1),
                 x1=c(1,1,1,0,0,0),x2=c(6:1))
# Data in long format using msprep
tglong <- msprep(time=c(NA,"illt","dt"),status=c(NA,"ills","ds"),
                 data=tg,keep=c("x1","x2"),trans=tmat, id="id")
#+end_src

#+RESULTS:
: Loading required package: survival

#+begin_src R
# Same thing in etm format
tra <- trans2tra(tmat)
tgetm <- msdata2etm(tglong, id="id")
tgetm <- msdata2etm(tglong, id="id", covs=c("x1", "x2")) # with covariates
# And back
etm2msdata(tgetm, id="id", tra=tra)
etm2msdata(tgetm, id="id", tra=tra, covs=c("x1", "x2")) # with covariates
#+end_src

*** msm R package
https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv
#+begin_src R
library(msm)
msmdat <- data.frame(
 subj = c(1, 1, 1, 1, 1, 2, 2, 2),
 days = c(0, 27, 75, 97, 1106, 0, 90, 1037),
 status = c(1, 2, 3, 4, 4, 1, 2, 2),
 age = c(66, 66, 66, 66, 69, 49, 49, 51),
 treat = c(1, 1, 1, 1, 1, 0, 0, 0)
)
# transitions only allowed to next state up or state 4
Q <- rbind(c(1, 1, 0, 1),
           c(0, 1, 1, 1),
           c(0, 0, 1, 1),
           c(0, 0, 0, 0))
dat <- msm2Surv(data=msmdat, subject="subj", time="days", state="status",
         Q=Q)
dat
attr(dat, "trans")
#+end_src

* piacc preproc
#+begin_src emacs-lisp
(delete-directory "input" t)
(make-directory "input")
#+end_src

#+RESULTS:
** item codebook
:PROPERTIES:
:header-args:R: :tangle R/itemcode.R
:END:
example for email. see the codebook! [[file:data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv][PS_BOOKLET_ITEM.csv]]
#+begin_src R
booklet = readr::read_csv("~/Dropbox/research/procmod/procmod-code/data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv")
booklet$NAME = stringr::str_replace_all(booklet$NAME, " ", "_")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "_-", "-")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "-_", "-")
booklet$NAME = tolower(booklet$NAME)
#+end_src

#+RESULTS:
:
: [36m──[39m [1m[1mColumn specification[1m[22m [36m──────────────────────────────────────────────────────────────────[39m
: cols(
:   ITEM = [31mcol_character()[39m,
:   NAME = [31mcol_character()[39m,
:   CODEBOOK = [31mcol_character()[39m,
:   RESPONSE = [31mcol_character()[39m
: )

*** party_invitation-1
#+begin_src R :results value list drawer silent
if (item_code == "U01a000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01a_(.*)","\\1\\2"), c("id=u01a_",  ""), c("\\*\\$target=",  ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

*** party_invitation-2
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)

if (item_code == "U01b000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01b_(.*)","\\1\\2"), c("id=u01b_",  ""), c("\\*\\$target=",  ""), c("\\*\\$value=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

*** cd_tally
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)
if (item_code == "U03a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

*** sprained_ankle-1
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)
if (item_code == "U06a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src
*** sprained_ankle-2
*** tickets
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)
if (item_code == "U21x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

*** class_attendance
*** club_membership-1
*** club_membership-2
*** book_order
*** meeting_room
*** reply_all
*** locate_email
*** lamp_return
** all-in-one R script
:PROPERTIES:
:header-args:R: :tangle R/preproc-data.R
:END:

- preprocess data, word2vec, prepare for C, run C, post analysis!
#+begin_src R :tangle R/all_in_one.R
item_code = "U03a000S"
source("R/itemcode.R")
out_dir = paste0(booklet$NAME[booklet$CODEBOOK == item_code], "/")
source("R/preproc-data.R")
system(paste0("sh run.sh ", out_dir))
#+end_src

#+begin_src R :tangle R/all_in_one.R
item_code = "U21x000S"
source("R/itemcode.R")
out_dir = paste0(booklet$NAME[booklet$CODEBOOK == item_code], "/")
source("R/preproc-data.R")
system(paste0("sh run.sh ", out_dir))
#+end_src

#+begin_src R :results value list drawer silent
library(diprom)
## library(dplyr)
## library(stringr)
## library(msm)
## library(foreach)
## library(doParallel)
stopImplicitCluster()
registerDoParallel(cores = detectCores() - 1)
## doParallel::registerDoParallel(2)
#+end_src

#+begin_src R :results value list drawer silent
setwd('~/workspace/procmod-code/')
piacc_orig_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_cleaned_1110.rdata"
piacc_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata"
## piacc_na.omit()
#+end_src

#+begin_src R :results value list drawer silent
item = read_piacc(piacc_path, item_code, sub_str, ignore_str)
item2sen(item)
item %>% group_by(SEQID) %>% summarise(mnum = max(event_num))
#+end_src

Run Python codes to do ~word2vec~.
#+begin_src R :results silent
system(". activate tf; python piacc_word2vec.py")
#+end_src

#+begin_src R :results silent
metadata_path = "input/metadata.tsv"
vectors_path = "input/vectors.tsv"
metadata = readr::read_tsv(metadata_path, col_names = FALSE)
vectors = readr::read_tsv(vectors_path, col_names = FALSE)

item = filter_item(item)
Q = get_trans(item)
Dml = get_cosdist()
#+end_src

#+begin_src R :results none
dat = item2long(item)

M = length(state)
N = length(unique(dat$id))
dq = nrow(Q)
dn = nrow(dat)
dc = ncol(dat)
#+end_src

#+begin_src R :results none
write_data()
source("R/init.R")
write_loop_index()
#+end_src

#+RESULTS:

** internal functions
#+begin_src R :tangle diprom/R/preproc.R :results value list drawer silent
read_piacc = function(piacc_path, item_code, sub_str, ignore_str, core_event = NULL) {
  load(piacc_path)
  item = PS %>% filter(CODEBOOK == item_code)

## core_event = c("MAIL_DRAG", "MAIL_MOVED", "MAIL_VIEWED")
## email$event_description[!(email$event_type %in% core_event)] <- ""

  timestamp = item$timestamp
  diff = c(diff(timestamp), 99999)
  item$diff = diff
  item = item[(diff >= 10) || (diff < 0 ), ]

  if (length(sub_str) != 0) {
    for (ii in 1:nrow(sub_str)){
      item$event_description = stringr::str_replace_all(item$event_description,
                                                    sub_str[ii,1], sub_str[ii,2])}
  }

  if (length(ignore_desc) != 0) {
    for (ii in 1:length(ignore_desc)) {
      item = item %>% mutate(event_description = ifelse(event_type == ignore_desc[ii],"",event_description)) }
  }

  item = item %>%
    mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
    mutate(word = gsub(" ", "_", event_concat))

  return(item)
}
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
item2sen = function(item) {
ww = item$word
id = item$SEQID
ww0 = ww[1:(length(ww)-1)]
ww1 = ww[2:(length(ww))]

dup = ww0 == ww1
dup0 = c(dup,  FALSE )
dup1 = c(FALSE ,  dup)

idx = dup1

cw = "NULL"
cid = 9999999999
for (kk in which(dup1)) {
  if(cw != ww[kk] && cid != id[kk]) idx[kk] = FALSE
  cw = ww[kk]
  cid = id[kk]
}

item = item[!idx, ]

n = nrow(item)
pre = item$word[1:(n-1)]
cur = item$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

item = item[!idx,]

id = unique(item$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in item$word[item$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(item$word[item$SEQID == i] , collapse = " ")
}

## for (i in id) {
##     seqs[i] = gsub("START ", "", seqs[i])
##     seqs[i] = gsub(" END", "", seqs[i])
## }
seqs = seqs[id]

data.table::fwrite(as.data.frame(seqs), "input/item_sentence.txt", col.names=F)
}
#+end_src

#+begin_src R :tangle diprom/R/preproc.R
filter_item = function(item) {
  ## a few more processing
  ## state to global env
item =  item %>% filter(word !="START")
state <<- unique(item$word)
ntate = numeric(nrow(item))
ii = 0
for (ww in item$word) {
  ii = ii + 1
  ntate[ii] = which(state == ww)
}
item$state = ntate

uid = unique(item$SEQID)
pid = numeric(nrow(item))
ii = 0
for (nn in item$SEQID) {
  ii = ii + 1
  pid[ii] = which(uid == nn)
  }
item$pid = pid
return(item)}

get_trans = function(item) {
id = unique(item$SEQID)
L = length(state <<- unique(item$word))
Q = matrix(0, L, L)
for (i in id) {
  ## for (word in item$word[item$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seq = item$state[item$SEQID == i]
  for (k in 1:(length(seq)-1))
    Q[seq[k], seq[k+1]] = 1
  ## cat(seq[length(seq)-1])
}

rsq = rowSums(Q) > 0
diag(Q) = 1 * rsq

return(Q)}
#+end_src

#+RESULTS:
#+begin_src R :tangle diprom/R/proproc.R
get_cosdist = function() {
dem = ncol(vectors)
vv = array(0, dim=c(M <- length(state), dem))
for (m in 1:M) {
  if (sum(state[m] == metadata[,1]) > 0)
  vv[m, ] = unlist(vectors[which(state[m] == metadata[,1]), ] )
}
Dml = cosdist(as.matrix(vv))

return(Dml)
}
#+end_src

#+RESULTS:

** write data
:PROPERTIES:
:header-args:R: :tangle R/write-data.R
:END:
#+begin_src R :results none :tangle diprom/R/proproc.R
item2long = function(item) {
df = item %>% select(pid, timestamp, response, state)
dat <- msm::msm2Surv(data=df, subject="pid", time="timestamp", state="state",
         Q=Q)
##dat
## attr(dat, "trans")
dat$time = dat$time / max(dat$Tstop)
dat$Tstart = dat$Tstart / max(dat$Tstop)
dat$Tstop = dat$Tstop / max(dat$Tstop)
dat$dist = foreach (i = 1:nrow(dat), .combine="c") %dopar%
  Dml[dat$from[i], dat$to[i]]
return(dat)
}
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
write_data = function(){
## R to C index conversion
wat = dat
wat$id = wat$id - 1
wat$from = wat$from - 1
wat$to = wat$to - 1

## to be read from cpp
readr::write_csv(as.data.frame(state),"input/state.csv", col_names = FALSE)
readr::write_csv(wat,"input/dat.csv", col_names = FALSE)
readr::write_csv(as.data.frame(Q),"input/trans.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(M, N, dq, dn, dc)),"input/mvar.csv", col_names = FALSE)
readr::write_csv(data.frame(Dml), "input/Dml.csv", col_names = FALSE)
}
#+end_src

** init
:PROPERTIES:
:header-args:R:
:END:
#+begin_src R :tangle R/init.R
init_val = list(
kappa = rep(0.1, M),
tau = rep(0.1, N),
theta = rep(0.1, N),
beta = 0,
sigma = 1,
mu_beta = 0,
sigma_beta = 1
)
#+end_src

#+RESULTS:

#+begin_src R :tangle R/init.R
## lambda
params = c(init_val, list(
a_kappa = matrix(0.1,M,1),
b_kappa = matrix(0.1,M,1),
jump_kappa = matrix(0.5,M,1),

jump_beta = matrix(0.1,1,1),

## mu_theta = matrix(0.0,N,1)
## sigma_theta = matrix(1.0,N,1)
jump_theta = matrix(1.0,N,1),

a_sigma = 1.0,
b_sigma = 1.0,

a_tau = matrix(0.1,N,1),
b_tau = matrix(0.1,N,1),
jump_tau = matrix(0.5,N,1),

a_sigma_beta = 1.0,
b_sigma_beta = 1.0,

mu_mu_beta = 0.0,
sigma_mu_beta = 10.0
))
#+end_src

#+RESULTS:

#+begin_src R :tangle R/init.R
write_init(params)
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
write_init = function(params) {
## parameter init
readr::write_csv(as.data.frame(params$kappa),"input/init_kappa.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$tau),"input/init_tau.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$theta),"input/init_theta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$sigma),"input/init_sigma.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$beta),"input/init_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$mu_beta),"input/init_mu_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$sigma_beta),"input/init_sigma_beta.csv", col_names = FALSE)
## hyper-parameter + jump
readr::write_csv(as.data.frame(cbind(params$a_kappa,params$b_kappa,params$jump_kappa)),"input/hyper_kappa.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$jump_beta),"input/hyper_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$jump_theta),"input/hyper_theta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_sigma,params$b_sigma)),"input/hyper_sigma.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_tau,params$b_tau,params$jump_tau)),"input/hyper_tau.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_sigma_beta,params$b_sigma_beta)),"input/hyper_sigma_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$mu_mu_beta,params$sigma_mu_beta)),"input/hyper_mu_beta.csv", col_names = FALSE)
  }
#+end_src

#+RESULTS:


** for-loop index
:PROPERTIES:
:header-args:R: :tangle diprom/R/preproc.R
:END:
#+begin_src R
write_loop_index = function() {
  fstr = NULL
  for (m in 1:M) {
    if(sum(dat$from==m)>0) {
      fstr = c(fstr, paste(which(dat$from == m) - 1, collapse = " ")) # R to C index conversion
    } else fstr = c(fstr, "-99")
  }
  fname = "input/out-of-state_index.txt"
  if (file.exists(fname)) {
    ##Delete file if it exists
    file.remove(fname)
  }
  fcon = file(fname)
  writeLines(fstr, fcon) # R to C index conversion
  close(fcon)

  fstr = NULL
  for (k in 1:max(item$pid)) {
    if(sum(dat$id==k)>0) {
      fstr = c(fstr, paste(which(dat$id == k) - 1, collapse = " ")) # R to C index conversion
    } else fstr = c(fstr, "-99")
}
fname = "input/person_index.txt"
if (file.exists(fname)) {
  ##Delete file if it exists
  file.remove(fname)
}
fcon = file(fname)
writeLines(fstr, fcon) # R to C index conversion
close(fcon)
}
#+end_src

#+begin_src R
write_loop_index_deprecated = function() {
sink("input/out-of-state_index.txt")
for (m in 1:M)
  if(sum(dat$from==m)>0) {
cat(which(dat$from == m) - 1,"\n") # R to C index conversion
  } else cat(-99,"\n")
sink()
sink("input/person_index.txt")
for (k in 1:max(item$pid))
  if(sum(dat$id==k)>0) {
cat(which(dat$id == k) - 1,"\n") # R to C index conversion
  } else cat(-99,"\n")
sink()
}
#+end_src

#+RESULTS:

* draft
We assume a continuously observed process, recurrent actions. The process seems to be irreversible, but I need to double check. See https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2017/10/multistate_enar_webinar.pdf

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$. It may depend on covariates $\mathbf{z}(t)$, the time t itself, and possibly also the “history” of the process up to that time, $\mathbf{F}_t$: the states previously visited or the length of time spent in them.

\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.

What questions can you answer using these models? The below are quantities of interests in the multi-state survival model. All functions of the transition intensities (similar to the fact that estimable quantities are all functions of hazard rates in competing risk analysis) are estimable.

- total time that is expected to spend in state $l$ before time $t$.
- expected first passage time (first visit time to a state)
- expected number of visits to a state
- if possible, prob. of first action

In Discussion, we decided to use time-scale: in-homogeneous and number of terminal states: one.
* R packaging
#+begin_src emacs-lisp
(progn (setq default-directory "~/Dropbox/research/procmod/procmod-code/diprom")
(ess-r-devtools-clean-and-rebuild-package))
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/misc.R :results silent
cl_plot <- function(x, cl, myname = NULL, size_ = 1, xlim = NA, ylim = NA) {
  ## plot 2d array x
  ## mark points by groups specified by cl

  cl <- as.factor(cl)
  position <- as.data.frame(x)
  ndim <- dim(x)[2]

  colnames(position) <- paste("position", 1:ndim, sep = "")

  padding <- 1.05
  if (any(is.na(xlim))) {
    x1 <- -max(abs(position[, 1])) * padding
    x2 <- max(abs(position[, 1])) * padding
  } else {
    x1 <- xlim[1]
    x2 <- xlim[2]
  }
  if (any(is.na(ylim))) {
    y1 <- -max(abs(position[, 2])) * padding
    y2 <- max(abs(position[, 2])) * padding
  } else {
    y1 <- ylim[1]
    y2 <- ylim[2]
  }

  mytheme <- theme(
    axis.line = element_line(colour = "black"),
    ## panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    ## panel.border = element_blank(),
    panel.background = element_blank()
  )

  ## plot
  pp <- ggplot(position, aes(x = position1, y = position2, colour = cl)) +
    ## theme(text=element_text(size=20)) +
    ## geom_point()+
    xlim(x1, x2) +
    ylim(y1, y2) +
    xlab(myname[1]) +
    ylab(myname[2]) +
    ## xlab("Position 1") + ylab("Position 2") +
    geom_hline(yintercept = 0, color = "gray70", linetype = 2) +
    geom_vline(xintercept = 0, color = "gray70", linetype = 2) +
    mytheme
  pp <- pp + geom_point(size = size_)
  ## pp +
  ##   geom_text(
  ##     data = subset(position, idx == "w"), aes(x = position1, y = position2, label = (1:n_w), colour = cl_w),
  ##     ## segment.color = "grey50",
  ##     check_overlap = FALSE, show.legend = FALSE, size = 4
  ##   )
}
#+end_src

#+RESULTS:

* Shell script to run C-code
#+begin_src sh :tangle run.sh
#!/usr/bin/env bash
# out_dir="party_invitations-1/"
# out_dir="cd_tally/"
# out_dir="sprained_ankle-1/"
# prename="R/party_invitations-1-preprocess.R"
# initname="R/party_invitations-1-init.R"

# first argument = output dir
out_dir=$1
n_chain=2
figlet "running MCMC"

echo "================================"
echo "output dir:" $out_dir
echo "preprocessing:" $prename
echo "initializing:" $initname
echo "n_chain:" $n_chain
echo "================================"

export STAN_NUM_THREADS=2
mkdir -p output
rm output/*
# rm input/*

# Rscript $prename
cp -r input output/
cp run.sh output/
# cp $prename output/
# cp $initname output/

for ((v = 1; v <= $n_chain; v++))
do
    # Rscript $initname
    ./main $v 10000 10000 10 parallel
done
## main function argument
 # chain #, iteration, burn-in, thin
 # parallel serial -> parallel computation?

mkdir -p $out_dir
mv output/* $out_dir
# Rscript R/run-analysis.R $out_dir $n_chain
echo "Outputs are moved to" $out_dir"."
echo "================================"
#+end_src

* R post MCMC analysis
#+begin_src R :tangle R/run-analysis.R
#!/usr/bin/env Rscript
args <- commandArgs(trailingOnly = TRUE)
# test if there is at least one argument: if not, return an error

if (length(args) < 1) {
  stop("out_dir must be supplied.\n", call. = FALSE)
} else out_dir <- args[1]

if (length(args) == 2) {
num_chain <- args[2]
  } else if (length(args) == 1) {
num_chain = NULL
num_chain = get_nchain(out_dir)
  }
## out_dir <- "chessB-singleZ-singleW/"
system(paste0("rm figure/*.pdf"))
source("R/analysis.R")
system(paste0("mkdir -p ", out_dir, "figure/"))
system(paste0("rsync -rv figure/*.pdf ", out_dir, "figure/"))
#+end_src

set the output directory. run the chain of analytic tools. move the results to the output dir.
#+begin_src R :results none
num_chain = 2
out_dir="party_invitations-1/"
out_dir="cd_tally/"
## out_dir="sprained_ankle-1/"
out_dir="tickets/"

system(paste0("rm figure/*.pdf"))
system(paste0("mkdir -p figure/"))

setwd("~/workspace/procmod-code")
source("R/analysis.R")

system(paste0("mkdir -p ", out_dir, "figure/"))
system(paste0("rsync -rv figure/*.pdf ", out_dir,"figure/"))
#+end_src

#+begin_src R :results none :tangle R/Renviron.R
library(diprom)
## library(coda)
## library(dplyr)
## library(ggplot2)
## library(stringr)
## library(magrittr)
## library(bayesplot)
## library(foreach)
## library(doParallel)
stopImplicitCluster()
registerDoParallel(cores = detectCores() - 1)
## doParallel::registerDoParallel(2)
#+end_src

#+RESULTS:
#+begin_src R :tangle R/analysis.R
source("R/Renviron.R")
source("R/load-outputs.R")
source("R/summary.R")
source("R/visual.R")
#+end_src

* load outputs
This section contains scripts to create an mcmc object. 1) read MCMC samples, 2) set their column names, and 3) Procrustean matching.
#+begin_src R :tangle diprom/R/post.R
set_cnames = function(M = M_, N = N_, dq = dq_, dn = dn_, dc = dc_) {
  cnames <- c(".chain", ".iteration")

  ## CSVFormat prints by row-major order!
  for (m in 1:M) {
    cnames <- c(cnames, paste0("kappa.", m))
  }
  for (m in 1:N) {
    cnames <- c(cnames, paste0("tau.", m))
  }
  for (k in 1:N) {
    cnames <- c(cnames, paste0("theta.", k))
  }
  cnames <- c(cnames, "sigma", "beta", "mu_beta", "sigma_beta", "llike_", "lpri_", "lp_")
  return(cnames)
}

get_nchain = function(out_dir) {
  ## use all sample files if nchain is not set
  ## assign if NULL
  if (is.null(num_chain)) {
    num_chain <- length(list.files(path = out_dir, pattern="sample_chain[0-9]+\\.csv"))
  }
  return(num_chain)
}

read_output = function(num_chain, cnames) {
  dlist <- list()
  for (cid in 1:num_chain) {
    dlist[[cid]] <- readr::read_csv(paste0(out_dir, "sample_chain", cid, ".csv"), col_names = F, skip = 0) %>% as.data.frame()
    colnames(dlist[[cid]]) <- cnames
  }
  return(dlist)
}

#+end_src

#+RESULTS:

#+begin_src R :tangle R/load-outputs.R :results silent
mvar <- readr::read_csv(paste0(out_dir, "input/mvar.csv"), col_names = F) %>% as.matrix()
M = M_ = mvar[1]
N = N_ = mvar[2]
dq = dq_ = mvar[3]
dn = dn_ = mvar[4]
dc = dc_ = mvar[5]

cnames = diprom::set_cnames(M, N, dq, dn, dc)
dlist = diprom::read_output(num_chain, cnames)

## unlist
## df <- bind_rows(dlist, .id = "column_label")

mclist <- mcmc.list()
for (cid in 1:num_chain) {
  mclist[[cid]] <- mcmc(dlist[[cid]])
}
#+end_src

#+RESULTS:

The posterior means of \beta, \theta, \lambda are exported to CSV files.
#+begin_src R :tangle R/summary.R
ss <- summary(mclist)
mm <- ss$statistics[, "Mean"]
## rr <- c(grep("^kappa", names(mm)), grep("^tau", names(mm)))
## dout <- data.frame(vname = names(mm[rr]), mean = mm[rr])
dout <- data.frame(vname = names(mm), mean = mm)

readr::write_csv(dout, paste0(out_dir, "param_mean.csv"))
#+end_src

#+RESULTS:

[[file:figure/theta_res.pdf]]
[[file:figure/theta_tau_res.pdf]]
[[file:figure/tau_res.pdf]]
#+begin_src R :results silent :tangle R/summary.R
mtheta <- dout[grep("^theta", dout$vname), 2]
mtau <- dout[grep("^tau", dout$vname), 2]
mkappa <- dout[grep("^kappa", dout$vname), 2]
dat <- readr::read_csv(paste0(out_dir,"input/dat.csv"), col_names = FALSE)
rr <- dat[, c(1, 8)] %>% as.data.frame()
rr <- rr[!duplicated(rr), 2]
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/theta_res.pdf")
dd <- data.frame(theta = mtheta, res = rr)
boxp <- ggplot(dd, aes(x = factor(res), y = theta, fill = factor(res))) +
  geom_boxplot() +
  theme(legend.position = "none")
print(boxp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/tau_res.pdf")
dd <- data.frame(tau = mtau, res = rr)
boxp <- ggplot(dd, aes(x = factor(res), y = tau, fill = factor(res))) +
  geom_boxplot() +
  theme(legend.position = "none")
boxp
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/theta_tau_res.pdf")
pp = cl_plot(cbind(mtheta, log(mtau)), rr, c("theta","log(tau)"))
print(pp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results none
## sink("output/mcmc_summary.txt")
cat("==================================")
cat("Rejection Rate")
cat("==================================")
rejectionRate(mclist)
cat("==================================")
cat("Effective Size")
cat("==================================")
effectiveSize(mclist)
cat("==================================")
cat("Summary")
cat("==================================")
summary(mclist)
## sink()
#+end_src

* MCMC trace plots
#+begin_src R :tangle diprom/R/post.R
plot_intervals = function(param, trans_ = "log", save_plot = TRUE){
  regexp = paste0("^",param,"\\.")
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_intervals.pdf"))
  p0 <- bayesplot::mcmc_intervals(
    mclist,
    regex_pars = regexp,
    transformations = trans_
  )
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
  print(p0)
  if (save_plot) dev.off(which = dev.cur())
}

plot_parcoord = function(param, trans_ = "log", save_plot = TRUE){
  regexp = paste0("^",param,"\\.")
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_parcoord.pdf"))
  p0 <- bayesplot::mcmc_parcoord(
    mclist,
    regex_pars = regexp,
    transformations = trans_
  )
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
  print(p0)
  if (save_plot) dev.off(which = dev.cur())
}

#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/post.R
plot_trace = function(param, max_, trans_ = "log", save_plot = TRUE){
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_trace.pdf"))
    regexp = paste0("^",param,"\\.[0-9]$")
  for (ii in 0:floor(max_/10)) {
 if (ii > 0) regexp = paste0("^",param,"\\.", ii, "[0-9]$")
  p <- bayesplot::mcmc_trace(
    mclist,
    regex_pars = regexp,
    transformations = trans_,
    facet_args = list(nrow = 4, labeller = label_parsed)
  )
  print(p <- p + bayesplot::facet_text(size = 10))
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
 }
  if (save_plot) dev.off(which = dev.cur())
  }
#+end_src

#+RESULTS:

[[file:figure/theta_mcmc_intervals.pdf]]
[[file:figure/tau_mcmc_intervals.pdf]]
[[file:figure/theta_mcmc_intervals.pdf]]
#+begin_src R :tangle R/visual.R
diprom::plot_intervals("kappa")
diprom::plot_intervals("tau")
diprom::plot_intervals("theta", list())
#+end_src

#+RESULTS:
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'

[[file:figure/theta_mcmc_parcoord.pdf]]
[[file:figure/tau_mcmc_parcoord.pdf]]
[[file:figure/theta_mcmc_parcoord.pdf]]
#+begin_src R
diprom::plot_parcoord("kappa")
diprom::plot_parcoord("tau")
diprom::plot_parcoord("theta", list())
#+end_src

#+RESULTS:
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'

[[file:figure/kappa_mcmc_trace.pdf]]
[[file:figure/tau_mcmc_trace.pdf]]
[[file:figure/theta_mcmc_trace.pdf]]
#+begin_src R :tangle R/visual.R
bayesplot::color_scheme_set("mix-blue-pink")
diprom::plot_trace("kappa", M)
diprom::plot_trace("tau", N)
diprom::plot_trace("theta", 20, list())
#+end_src

#+RESULTS:
: Error in facet_text(size = 10) : could not find function "facet_text"
: Error in facet_text(size = 10) : could not find function "facet_text"
: Error in facet_text(size = 10) : could not find function "facet_text"

[[file:figure/other_params_trace.pdf]]
#+begin_src R :tangle R/visual.R
pdf("figure/other_params_trace.pdf")
bayesplot::color_scheme_set("mix-blue-pink")
p <- bayesplot::mcmc_trace(mclist[[1]],
                           pars = c("sigma", "beta", "mu_beta", "sigma_beta", "llike_", "lpri_", "lp_"),
                           facet_args = list(nrow = 3, labeller = label_parsed)
                           )
print(p <- p + bayesplot::facet_text(size = 10))
dev.off(which = dev.cur())
#+end_src

#+RESULTS:
: pdf
:   2

* word2vec code
** COMMENT email transitions
#+begin_src R
library(dplyr)
load('data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
email = PS[PS$CODEBOOK == "U01a000S",]

## only core event description included
## core_event = c("MAIL_DRAG", "MAIL_VIEWED", "FOLDER_VIEWED", "MAIL_MOVED")
core_event = c("MAIL_DRAG", "MAIL_MOVED")
email$event_description[!(email$event_type %in% core_event)] <- ""

email$action = paste(email$event_type, email$event_description)
email$action[email$event_num == 1] = "START"
email$action[email$action == "END END"] = "END"
## email$action = email$event_type
ee = email %>% select(SEQID, event_type, event_description)
#+end_src

#+begin_src R
n = nrow(email)
trans = paste(email$action[1:(n-1)], "->", email$action[2:n])
trans = trans[trans != "END -> START"]
#+end_src

#+RESULTS:

#+begin_src R
aa = email
aa = aa %>% filter(event_description != "") %>% select(SEQID,event_type,event_description)
#+end_src

#+RESULTS:

#+begin_src R
length(unique(email$event_type))
length(unique(email$action))
length(unique(trans))
#+end_src

#+RESULTS:
: [1] 51
: [1] 109
: [1] 752

: [1] 1774

#+begin_src R
tab = table(trans)
ntab = as.numeric(tab)
summary(ntab)
sum(ntab > 1)
#+end_src

#+RESULTS:
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
:     1.0     1.0     2.0    45.1     9.0  3932.0
: [1] 479

#+begin_src R
toend = grepl("END$",trans)
nonext = grepl("NEXT\\_INQUIRY REQUEST ->",trans)
trans[toend & !nonext]
#+end_src

#+RESULTS:
: character(0)

#+begin_src R
texton = grepl("TEXTBOX_ONFOCUS", trans)
textoff = grepl("TEXTBOX_KILLFOCUS", trans)
#+end_src

#+RESULTS:

#+begin_src R
edes = email$event_description
cfolder = grepl("createfoldernameinput", edes)
email$SEQID[cfolder]
#+end_src

#+RESULTS:
: numeric(0)

#+begin_src R :tangle no
email %>% filter(SEQID == 4444) %>% select(event_type, event_description)
#+end_src

#+begin_src R :tangle no
email %>% filter(SEQID == 782) %>% select(event_type, event_description)
#+end_src

#+RESULTS:
#+begin_example
           event_type event_description
1               START
2         MAIL_VIEWED
3         MAIL_VIEWED
4         MAIL_VIEWED
5         MAIL_VIEWED
6         MAIL_VIEWED
7         MAIL_VIEWED
8         MAIL_VIEWED
9         MAIL_VIEWED
10        MAIL_VIEWED
11      FOLDER_VIEWED
12               MENU
13 MENUITEM_newfolder
14      FOLDER_VIEWED
15    TEXTBOX_ONFOCUS
16           KEYPRESS
17  TEXTBOX_KILLFOCUS
18      ADD_FOLDER_ok
19        MAIL_VIEWED
20       TOOLBAR_help
21  TOOLBAR_replymail
22        MAIL_VIEWED
23        MAIL_VIEWED
24        MAIL_VIEWED
25        MAIL_VIEWED
26               MENU
27    MENUITEM_delete
28        MAIL_VIEWED
29               MENU
30    MENUITEM_delete
31    TOOLBAR_mailApp
32      FOLDER_VIEWED
33               MENU
34 MENUITEM_newfolder
35      FOLDER_VIEWED
36      ADD_FOLDER_ok
37       NEXT_INQUIRY
38                END
#+end_example
** COMMENT Keras word2vec
:PROPERTIES:
:header-args:R: :results silent :exports both :noweb yes :eval never-export
:END:

see https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/

#+begin_src emacs-lisp
;; python
(require 'conda)
(conda-env-activate "tf")
#+end_src

#+RESULTS:
: Switched to conda environment: /Users/yunj/.conda/envs/r-tensorflow/

#+begin_src R :tangle word2vec.R
library(readr)
library(stringr)
reviews <- read_lines("finefoods.txt.gz", n_max = 100)
reviews <- reviews[str_sub(reviews, 1, 12) == "review/text:"]
reviews <- str_sub(reviews, start = 14)
reviews <- iconv(reviews, to = "UTF-8")
#+end_src

#+begin_src R :tangle word2vec.R
library(keras)
tokenizer <- text_tokenizer(num_words = 200)
tokenizer %>% fit_text_tokenizer(reviews)
#+end_src

#+begin_src R :tangle word2vec.R
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
  gen <- texts_to_sequences_generator(tokenizer, sample(text))
  function() {
    skip <- generator_next(gen) %>%
      skipgrams(
        vocabulary_size = tokenizer$num_words,
        window_size = window_size,
        negative_samples = 0
      )
    x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
    y <- skip$labels %>% as.matrix(ncol = 1)
    list(x, y)
  }
}
#+end_src

#+begin_src R :tangle word2vec.R
## embedding_size <- 128  # Dimension of the embedding vector.
embedding_size <- 8  # Dimension of the embedding vector.
skip_window <- 1       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
#+end_src

#+begin_src R :tangle word2vec.R
embedding <- layer_embedding(
  input_dim = tokenizer$num_words + 1,
  output_dim = embedding_size,
  input_length = 1,
  name = "embedding"
)

target_vector <- input_target %>%
  embedding() %>%
  layer_flatten()

context_vector <- input_context %>%
  embedding() %>%
  layer_flatten()

dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")

#+end_src

#+begin_src R :tangle word2vec.R


model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
summary(model)

#+end_src


#+begin_src R :tangle word2vec.R
model %>%
  fit_generator(
    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples),
    ## steps_per_epoch = 100000, epochs = 5
    steps_per_epoch = 100, epochs = 5
    )
#+end_src

** email word2vec preprocessing
:PROPERTIES:
:header-args:R: :results silent :tangle email_word2vec_preproc.r :eval never-export
:END:
goal: process actions to be used for =word2vec=
- [-] remove =event_description= with no information
- [-] remove *consecutive* action repetition.
- [X] concat =event_type= and =event_description=
- [X] substitue SPACE with "_"
- [X] concat actions into a sequence
- [X] remove START and END.
- [X] write sequences to a txt file

  #+begin_src R :tangle no
source("email_word2vec_preproc.r")
  #+end_src

- keep a small number of event_description that are highly relevant to movement of emails.
- remove action taking < 50ms ([[https://www.tobiipro.com/learn-and-support/learn/eye-tracking-essentials/how-fast-is-human-perception/][How fast is human perception? About 80 ms | Find out more]])
#+begin_src R
library(dplyr)
library(stringr)
setwd('~/workspace/procmod-code/')
load('./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata')
email = PS %>% filter(CODEBOOK == "U01a000S")

## core_event = c("MAIL_DRAG", "MAIL_MOVED")
core_event = c("MAIL_DRAG", "MAIL_MOVED", "MAIL_VIEWED")
## core_event = c("MAIL_MOVED")
email$event_description[!(email$event_type %in% core_event)] <- ""
## email$event_description <- ""

timestamp = email$timestamp
diff = c(diff(timestamp), 99999)
email$diff = diff
email = email[(diff > 99) || (diff < 0 ), ]

email$event_description = stringr::str_replace(email$event_description, "(.*)\\*\\$target=u01a_(.*)",  "\\1\\2")
email$event_description = stringr::str_replace(email$event_description, "id=u01a_",  "")
email$event_description = stringr::str_replace(email$event_description, "\\*\\$target=",  "")

email = email %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

we need to remove transition between the same action
#+begin_src R
ww = email$word
id = email$SEQID
ww0 = ww[1:(length(ww)-1)]
ww1 = ww[2:(length(ww))]

dup = ww0 == ww1
dup0 = c(dup,  FALSE )
dup1 = c( FALSE ,  dup)

idx = dup1

cw = "NULL"
cid = 9999999999
for (kk in which(dup1)) {
  if(cw != ww[kk] && cid != id[kk]) idx[kk] = FALSE
  cw = ww[kk]
  cid = id[kk]
}

email = email[!idx, ]
#+end_src


#+begin_src R
n = nrow(email)
pre = email$word[1:(n-1)]
cur = email$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

email = email[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(email$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in email$word[email$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(email$word[email$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

this leave only one action in some cases... let's keep start and end
#+begin_src R
## for (i in id) {
##     seqs[i] = gsub("START ", "", seqs[i])
##     seqs[i] = gsub(" END", "", seqs[i])
## }
seqs = seqs[id]
#+end_src

#+RESULTS:


#+begin_src R
data.table::fwrite(as.data.frame(seqs), "email_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R :tangle no
vv = numeric(max(id))
for (i in id) {
  vv[i] =length(email$word[email$SEQID == i])
  }

length(unique(email$word))
n = nrow(email)
trans = paste(email$word[1:(n-1)], "->", email$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
90 * 89 / 2

ee = PS %>% filter(CODEBOOK == "U01a000S")
ww = paste(ee$event_type, ee$event_description)
length(unique(ww))
nn = nrow(ee)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src
** email word2vec
window_size = 2 should be [5,20] for a small data set.
(negative samples) num_ns = 4
See [[file:email_word2vec.ipynb]]
Download the vectors.tsv and metadata.tsv to analyze the obtained embeddings in the Embedding Projector. https://projector.tensorflow.org/
* email subject segmentation
#+begin_src R :results silent
source("email_word2vec_preproc.r")
#+end_src

#+begin_src R :results silent
vv = readr::read_tsv("vectors.tsv")
mm = readr::read_tsv("metadata.tsv")

ww = email$word
#+end_src

#+begin_src R
myvv = matrix(0, length(id), ncol(vv))
mymm=numeric(length(id))

idx =0
for (ii in id) {
  idx=idx+1
  ss=ww[email$SEQID==ii]
  ss=ss[ss != "START"]
  ss=ss[ss != "END  "]
 ss=ss[ss != "END_CANCEL"]
  ## ss=ss[ss != "MENUITEM_help"]
ee = numeric(length(ss))
  for (kk in 1:length(ss)) {
    ee[kk]= which(ss[kk] == mm)
  }
  myvv[idx,]=colMeans(vv[ee,])
  mymm[idx]=email$response[email$SEQID==ii][1]
}

readr::write_tsv(as.data.frame(mymm),"mymm.tsv", col_names=F)
readr::write_tsv(as.data.frame(myvv),"myvv.tsv", col_names=F)
#+end_src

#+RESULTS:

- I didn't remove START and END.
- those with small event duration are deleted.
- centroid of action sequence is calculated and used for the clustering.
- timestamp information is not used for clustring
TSNE can identify many subect groups. wouldn't that be useful for futher analysis?
* Football ticket

#+begin_src R
library(dplyr)
library(stringr)
load('./data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
df = PS %>% filter(CODEBOOK == "U21x000S")

ww = paste(df$event_type, ee$event_description)
length(unique(ww))
nn = nrow(df)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src

#+begin_src R
core_event = c("COMBOBOX", "TAB", "CHECKBOX")
## core_event = c("MAIL_MOVED")
df$event_description[!(df$event_type %in% core_event)] <- ""
timestamp = df$timestamp
diff = c(0, diff(timestamp))
df = df[diff > 50, ]

df$event_description = stringr::str_replace_all(df$event_description, "\\*\\$index=", "")
df$event_description = stringr::str_replace_all(df$event_description, "u021_(.*?)_","")
df$event_description = stringr::str_replace(df$event_description, "id=",  "")
df$event_description = stringr::str_replace(df$event_description, "button",  "")

df = df %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

#+begin_src R
n = nrow(df)
pre = df$word[1:(n-1)]
cur = df$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

df = df[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(df$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in df$word[df$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(df$word[df$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

#+begin_src R
for (i in id) {
    seqs[i] = gsub("START ", "", seqs[i])
    seqs[i] = gsub(" END", "", seqs[i])
}
seqs = seqs[id]
#+end_src

#+RESULTS:


#+begin_src R
data.table::fwrite(as.data.frame(seqs), "ticket_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R
mm = 0
for (i in id) {
  mm  = max(mm , length(df$word[df$SEQID == i]))
  }
mm
length(unique(df$word))
n = nrow(df)
trans = paste(df$word[1:(n-1)], "->", df$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
#+end_src
* CD Tally :ATTACH:
:PROPERTIES:
:header-args:R: :results silent :session *R-PIACC* :exports both :noweb yes :eval never-export
:ID:       673df774-623f-44b5-a262-b22739c9a506
:END:

#+attr_org: :width 700
[[attachment:_20210426_063856screenshot.png]]

[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/CDTallyPart1/pages/cd-start.html][CD Tally Part 1]]
- look at the spreadsheet and calculate the value

#+begin_src R
library(dplyr)
library(stringr)
load('./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_cleaned_1110.rdata')
df = PS %>% filter(CODEBOOK == "U03a000S")

ww = paste(df$event_type, df$event_description)
length(unique(ww))
nn = nrow(df)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src

#+begin_src R
core_event = c("COMBOBOX", "TAB", "CHECKBOX")
## core_event = c("MAIL_MOVED")
df$event_description[!(df$event_type %in% core_event)] <- ""
timestamp = df$timestamp
diff = c(diff(timestamp), 9999)
df = df[diff > 99, ]

df$event_description = stringr::str_replace_all(df$event_description, "\\*\\$index=", "")
df$event_description = stringr::str_replace_all(df$event_description, "u021_(.*?)_","")
df$event_description = stringr::str_replace(df$event_description, "id=",  "")
df$event_description = stringr::str_replace(df$event_description, "button",  "")

df = df %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

#+begin_src R
n = nrow(df)
pre = df$word[1:(n-1)]
cur = df$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

df = df[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(df$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in df$word[df$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(df$word[df$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

#+begin_src R
for (i in id) {
    seqs[i] = gsub("START ", "", seqs[i])
    seqs[i] = gsub(" END", "", seqs[i])
}
seqs = seqs[id]
#+end_src

#+RESULTS:

#+begin_src R
data.table::fwrite(as.data.frame(seqs), "ticket_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R
mm = 0
for (i in id) {
  mm = max(mm, length(df$word[df$SEQID == i]))
  }
mm
length(unique(df$word))
n = nrow(df)
trans = paste(df$word[1:(n-1)], "->", df$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
#+end_src
* COMMENT Local Variables
# Local Variables:
# org-babel-default-header-args:R: ((:session . "*R-PIACC*") (:export . "both") (:results . "output replace"))
# eval: (flyspell-mode -1)
# eval: (spell-fu-mode -1)
# End:
