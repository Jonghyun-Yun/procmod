#+title: PIACC data processing

#+hugo_base_dir: ./docs
#+hugo_front_matter_format: toml
#+hugo_level_offset: 0

# https://orgmode.org/manual/Export-Settings.html#Export-Settings
#+options: H:10 num:nil toc:t \n:nil @:t ::t |:t ^:nil ^:{} -:t f:t *:t <:t ':nil -:nil pri:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:nil

#+startup: overview noinlineimages logdone indent

#+latex_class: article
#+latex_class_options: [letterpaper,11pt]

#+latex_compiler: pdflatex

# comment out for reveal.js
# #+setupfile: ~/setup/my-theme-readtheorg.setup
#+setupfile: ~/org/latex_header.setup
#+setupfile: ~/org/orgmode_header.setup

#+property: header-args :eval never-export
#+property: header-args:ein :session localhost
#+property: header-args:jupyter-python :session *jupyter-piacc* :kernel tf

#+begin_src emacs-lisp :results silent
  (jyun/org-latex-set-options 2.5)
#+end_src

* discussion
- [2021-05-09 Sun] note
  I drop event description and tryed a few more embeddings.
it seems incorrect process actions have low cosine similarity. This means they are difficult to form a semantic cluster(s).
actions associated to the task (or problem solving) has high similarity. so as long as event description reoains the same, I can set the subsequence as a new action. embedding of a new action is determined by the mean (center) embeddings of its members.
** party invitation I :noexport:ATTACH:
:PROPERTIES:
:ID:       468ae777-6949-4e3f-9521-fa63037fdf73
:END:
#+caption: Email invitation
#+attr_org: :width 500
[[attachment:_20210426_142403screenshot.png]]
[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/PartyInvitations1/pages/pi-start.html][Party Invitations Part 1]]

#+begin_src R
source("email_word2vec_preproc.r")
email %>% filter(SEQID == 101) %>% select(event_type,  event_description, timestamp, diff)
#+end_src

#+RESULTS:
#+begin_example
       event_type        event_description timestamp   diff
 1:         START                                  0  32679
 2:     MAIL_DRAG                  item101     32679   2784
 3:   MAIL_VIEWED                  item104     35463      4
 4: FOLDER_VIEWED            CanComeFolder     35467     70
 5:    MAIL_MOVED    item101|CanComeFolder     35537   5294
 6:   MAIL_VIEWED                  item102     40831   4551
 7:     MAIL_DRAG                  item102     45382   1912
 8:   MAIL_VIEWED                  item105     47294      4
 9: FOLDER_VIEWED         CannotComeFolder     47298     56
10:    MAIL_MOVED item102|CannotComeFolder     47354   5049
11:   MAIL_VIEWED                  item104     52403   3589
12:     MAIL_DRAG                  item104     55992   1542
13:   MAIL_VIEWED                  item105     57534      4
14: FOLDER_VIEWED            CanComeFolder     57538     50
15:    MAIL_MOVED    item104|CanComeFolder     57588   4722
16:   MAIL_VIEWED                  item105     62310   6690
17:   MAIL_VIEWED                  item103     69000   4563
18: FOLDER_VIEWED            CanComeFolder     73563   3468
19:   MAIL_VIEWED                  item101     77031   9667
20:  NEXT_INQUIRY                              86698   5229
21:           END                              91927 -91927
       event_type        event_description timestamp   diff
#+end_example
** party invitation II :ATTACH:
:PROPERTIES:
:ID:       b782e36e-45f5-40c7-947c-a3f668b53610
:END:

#+attr_org: :width 500
[[attachment:_20210903_105241screenshot.png]]

** [[mu4e:msgid:CAC72WzTzKpZBEHrobD4TFkHZrUHqVu6NR24qsHOYcDZ-dHFP=g@mail.gmail.com][Re: Questions about Data]] :noexport:
- initial location
  + inbox: 100s
  + can come: 200s
  + cannot come: 301

- response = (0 - 3)
  + can come: 101, 104
  + cannot come: 102
  + can come: 201, 202 (don't move, or move but put back(e.g ~TOOLBAR_trash~))
  + anywhere: 103, 105
** to sequence
- discard actions being taken for very short time (100ms)
- otherwise embeddings doesn't help much
#+begin_src R
seqs[4]
#+end_src

#+RESULTS:
: [1] "MAIL_DRAG-item101 MAIL_MOVED-item101|CanComeFolder MAIL_VIEWED-item102 MAIL_DRAG-item102 MAIL_MOVED-item102|CannotComeFolder MAIL_VIEWED-item104 MAIL_DRAG-item104 MAIL_MOVED-item104|CanComeFolder MAIL_VIEWED-item105 MAIL_VIEWED-item103 FOLDER_VIEWED-CanComeFolder MAIL_VIEWED-item101 NEXT_INQUIRY"
** [[id:1b2724b9-324b-463b-967a-3b064a0f115e][Word2Vec]] :noexport:
:PROPERTIES:
:ID:       9aad4ce5-7a81-436c-9f59-155d4cd47279
:END:
https://www.tensorflow.org/tutorials/text/word2vec
- Words that tend to “behave similarly” end up close to one another in the embedding space. Instead of using the word symbol as a feature in the model, we can use its vector, which exploits such similarities.

\[
  p\left(w_{j} \mid w_{0}, u, v\right)=\frac{\exp \left(u\left(w_{0}\right)^{\top} v\left(w_{j}\right)\right)}{\sum_{w \in V} \exp \left(u\left(w_{0}\right)^{\top} v(w)\right)}
\]
where \(u: V \rightarrow \mathbb{R}^{k} \) and \(v: V \rightarrow \mathbb{R}^{k}\) are functions which map words to a word embedding—one for the pivot words, and the other for context.
** transition models
- action based transition: >= 500
- embeddings based transition (~embedding_dim~)

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$. It may depend on covariates $\mathbf{z}(t)$, the time t itself, and possibly also the “history” of the process up to that time, $\mathbf{F}_t$: the states previously visited or the length of time spent in them.
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta} d_{i,m,l} ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.
** ticket :noexport:ATTACH:
:PROPERTIES:
:ID:       a183bf8a-10f9-4ccb-861b-9658d5b2b9f5
:header-args:R: :results silent :session *R-PIACC* :exports both :noweb yes :eval never-export
:END:
#+attr_org: :width 300
[[attachment:_20210426_040805screenshot.png]]

[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/FootballTickets/pages/ft-home.html][Football Tickets]]
- calendar: TAB-id=tabbutton2
- ticketing: TAB-id=tabbutton1
- event type: COMBOBOX-id=u021_default_menu1|*$index=7 (football)
- location: COMBOBOX-id=u021_default_menu2|*$index=2 (Bakerton)
- response = 1 (correct) 7 (incorrect) 0 missing
  menu1 and 6 = 9 (8 seems ok too)
- N = 1344
  src_R[:session *R-PIACC* :exports results]{length(unique(df$SEQID))}

#+transclude: t
[[id:673df774-623f-44b5-a262-b22739c9a506][CD Tally]]

* multi-state model
- https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2017/10/multistate_enar_webinar.pdf
- https://cran.r-project.org/web/packages/msm/msm.pdf
- [[id:57b08111-9c48-4fa9-b289-10f01fc7a0d6][cite:hill_relaxing_2021]]
* piacc preproc
#+begin_src emacs-lisp
(delete-directory "input" t)
(make-directory "input")
#+end_src
** background info join? :ARCHIVE:
#+begin_src R
piacc_orig_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_cleaned_1110.rdata"
load(piacc_orig_path)
item = PS
b2012 = readr::read_csv("./data/PIAAC_cleaned_data_1110/Prgusap1_2012.csv")
b2017 = readr::read_csv("./data/PIAAC_cleaned_data_1110/Prgusap1_2017.csv")
## b2012 = foreign::read.spss("./data/PIAAC_cleaned_data_1110/Prgusap1_2012.sav", to.data.frame = T)
## b2017 = foreign::read.spss("./data/PIAAC_cleaned_data_1110/Prgusap1_2017.sav", to.data.frame = T)
iid = unique(item$SEQID)
b7id = unique(b2017$SEQID)
b2id = unique(b2012$SEQID)

kk = 0
for (id in iid) {
  if (any(id == b2id) && any(id == b7id)) error("duplicated id")
  kk = kk + sum(id == b2id)
  kk = kk + sum(id == b7id)
    }

jj = 0
for (ii in names(b2017)) {
  if (any(ii == names(b2012))) jj = jj +1
    }
#+end_src

** all-in-one R script
:PROPERTIES:
:header-args:R: :tangle R/preproc-data.R
:END:

- preprocess data, word2vec, prepare for C, run C, post analysis!
#+begin_src R :tangle R/all_in_one.R
ipath = "input"
if (dir.exists(paths)) {
  do.call(file.remove, list(list.files("input", full.names = TRUE)))
  } else dir.create(ipath)
source("R/itemcode.R")
out_dir = paste0(booklet$NAME[booklet$CODEBOOK == item_code], "/")
system(paste0("figlet ", item_code))
system(paste0("figlet ", out_dir))
source("R/preproc-data.R")
system(paste0("sh run.sh ", out_dir))
#+end_src

#+begin_src R :tangle R/run_all_in_one.R
## item_code = "U04a000S" ## cannot convert to long format (msm)
## item_code = "U01b000S"

item_code = "U19a000S"
source("R/all_in_one.R")

item_code = "U07x000S"
source("R/all_in_one.R")

item_code = "U02x000S"
source("R/all_in_one.R")

item_code = "U16x000S"
source("R/all_in_one.R")

item_code = "U11b000S"
source("R/all_in_one.R")

item_code = "U23x000S"
source("R/all_in_one.R")

item_code = "U06a000S"
source("R/all_in_one.R")
#+end_src

#+begin_src R :results value list drawer silent
library(diprom)
## library(dplyr)
## library(stringr)
## library(msm)
## library(foreach)
## library(doParallel)
stopImplicitCluster()
registerDoParallel(cores = detectCores() - 1)
## doParallel::registerDoParallel(2)
#+end_src

#+begin_src R :results value list drawer silent
setwd('~/workspace/procmod-code/')
piacc_orig_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_cleaned_1110.rdata"
piacc_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata"
## piacc_na.omit()
piacc_background_path = "./data/PIAAC_cleaned_data_1110/Prgusap1_2017.csv"
piacc_background_spss = "./data/PIAAC_cleaned_data_1110/Prgusap1_2017.sav"
## xxx = foreign::read.spss("./data/PIAAC_cleaned_data_1110/Prgusap1_2017.sav")
#+end_src

#+begin_src R :results value list drawer silent
item = read_piacc(piacc_path, item_code, sub_str, ignore_str)
item2sen(item)
item %>% group_by(SEQID) %>% summarise(mnum = max(event_num))
#+end_src

Run Python codes to do ~word2vec~.
#+begin_src R :results silent
system(". activate tf; python piacc_word2vec.py")
#+end_src

#+begin_src R :results silent
metadata_path = "input/metadata.tsv"
vectors_path = "input/vectors.tsv"
metadata = readr::read_tsv(metadata_path, col_names = FALSE)
vectors = readr::read_tsv(vectors_path, col_names = FALSE)

item = filter_item(item)
Q = get_trans(item)
Dml = get_cosdist()
#+end_src

#+begin_src R :results none
dat = item2long(item)

M = length(state)
N = length(unique(dat$id))
dq = nrow(Q)
dn = nrow(dat)
dc = ncol(dat)
#+end_src

#+begin_src R :results none
write_data()
source("R/init.R")
write_loop_index()
#+end_src

#+RESULTS:

** internal functions
#+begin_src R :tangle diprom/R/preproc.R :results value list drawer silent
read_piacc = function(piacc_path, item_code, sub_str, ignore_str, core_event = NULL) {
  load(piacc_path)
  item = PS %>% filter(CODEBOOK == item_code)

## core_event = c("MAIL_DRAG", "MAIL_MOVED", "MAIL_VIEWED")
## email$event_description[!(email$event_type %in% core_event)] <- ""

  timestamp = item$timestamp
  diff = c(diff(timestamp), 99999)
  item$diff = diff
  item = item[(diff >= 10) || (diff < 0 ), ]

  if (length(sub_str) != 0) {
    for (ii in 1:nrow(sub_str)){
      item$event_description = stringr::str_replace_all(item$event_description,
                                                    sub_str[ii,1], sub_str[ii,2])}
  }

  if (length(ignore_desc) != 0) {
    for (ii in 1:length(ignore_desc)) {
      item = item %>% mutate(event_description = ifelse(event_type == ignore_desc[ii],"",event_description)) }
  }

  item = item %>%
    mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
    mutate(word = gsub(" ", "_", event_concat))

  return(item)
}
#+end_src

#+begin_src R :tangle diprom/R/preproc.R
item2sen = function(item) {
ww = item$word
id = item$SEQID
ww0 = ww[1:(length(ww)-1)]
ww1 = ww[2:(length(ww))]

dup = ww0 == ww1
dup0 = c(dup,  FALSE )
dup1 = c(FALSE ,  dup)

idx = dup1

cw = "NULL"
cid = 9999999999
for (kk in which(dup1)) {
  if(cw != ww[kk] && cid != id[kk]) idx[kk] = FALSE
  cw = ww[kk]
  cid = id[kk]
}

item = item[!idx, ]

n = nrow(item)
pre = item$word[1:(n-1)]
cur = item$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

item = item[!idx,]

id = unique(item$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in item$word[item$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(item$word[item$SEQID == i] , collapse = " ")
}

## for (i in id) {
##     seqs[i] = gsub("START ", "", seqs[i])
##     seqs[i] = gsub(" END", "", seqs[i])
## }
seqs = seqs[id]

data.table::fwrite(as.data.frame(seqs), "input/item_sentence.txt", col.names=F)
}
#+end_src

#+begin_src R :tangle diprom/R/preproc.R
filter_item = function(item) {
  ## a few more processing
  ## state to global env
item =  item %>% filter(word !="START")
state <<- unique(item$word)
ntate = numeric(nrow(item))
ii = 0
for (ww in item$word) {
  ii = ii + 1
  ntate[ii] = which(state == ww)
}
item$state = ntate

uid = unique(item$SEQID)
pid = numeric(nrow(item))
ii = 0
for (nn in item$SEQID) {
  ii = ii + 1
  pid[ii] = which(uid == nn)
  }
item$pid = pid
return(item)}

get_trans = function(item) {
id = unique(item$SEQID)
L = length(state <<- unique(item$word))
Q = matrix(0, L, L)
for (i in id) {
  ## for (word in item$word[item$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seq = item$state[item$SEQID == i]
  for (k in 1:(length(seq)-1))
    Q[seq[k], seq[k+1]] = 1
  ## cat(seq[length(seq)-1])
}

rsq = rowSums(Q) > 0
diag(Q) = 1 * rsq

return(Q)}
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
get_cosdist = function() {
dem = ncol(vectors)
vv = array(0, dim=c(M <- length(state), dem))
for (m in 1:M) {
  if (sum(state[m] == metadata[,1]) > 0)
  vv[m, ] = unlist(vectors[which(state[m] == metadata[,1]), ] )
}
Dml = cosdist(as.matrix(vv))

return(Dml)
}
#+end_src

#+RESULTS:

** write data
:PROPERTIES:
:header-args:R: :tangle R/write-data.R
:END:
#+begin_src R :results none :tangle diprom/R/preproc.R
item2long = function(item) {
df = item %>% select(pid, timestamp, response, state)
dat <- msm::msm2Surv(data=df, subject="pid", time="timestamp", state="state",
         Q=Q)
##dat
## attr(dat, "trans")
dat$time = dat$time / max(dat$Tstop)
dat$Tstart = dat$Tstart / max(dat$Tstop)
dat$Tstop = dat$Tstop / max(dat$Tstop)
dat$dist = foreach (i = 1:nrow(dat), .combine="c") %dopar%
  Dml[dat$from[i], dat$to[i]]
return(dat)
}
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
write_data = function(){
## R to C index conversion
wat = dat
wat$id = wat$id - 1
wat$from = wat$from - 1
wat$to = wat$to - 1

## to be read from cpp
readr::write_csv(as.data.frame(item),"input/item.csv", col_names = TRUE)
readr::write_csv(as.data.frame(state),"input/state.csv", col_names = FALSE)
readr::write_csv(wat,"input/dat.csv", col_names = FALSE)
readr::write_csv(as.data.frame(Q),"input/trans.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(M, N, dq, dn, dc)),"input/mvar.csv", col_names = FALSE)
readr::write_csv(data.frame(Dml), "input/Dml.csv", col_names = FALSE)
}
#+end_src

** background_V1
:PROPERTIES:
:header-args:R: :tangle R/background.R
:END:
#+begin_src R :results silent
binfo = readr::read_csv("./data/PIAAC_cleaned_data_1110/Prgusap1_2012.csv")
## binfo = foreign::read.spss("./data/PIAAC_cleaned_data_1110/Prgusap1_2012.sav")
item = readr::read_csv("input/item.csv")
pal = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUFs_values.csv", col_types = readr::cols(.default = readr::col_character()))
#+end_src

#+begin_src R
pal[,1] = pal[,1] %>% unlist %>% toupper
vname = unique(pal[,1]) %>% unlist
bname = names(binfo)
SAS = pal[,3] %>% unlist %>% str_replace_all("\\.", "")
SPSS = pal[,4] %>% unlist
#+end_src

#+RESULTS:

ii = 827
LNG_BQ

#+begin_src R
for (ii in 1:length(vname)) {
  if (any(vname[ii] == bname)) {
    binfo[,vname[ii]] = plyr::mapvalues(unlist(binfo[,vname[ii]]), SAS[pal[,1] == vname[ii]], SPSS[pal[,1] == vname[ii]])
    ## binfo[is.na(binfo[,vname[ii]]),vname[ii]] = "999999999999"
    binfo[,vname[ii]] = binfo[,vname[ii]] %>% mutate_if(is.character,as.factor) %>% mutate_if(is.factor, as.numeric)
    binfo[is.na(binfo[,vname[ii]]),vname[ii]] = 999999999999
}}
binfo = data.frame(SEQID = binfo$SEQID, binfo[, names(binfo) %in% vname])
#+end_src

#+begin_src R
binfo[is.na(binfo)] = 999999999999
readr::write_csv(binfo, "./data/PIAAC_cleaned_data_1110/PUFs_spss.csv")

for (ii in 6:ncol(binfo)) {
## binfo[,ii] = plyr::mapvalues(unlist(binfo[,ii]), "N", 9^50) %>% as.numeric
binfo[is.na(binfo[,ii]),ii] = -9^50
}
readr::write_csv(binfo, "./data/PIAAC_cleaned_data_1110/PUFs_noN.csv")
#+end_src

** background :ARCHIVE:
:PROPERTIES:
:header-args:R: 
:END:
#+begin_src R :results silent
## binfo = readr::read_csv("./data/PIAAC_cleaned_data_1110/Prgusap1_2012.csv")
binfo = foreign::read.spss("./data/PIAAC_cleaned_data_1110/Prgusap1_2012.sav", to.data.frame = TRUE) %>% as.data.frame
item = readr::read_csv("input/item.csv")
var = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUF_Variables.csv")
pal = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUFs_values.csv", col_types = readr::cols(.default = readr::col_character()))
#+end_src

#+begin_src R
pal[,1] = pal[,1] %>% unlist %>% toupper
vname = unique(pal[,1]) %>% unlist
SAS = pal[,3] %>% unlist %>% str_replace_all("\\.", "")
SPSS = pal[,4] %>% unlist
bname = names(binfo)
vfact = pal %>% group_by(pal[,1]) %>% summarise(n = n())
#+end_src

#+RESULTS:
: `summarise()` ungrouping output (override with `.groups` argument)

#+begin_src R
plist = list()
for (ii in unique(unlist(pal[,1]))) {
  tmp = pal[pal[,1] == ii,]
  plist[[ii]] = data.frame(Value = tmp[,2] ,SAS = tmp[,3], SPSS = tmp[,4], Type = tmp[,5])
  }
#+end_src

#+RESULTS:

ii = 827
LNG_BQ

#+begin_src R :resutls silent
pinfo = data.frame(SEQID = binfo$SEQID)
for (ii in vname) {
  if (any(ii == bname)) {
    if (vfact[vfact[,1] == ii,2] < 4) {
    pinfo[,ii] = as.character(binfo[,ii])
    pinfo[,ii] = plyr::mapvalues(unlist(pinfo[,ii]), plist[[ii]][,1], plist[[ii]][,3])
    pinfo[is.na(pinfo[,ii]),ii] = "999999999999"
    ## binfo[,vname[ii]] = binfo[,vname[ii]] %>% mutate_if(is.character,as.factor) %>% mutate_if(is.factor, as.numeric)
    pinfo[,ii] = as.numeric(pinfo[,ii])
    }
else 
    pinfo[,ii] = as.numeric(binfo[,ii])
    pinfo[is.na(pinfo[,ii]),ii] = 999999999999
  }}
#+end_src

#+begin_src R
readr::write_csv(pinfo, "./data/PIAAC_cleaned_data_1110/PUFs_spss.csv")
#+end_src

#+RESULTS:

** init
:PROPERTIES:
:header-args:R:
:END:
#+begin_src R :tangle R/init.R
init_val = list(
kappa = rep(0.1, M),
tau = rep(0.1, N),
theta = rep(0.1, N),
beta = 0,
sigma = 1,
mu_beta = 0,
sigma_beta = 1
)
#+end_src

#+RESULTS:

#+begin_src R :tangle R/init.R
## lambda
params = c(init_val, list(
a_kappa = matrix(0.1,M,1),
b_kappa = matrix(0.1,M,1),
jump_kappa = matrix(0.5,M,1),

jump_beta = matrix(0.1,1,1),

## mu_theta = matrix(0.0,N,1)
## sigma_theta = matrix(1.0,N,1)
jump_theta = matrix(1.0,N,1),

a_sigma = 1.0,
b_sigma = 1.0,

a_tau = matrix(0.1,N,1),
b_tau = matrix(0.1,N,1),
jump_tau = matrix(0.5,N,1),

a_sigma_beta = 1.0,
b_sigma_beta = 1.0,

mu_mu_beta = 0.0,
sigma_mu_beta = 10.0
))
#+end_src

#+RESULTS:

#+begin_src R :tangle R/init.R
write_init(params)
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
write_init = function(params) {
## parameter init
readr::write_csv(as.data.frame(params$kappa),"input/init_kappa.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$tau),"input/init_tau.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$theta),"input/init_theta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$sigma),"input/init_sigma.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$beta),"input/init_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$mu_beta),"input/init_mu_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$sigma_beta),"input/init_sigma_beta.csv", col_names = FALSE)
## hyper-parameter + jump
readr::write_csv(as.data.frame(cbind(params$a_kappa,params$b_kappa,params$jump_kappa)),"input/hyper_kappa.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$jump_beta),"input/hyper_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$jump_theta),"input/hyper_theta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_sigma,params$b_sigma)),"input/hyper_sigma.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_tau,params$b_tau,params$jump_tau)),"input/hyper_tau.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_sigma_beta,params$b_sigma_beta)),"input/hyper_sigma_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$mu_mu_beta,params$sigma_mu_beta)),"input/hyper_mu_beta.csv", col_names = FALSE)
  }
#+end_src

#+RESULTS:
** for-loop index
:PROPERTIES:
:header-args:R: :tangle diprom/R/preproc.R
:END:
#+begin_src R
write_loop_index = function() {
  fstr = NULL
  for (m in 1:M) {
    if(sum(dat$from==m)>0) {
      fstr = c(fstr, paste(which(dat$from == m) - 1, collapse = " ")) # R to C index conversion
    } else fstr = c(fstr, "-99")
  }
  fname = "input/out-of-state_index.txt"
  if (file.exists(fname)) {
    ##Delete file if it exists
    file.remove(fname)
  }
  fcon = file(fname)
  writeLines(fstr, fcon) # R to C index conversion
  close(fcon)

  fstr = NULL
  for (k in 1:max(item$pid)) {
    if(sum(dat$id==k)>0) {
      fstr = c(fstr, paste(which(dat$id == k) - 1, collapse = " ")) # R to C index conversion
    } else fstr = c(fstr, "-99")
}
fname = "input/person_index.txt"
if (file.exists(fname)) {
  ##Delete file if it exists
  file.remove(fname)
}
fcon = file(fname)
writeLines(fstr, fcon) # R to C index conversion
close(fcon)
}
#+end_src

#+begin_src R
write_loop_index_deprecated = function() {
sink("input/out-of-state_index.txt")
for (m in 1:M)
  if(sum(dat$from==m)>0) {
cat(which(dat$from == m) - 1,"\n") # R to C index conversion
  } else cat(-99,"\n")
sink()
sink("input/person_index.txt")
for (k in 1:max(item$pid))
  if(sum(dat$id==k)>0) {
cat(which(dat$id == k) - 1,"\n") # R to C index conversion
  } else cat(-99,"\n")
sink()
}
#+end_src
* draft
We assume a continuously observed process, recurrent actions. The process seems to be irreversible, but I need to double check. See https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2017/10/multistate_enar_webinar.pdf

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$. It may depend on covariates $\mathbf{z}(t)$, the time t itself, and possibly also the “history” of the process up to that time, $\mathbf{F}_t$: the states previously visited or the length of time spent in them.

\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.

What questions can you answer using these models? The below are quantities of interests in the multi-state survival model. All functions of the transition intensities (similar to the fact that estimable quantities are all functions of hazard rates in competing risk analysis) are estimable.

- total time that is expected to spend in state $l$ before time $t$.
- expected first passage time (first visit time to a state)
- expected number of visits to a state
- if possible, prob. of first action

In Discussion, we decided to use time-scale: in-homogeneous and number of terminal states: one.
* R packaging
#+begin_src emacs-lisp
(progn (setq default-directory "~/Dropbox/research/procmod/procmod-code/diprom")
(ess-r-devtools-clean-and-rebuild-package))
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/misc.R :results silent
kable2file <- function(fstr,fname) {
if (file.exists(fname)) {
  ##Delete file if it exists
  file.remove(fname)
}
fcon = file(fname)
writeLines(knitr::kable(fstr), fcon)
close(fcon)
}

list2file <- function(fstr,fname) {
  if (!is.list(fstr)) error("fstr must be a list object ")
  if (file.exists(fname)) {
    ##Delete file if it exists
    file.remove(fname)
  }
  fcon = file(fname)
  for (kk in 1:length(fstr)) writeLines(fstr[[kk]], fcon)
  close(fcon)
}

cl_box = function(y, cl, myname = NULL) {
  ## side by side box plot
  dd <- data.frame(y = y, cl = cl)
  boxp <- ggplot(dd, aes(x = factor(cl), y = y, fill = factor(cl))) +
    geom_boxplot() +
    theme(legend.position = "none")
  if (!is.null(myname)) {
    boxp = boxp + labs(y = myname[1], x = myname[2])}
  print(boxp)
}

cl_plot <- function(x, cl, myname = NULL, size_ = 1, xlim = NA, ylim = NA) {
  ## plot 2d array x
  ## mark points by groups specified by cl

  cl <- as.factor(cl)
  position <- as.data.frame(x)
  ndim <- dim(x)[2]

  colnames(position) <- paste("position", 1:ndim, sep = "")

  padding <- 1.05
  if (any(is.na(xlim))) {
    x1 <- -max(abs(position[, 1])) * padding
    x2 <- max(abs(position[, 1])) * padding
  } else {
    x1 <- xlim[1]
    x2 <- xlim[2]
  }
  if (any(is.na(ylim))) {
    y1 <- -max(abs(position[, 2])) * padding
    y2 <- max(abs(position[, 2])) * padding
  } else {
    y1 <- ylim[1]
    y2 <- ylim[2]
  }

  mytheme <- theme(
    axis.line = element_line(colour = "black"),
    ## panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    ## panel.border = element_blank(),
    panel.background = element_blank()
  )

  ## plot
  pp <- ggplot(position, aes(x = position1, y = position2, colour = cl)) +
    ## theme(text=element_text(size=20)) +
    ## geom_point()+
    xlim(x1, x2) +
    ylim(y1, y2) +
    xlab(myname[1]) +
    ylab(myname[2]) +
    ## xlab("Position 1") + ylab("Position 2") +
    geom_hline(yintercept = 0, color = "gray70", linetype = 2) +
    geom_vline(xintercept = 0, color = "gray70", linetype = 2) +
    mytheme
  pp <- pp + geom_point(size = size_)
  ## pp +
  ##   geom_text(
  ##     data = subset(position, idx == "w"), aes(x = position1, y = position2, label = (1:n_w), colour = cl_w),
  ##     ## segment.color = "grey50",
  ##     check_overlap = FALSE, show.legend = FALSE, size = 4
  ##   )
}
#+end_src
* Shell script to run C-code
#+begin_src sh :tangle run.sh
#!/usr/bin/env bash
# out_dir="party_invitations-1/"
# out_dir="cd_tally/"
# out_dir="sprained_ankle-1/"
# prename="R/party_invitations-1-preprocess.R"
# initname="R/party_invitations-1-init.R"

# first argument = output dir
out_dir=$1
n_chain=2
figlet "running MCMC"

echo "================================"
echo "output dir:" $out_dir
echo "preprocessing:" $prename
echo "initializing:" $initname
echo "n_chain:" $n_chain
echo "================================"

export STAN_NUM_THREADS=2
mkdir -p output
rm output/*
# rm input/*

# Rscript $prename
cp -r input output/
cp run.sh output/
# cp $prename output/
# cp $initname output/

for ((v = 1; v <= $n_chain; v++))
do
    # Rscript $initname
    ./main $v 10000 10000 10 parallel
done
## main function argument
 # chain #, iteration, burn-in, thin
 # parallel serial -> parallel computation?

mkdir -p $out_dir
mv output/* $out_dir
echo "Outputs are moved to" $out_dir"."
echo "================================"

Rscript R/run-analysis.R $out_dir $n_chain
echo "Post analysis is finished for " $out_dir"."
echo "================================"
#+end_src

* R post MCMC analysis
#+begin_src R :tangle R/run-analysis.R
#!/usr/bin/env Rscript
args <- commandArgs(trailingOnly = TRUE)
# test if there is at least one argument: if not, return an error

if (length(args) < 1) {
  stop("out_dir must be supplied.\n", call. = FALSE)
} else out_dir <- args[1]

if (length(args) == 2) {
num_chain <- args[2]
  } else if (length(args) == 1) {
num_chain = NULL
num_chain = get_nchain(out_dir)
  }
## out_dir <- "chessB-singleZ-singleW/"
system(paste0("rm figure/*.pdf"))
source("R/analysis.R")
system(paste0("mkdir -p ", out_dir, "figure/"))
system(paste0("rsync -rv figure/*.pdf ", out_dir, "figure/"))
#+end_src

set the output directory. run the chain of analytic tools. move the results to the output dir.
#+begin_src R :results none :async
setwd("~/workspace/procmod-code")

num_chain = 2

ldir = c(
"party_invitations-1/",
"party_invitations-2/",
"cd_tally/",
"sprained_ankle-1/",
## "sprained_ankle-2/", ## didn't run
"tickets/",
## "class_attendance/", ## didn't run
"club_membership-1/",
## "club_membership-2/", ## didn't run
"book_order/",
"meeting_room/",
## "reply_all/", ## failed
"locate_email/",
"lamp_return/")

## out_dir="party_invitations-1/"
## out_dir="party_invitations-2/"
## out_dir="cd_tally/"
## out_dir="sprained_ankle-1/"
## out_dir="sprained_ankle-2/"
## out_dir="tickets/"
## out_dir="class_attendance/"
## out_dir="club_membership-1/"
## out_dir="club_membership-2/"
## out_dir="book_order/"
## out_dir="meeting_room/"
## out_dir="reply_all/"
## out_dir="locate_email/"
## out_dir="lamp_return/"

for (out_dir in ldir) {
system(paste0("rm figure/*.pdf"))
system(paste0("mkdir -p figure/"))

source("R/analysis.R")

system(paste0("mkdir -p ", out_dir, "figure/"))
system(paste0("rsync -rv figure/*.pdf ", out_dir,"figure/"))
}
#+end_src

#+begin_src R :results none :tangle R/Renviron.R
library(diprom)
## library(coda)
## library(dplyr)
## library(ggplot2)
## library(stringr)
## library(magrittr)
## library(bayesplot)
## library(foreach)
## library(doParallel)
stopImplicitCluster()
## doParallel::registerDoParallel(2)
registerDoParallel(cores = detectCores() - 1)
#+end_src

#+RESULTS:
#+begin_src R :tangle R/analysis.R
source("R/Renviron.R")
source("R/load-outputs.R")
source("R/summary.R")
## source("R/visual.R")
#+end_src

* load outputs
This section contains scripts to create an mcmc object. 1) read MCMC samples, 2) set their column names, and 3) Procrustean matching.
#+begin_src R :tangle diprom/R/post.R
set_cnames = function(M = M_, N = N_, dq = dq_, dn = dn_, dc = dc_) {
  cnames <- c(".chain", ".iteration")

  ## CSVFormat prints by row-major order!
  for (m in 1:M) {
    cnames <- c(cnames, paste0("kappa.", m))
  }
  for (m in 1:N) {
    cnames <- c(cnames, paste0("tau.", m))
  }
  for (k in 1:N) {
    cnames <- c(cnames, paste0("theta.", k))
  }
  cnames <- c(cnames, "sigma", "beta", "mu_beta", "sigma_beta", "llike_", "lpri_", "lp_")
  return(cnames)
}

get_nchain = function(out_dir) {
  ## use all sample files if nchain is not set
  ## assign if NULL
  if (is.null(num_chain)) {
    num_chain <- length(list.files(path = out_dir, pattern="sample_chain[0-9]+\\.csv"))
  }
  return(num_chain)
}

read_output = function(num_chain, cnames) {
  dlist <- list()
  for (cid in 1:num_chain) {
    dlist[[cid]] <- readr::read_csv(paste0(out_dir, "sample_chain", cid, ".csv"), col_names = F, skip = 0) %>% as.data.frame()
    colnames(dlist[[cid]]) <- cnames
  }
  return(dlist)
}

#+end_src

#+begin_src R :tangle R/load-outputs.R :results silent
mvar <- readr::read_csv(paste0(out_dir, "input/mvar.csv"), col_names = F) %>% as.matrix()
M = M_ = mvar[1]
N = N_ = mvar[2]
dq = dq_ = mvar[3]
dn = dn_ = mvar[4]
dc = dc_ = mvar[5]

cnames = diprom::set_cnames(M, N, dq, dn, dc)
dlist = diprom::read_output(num_chain, cnames)

## unlist
## df <- bind_rows(dlist, .id = "column_label")

mclist <- mcmc.list()
for (cid in 1:num_chain) {
  mclist[[cid]] <- mcmc(dlist[[cid]])
}
#+end_src

#+RESULTS:
* summary
The posterior means of \beta, \theta, \lambda are exported to CSV files.
#+begin_src R :tangle R/pmean.R
param_path = paste0(out_dir, "param_mean.csv")
if (file.exists(param_path)) {
  dout = readr::read_csv(param_path)
} else {
  ss <- summary(mclist)
  mm <- ss$statistics[, "Mean"]
  ## rr <- c(grep("^kappa", names(mm)), grep("^tau", names(mm)))
  ## dout <- data.frame(vname = names(mm[rr]), mean = mm[rr])
  dout <- data.frame(vname = names(mm), mean = mm)
  readr::write_csv(dout, param_path)
}
mtheta <- dout[grep("^theta", dout$vname), 2]
mtau <- dout[grep("^tau", dout$vname), 2]
mkappa <- dout[grep("^kappa", dout$vname), 2]

item <- readr::read_csv(paste0(out_dir, "input/item.csv"), col_names = TRUE)
#+end_src

#+RESULTS:
:
: [36m──[39m [1m[1mColumn specification[1m[22m [36m──────────────────────────────────────────────────────────────────[39m
: cols(
:   vname = [31mcol_character()[39m,
:   mean = [32mcol_double()[39m
: )

[[file:figure/theta_res.pdf]]
[[file:figure/theta_tau_res.pdf]]
[[file:figure/tau_res.pdf]]
[[file:figure/time_action.pdf]]
[[file:figure/time_action_more.pdf]]

#+name: get_traits
#+begin_src R :results silent :tangle R/summary.R
source("R/pmean.R")
dat <- readr::read_csv(paste0(out_dir, "input/dat.csv"), col_names = FALSE)
rr <- dat[, c(1, 8)] %>% as.data.frame()
rr <- rr[!duplicated(rr), 2]
gg <- item %>%
  group_by(pid) %>%
  summarize(ftime = timestamp[1] / 1000, naction = n(), spd = naction / ftime, time = timestamp[n()] / 1000)
## binfo = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUFs_noN.csv")
#+end_src

WIP: stopped here
#+begin_src R
pid = unique(item$SEQID)
#+end_src

#+RESULTS:

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/tau_action.pdf")
pp <- cl_plot(cbind(log(mtau), log(gg$naction)), rr, c("log(tau)", "log(#action)"))
print(pp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/theta_res.pdf")
dd <- data.frame(theta = mtheta, res = rr)
boxp <- ggplot(dd, aes(x = factor(res), y = theta, fill = factor(res))) +
  geom_boxplot() +
  theme(legend.position = "none")
print(boxp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/tau_res.pdf")
dd <- data.frame(tau = mtau, res = rr)
boxp <- ggplot(dd, aes(x = factor(res), y = tau, fill = factor(res))) +
  geom_boxplot() +
  theme(legend.position = "none")
print(boxp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/theta_tau_res.pdf")
pp <- cl_plot(cbind(mtheta, log(mtau)), rr, c("theta", "log(tau)"))
print(pp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/time_action_more.pdf")
cl_box(log(gg$time), rr, c("log(time)", "res"))
cl_box(log(gg$ftime / gg$time), rr, c("log(ftime / time)", "res"))
cl_box(log(gg$time - gg$ftime), rr, c("log(time - ftime)", "res"))
cl_box(log(gg$naction / gg$time), rr, c("log(#action / time)", "res"))
pp <- cl_plot(cbind(log(gg$time), log(gg$naction)), rr, c("log(time)", "log(#actions)"))
print(pp)
pp <- cl_plot(cbind(log(gg$time), log(gg$ftime)), rr, c("log(time)", "log(ftime)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$time)), rr, c("theta", "log(time)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$time - gg$ftime)), rr, c("theta", "log(time - ftime)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$naction / gg$time)), rr, c("theta", "log(#action / time)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$naction / (gg$time - gg$ftime))), rr, c("theta", "log(#action / (time - ftime))"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$time)), rr, c("tau", "log(time)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$time - gg$ftime)), rr, c("tau", "log(time - ftime)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$naction / gg$time)), rr, c("tau", "log(#action / time)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$naction / (gg$time - gg$ftime))), rr, c("tau", "log(#action / (time - ftime))"))
print(pp)
dev.off(which = dev.cur())
#+end_src

- first action time and number of actions
#+begin_src R :results silent :tangle R/summary.R
pdf("figure/time_action.pdf")
cl_box(log(gg$ftime), rr, c("log(first_action_time)", "res"))
cl_box(log(gg$naction), rr, c("log(naction)", "res"))
pp <- cl_plot(cbind(mtheta, log(gg$ftime)), rr, c("theta", "log(first_action_time)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$naction)), rr, c("theta", "log(#actions)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$ftime)), rr, c("tau", "log(first_action_time)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$naction)), rr, c("tau", "log(#actions)"))
print(pp)
pp <- cl_plot(cbind(log(gg$ftime), log(gg$naction)), rr, c("log(first_action_time)", "log(#action)"))
print(pp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results none
## sink("output/mcmc_summary.txt")
cat("==================================")
cat("Rejection Rate")
cat("==================================")
rejectionRate(mclist)
cat("==================================")
cat("Effective Size")
cat("==================================")
effectiveSize(mclist)
cat("==================================")
cat("Summary")
cat("==================================")
summary(mclist)
## sink()
#+end_src

* RF: bakcground
:PROPERTIES:
:header-args:R: :results silent
:END:
We have three base models. Each model has been tuned using 4 fold-cross
validation. Since we aim to train a super learner instead of simple blend of
base models, predctions collected from CV should be passed to the super learner.
The CV-folds should be consistent for all models.

#+begin_src R :async
library(diprom)

ldir = c(
"party_invitations-1/",
"party_invitations-2/",
"cd_tally/",
"sprained_ankle-1/",
## "sprained_ankle-2/", ## didn't run
"tickets/",
## "class_attendance/", ## didn't run
"club_membership-1/",
## "club_membership-2/", ## didn't run
"book_order/",
"meeting_room/",
## "reply_all/", ## failed
"locate_email/",
"lamp_return/")
#+end_src

#+begin_src R :tangle R/gender_wilcoxon.R
for (out_dir in ldir) {
fwil_path = paste0(out_dir, "wilcoxon.txt")
  if (file.exists(fwil_path)) {
    ##Delete file if it exists
    file.remove(fwil_path)
  }
source("R/pmean.R")
source("R/pinfo_preproc.R")

sink(fwil_path)
x = minfo$tau[minfo$GENDER_R == 1]
y = minfo$tau[minfo$GENDER_R == 2]
cat(wilcox.test(x, y, alternative = "two.sided")$p.value)
cat("\n")
x = minfo$theta[minfo$GENDER_R == 1]
y = minfo$theta[minfo$GENDER_R == 2]
cat(wilcox.test(x, y, alternative = "two.sided")$p.value)
sink()
  }
#+end_src

#+begin_src R
for (out_dir in ldir) {
cat(out_dir,"\n")
pp = read.table(paste0(out_dir, "wilcoxon.txt"))
cat(unlist(pp),"\n")
cat("#############\n")
}
#+end_src


#+begin_src R :async :results output replace
## out_dir="party_invitations-1/"
## out_dir="party_invitations-2/"
## out_dir="cd_tally/"
## out_dir="sprained_ankle-1/"
## out_dir="sprained_ankle-2/"
## out_dir="tickets/"
## out_dir="class_attendance/"
## out_dir="club_membership-1/"
## out_dir="club_membership-2/"
## out_dir="book_order/"
## out_dir="meeting_room/"
## out_dir="reply_all/"
## out_dir="locate_email/"
## out_dir="lamp_return/"

num_chain = 2
stopImplicitCluster()
## doParallel::registerDoParallel(2)
registerDoParallel(cores = detectCores() - 1)

for (out_dir in ldir) {
lname = list(paste0(out_dir,"tau_imp.txt"), paste0(out_dir,"theta_imp.txt"))

source("R/pmean.R")
source("R/pinfo_preproc.R")

res = list(tau = minfo$tau, theta = minfo$theta)

for (kk in 1:2) {
fname = lname[[kk]]
if (file.exists(fname)) {
  ##Delete file if it exists
  file.remove(fname)
}
fcon = file(fname)
y = res[[kk]]
## source("R/tune_ranger.R")
source("R/train_ranger.R")
source("R/train_pimp_rf.R")
## source("R/retrain_pimp_rf.R")
fstr = print(imp[,-3])
writeLines(knitr::kable(fstr), fcon) 
close(fcon)
}
}
#+end_src

#+begin_src R :tangle R/lpa.R
library(diprom)
library(tidyLPA)

ldir = c(
## "party_invitations-1/",
## "party_invitations-2/",
## "cd_tally/",
## "sprained_ankle-1/",
## ## "sprained_ankle-2/", ## didn't run
## "tickets/",
## ## "class_attendance/", ## didn't run
## "club_membership-1/",
## ## "club_membership-2/", ## didn't run
## "book_order/",
## "meeting_room/",
## "reply_all/", ## failed
## "locate_email/", ## LPA not working
"lamp_return/")

## out_dir="party_invitations-1/"
## out_dir="party_invitations-2/"
## out_dir="cd_tally/"
## out_dir="sprained_ankle-1/"
## out_dir="sprained_ankle-2/"
## out_dir="tickets/"
## out_dir="class_attendance/"
## out_dir="club_membership-1/"
## out_dir="club_membership-2/"
## out_dir="book_order/"
## out_dir="meeting_room/"
## out_dir="reply_all/"
## out_dir="locate_email/"
## out_dir="lamp_return/"
#+end_src

#+begin_src R :tangle R/lpa.R
for (out_dir in ldir) {
flpa_path = paste0(out_dir, "lpa_mods.txt")
  if (file.exists(flpa_path)) {
    ##Delete file if it exists
    file.remove(flpa_path)
  }

num_chain = 2
stopImplicitCluster()
## doParallel::registerDoParallel(2)
registerDoParallel(cores = detectCores() - 1)

source("R/pmean.R")
source("R/pinfo_preproc.R")

gg <- item %>%
  group_by(SEQID) %>%
  summarize(ftime = timestamp[1] / 1000, naction = n(), time = timestamp[n()] / 1000, spd = naction / (ftime - time))

ainfo = plyr::join(minfo, gg)
ainfo = ainfo %>% mutate(ltau = log(tau), laction = log(naction))

mod1 = ainfo %>%
  select(tau, theta) %>%
  single_imputation() %>%
  scale() %>%
  estimate_profiles(1:4,
                    variances = c( "varying"),
                    covariances = c( "varying")) %>%
  compare_solutions(statistics = c("AIC","AWE", "BIC", "CLC", "KIC"))

mod2 = ainfo %>%
  select(tau, theta, naction, spd) %>%
  single_imputation() %>%
  scale() %>%
  estimate_profiles(1:4,
                    variances = c( "varying"),
                    covariances = c( "varying")) %>%
  compare_solutions(statistics = c("AIC","AWE", "BIC", "CLC", "KIC"))

mod3 = ainfo %>%
  select(naction, spd) %>%
  single_imputation() %>%
  scale() %>%
  estimate_profiles(1:4,
                    variances = c( "varying"),
                    covariances = c( "varying")) %>%
  compare_solutions(statistics = c("AIC","AWE", "BIC", "CLC", "KIC"))

sink(flpa_path)
cat("\n--------------------------------------------------------------\n")
cat("tau and theta")
cat("\n--------------------------------------------------------------\n")
print(mod1)
cat("\n--------------------------------------------------------------\n")
cat("tau and theta + naction, spd")
cat("\n--------------------------------------------------------------\n")
print(mod2)
cat("\n--------------------------------------------------------------\n")
cat("naction, spd")
cat("\n--------------------------------------------------------------\n")
print(mod3)
sink()
}
## mod1 = ainfo %>%
##   select(ltau, theta) %>%
##   single_imputation() %>%
##   estimate_profiles(3, variances = "varying", covariances = "varying")

## mod2 = ainfo %>%
##   select(ltau, theta, laction, spd) %>%
##   single_imputation() %>%
##   estimate_profiles(2, variances = c("varying"), covariances = c("zero"))

## mod3 = ainfo %>%
##   select(laction, spd) %>%
##   single_imputation() %>%
##   estimate_profiles(2, variances = "varying", covariances = "varying")

#+end_src

#+begin_src R
pdf("figure/param_age.pdf")
dat <- readr::read_csv(paste0(out_dir, "input/dat.csv"), col_names = FALSE)
rr <- dat[, c(1, 8)] %>% as.data.frame()
rr <- rr[!duplicated(rr), 2]
gg = cl_plot(cbind(minfo$AGEG5LFS, minfo$tau), rr, c("age","tau"), xlim=c(0,max(minfo$AGEG5LFS)), ylim = c(0, max( minfo$tau) ))
print(gg)
gg = cl_plot(cbind(minfo$AGEG5LFS, minfo$theta), rr, c("age","theta"), xlim=c(0,max(minfo$AGEG5LFS)), ylim = c(0, max( minfo$theta) ))
print(gg)
dev.off()
#+end_src

#+name: PUF_filter
#+begin_src R :tangle R/pinfo_preproc.R
var = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUF_Variables.csv")
var = var %>% filter(Domain %in% c("Sampling / weighting", "Not assigned" ,"Sampling / weighting (derived)", "Background questionnaire (trend)"  ,"Background questionnaire", "Background questionnaire (derived)")
)
param = data.frame(SEQID = unique(item$SEQID), theta = mtheta$mean, tau = mtau$mean)
pinfo = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUFs_spss.csv")
pinfo = pinfo %>% select(SEQID) %>% cbind(pinfo[,names(pinfo) %in% toupper(var$Name)])
minfo = plyr::join(param, pinfo, by = 'SEQID', type = "inner")
dx = minfo[,4:ncol(minfo)]
#+end_src


#+begin_src R :tangle R/train_ranger.R
xtrain <- model.matrix(~ 0 + ., data = dx)
ytrain <- y
weights <- rep(1, length(y)) / length(y)

set.seed(1)
nfold <- 4
shu <- sample(1:length(ytrain))
xtrain <- xtrain[shu, ]
ytrain <- ytrain[shu]
data_folds <- caret::createFolds(ytrain, k = nfold)
#+end_src

An R package ranger is used to train the random forest. An R pacakge caret is a
wrapper of many R packages, which we will use for training. The below is caret's model training parameters.
#+begin_src R :tangle R/tune_ranger.R
my_tr <- trainControl(
  method = "cv",
  number = nfold,
  ## classProbs = TRUE,
  savePredictions = "all",
  ## summaryFunction = twoClassSummary, # AUC
  ## ,summaryFunction = prSummary # PR-AUC
  ## ,summaryFunction = fSummary # F1
  ## summaryFunction = mnLogLoss,
  search = "random",
  verboseIter = TRUE,
  allowParallel = TRUE,
  indexOut = data_folds
)
#+end_src

#+RESULTS:

Unlike CatBoost or XGBoost, ranger doesn't have an internal handling mecahnism
of missing values.
#+begin_src R :tangle R/train_ranger.R
## imputation needs for ranger
## ixtrain <- xtrain
## ixtrain[is.na(ixtrain)] <- -99

## above50k needs to be "positive"
## caret considers 1st class as "positive" class
## fytrain <- factor(ytrain)
## levels(fytrain) <- c("no_cor", "cor")
#+end_src

#+RESULTS:

- Tuned hyperparameters of ranger.
#+begin_src R :tangle R/tune_ranger.R
ranger_grid <- expand.grid(
  mtry = c(5,10,15,20,25,30),
  splitrule = "variance",
  min.node.size = c(5,10,15,20)
)
#+end_src

#+RESULTS:

The below is for CV and saving a final model.
#+begin_src R :tangle R/tune_ranger.R
set.seed(1)
ranger_tune <- train(
  x = xtrain, y = ytrain,
  method = "ranger",
  trControl = my_tr,
  tuneGrid = ranger_grid,
  preProc = NULL,
  importance = "impurity",
  num.trees = 500)
#+end_src

#+begin_src R :tangle R/tune_ranger.R
temp <- ranger_tune$pred$co
ranger_id <- ranger_tune$pred$rowIndex
ranger_prob <- temp[order(ranger_id)]
ranger_final <- ranger_tune$finalModel
ranger_imp <- varImp(ranger_tune)$importance
#+end_src

=ranger= gives the confusion matrix if =probability= is set to =FALSE=.

#+name: ranger_full_training
#+begin_src R :tangle R/train_pimp_rf.R
set.seed(9999)
mydd = data.frame(y=ytrain, xtrain)

## to calulate importance p-value based on purmutation
ranger_ff <- ranger(y~.,data = mydd,
                       num.trees = 500, mtry = 30,
  ## importance = "impurity_corrected",
  importance = "permutation",
  ## importance = "hold-out",
  ## importance = "impurity",
  write.forest = TRUE,
  ## probability = TRUE,
  min.node.size = 5,
  class.weights = NULL, splitrule = "variance", classification = FALSE,
  seed = 99
)
#+end_src

#+begin_src R :tangle R/train_pimp_rf.R
## ranger_ho <- holdoutRF(y~.,data = mydd,
##                        num.trees = 500, mtry = 30,
##   ## importance = "impurity_corrected",
##   ## importance = "permutation",
##   ## importance = "hold-out",
##   ## importance = "impurity",
##   write.forest = TRUE,
##   ## probability = TRUE,
##   min.node.size = 5,
##   class.weights = NULL, splitrule = "variance", classification = FALSE,
##   seed = 99
## )
#+end_src

#+begin_src R :results output replace :tangle R/train_pimp_rf.R
ranger_imp <- ranger_ff$variable.importance
rtemp <- data.frame(ranger_imp = ranger_imp)
row.names(rtemp) <- names(ranger_imp)
rrtemp <- rtemp %>%
  arrange(-ranger_imp) # %>% slice(1:15)
rrtemp[,1] = rrtemp[,1] / rrtemp[1,1] * 100
vimp = g10 = rrtemp %>% filter(ranger_imp > 25)
vimp = data.frame(Name = row.names(vimp), imp = vimp)
tmp = var %>% filter(Name %in% vimp$Name) %>% select(Name,Label, 'Value scheme') 
vimp = plyr::join(tmp, vimp)
vimp[,-3]
g10 = vimp %>% filter(!(Name %in% c("AGEG10LFS","AGEG10LFS_T"))) %>% select(Name, ranger_imp)
#+end_src

#+RESULTS:
#+begin_example
Joining by: Name
              Name
1          C_Q09_C
2          D_Q10_T
3         AGEG5LFS
4           AETPOP
5       FNFAET12JR
6      FNFAET12NJR
7           ISIC2C
8           ISCO1C
9           ISCO2C
10       EARNHRDCL
11  EARNHRBONUSDCL
12     LEARNATWORK
13         ICTHOME
14         ICTWORK
15       INFLUENCE
16         NUMHOME
17         NUMWORK
18        PLANNING
19 PLANNING_WLE_CA
20        READWORK
21        TASKDISC
22        WRITHOME
23 WRITHOME_WLE_CA
24        WRITWORK
                                                                                                                     Label
1                                       Current status/work history - Years of paid work during lifetime (top-coded at 47)
2                               Hours per week at this job or business - number of hours (top coded at 97, Trend-IALS/ALL)
3                                                          Age groups in 5-year intervals based on LFS groupings (derived)
4                  Adult education/training population (AET) - excludes youths 16-24 in initial cycle of studies (derived)
5    Participated in formal or non-formal AET for job-related reasons in 12 months preceding survey (see AETPOP - derived)
6  Participated in formal or non-formal AET for non job-related reasons in 12 mon. preceding survey (see AETPOP - derived)
7                         Industry classification of respondent's job at 2-digit level (ISIC rev 4), current job (derived)
8                      Occupational classification of respondent's job at 1-digit level (ISCO 2008), current job (derived)
9                      Occupational classification of respondent's job at 2-digit level (ISCO 2008), current job (derived)
10                                     Hourly earnings excluding bonuses for wage and salary earners, in deciles (derived)
11                                     Hourly earnings including bonuses for wage and salary earners, in deciles (derived)
12                                                                                     Index of learning at work (derived)
13                                                                            Index of use of ICT skills at home (derived)
14                                                                            Index of use of ICT skills at work (derived)
15                                                                    Index of use of influencing skills at work (derived)
16                                                  Index of use of numeracy skills at home (basic and advanced - derived)
17                                                  Index of use of numeracy skills at work (basic and advanced - derived)
18                                                                       Index of use of planning skills at work (derived)
19                                                      Index of use of planning skills at work, categorised WLE (derived)
20                                             Index of use of reading skills at work (prose and document texts - derived)
21                                                                       Index of use of task discretion at work (derived)
22                                                                        Index of use of writing skills at home (derived)
23                                                       Index of use of writing skills at home, categorised WLE (derived)
24                                                                        Index of use of writing skills at work (derived)
   ranger_imp
1    49.17409
2    31.03164
3    41.06256
4    26.51498
5    38.10002
6    27.69774
7    34.98777
8    30.93036
9    46.15439
10   36.01960
11   40.90006
12   29.17521
13   27.67733
14   51.52846
15   55.08549
16   27.31183
17   29.12195
18   29.72996
19   26.99328
20  100.00000
21   29.54487
22   42.36498
23   26.48946
24   66.36241
#+end_example

#+begin_src R :results output replace :tangle R/train_pimp_rf.R
set.seed(9999)
mydd = mydd %>% select(y, g10$Name)
ranger_ff <- ranger(y~.,data = mydd,
                       num.trees = 500, mtry = min(10,nrow(g10)),
  ## importance = "impurity_corrected",
  importance = "permutation",
  ## importance = "hold-out",
  ## importance = "impurity",
  write.forest = TRUE,
  ## probability = TRUE,
  min.node.size = 5,
  class.weights = NULL, splitrule = "variance", classification = FALSE,
  seed = 99
)
#+end_src

#+RESULTS:

#+begin_src R :tangle R/train_pimp_rf.R :results output replace
im_pp = importance_pvalues(ranger_ff, method = "altmann", formula = y ~ ., data = mydd,
                                  num.permutations = 100)

imp = data.frame(Name = row.names(im_pp), imp = im_pp[,1], pval = im_pp[,2])
tmp = var %>% filter(Name %in% imp$Name) %>% select(Name, Label, 'Value scheme')
imp = plyr::join(tmp, imp) %>% arrange(pval)
imp[,-3]

pimp = sort(im_pp[,2])
pimp[pimp < 0.05] %>% knitr::kable()
pimp_vv = names(pimp)[pimp < 0.05]
#+end_src

#+begin_example


|               |        x|
|:--------------|--------:|
|AGEG5LFS       | 0.009901|
|ICTHOME        | 0.009901|
|J_Q08          | 0.019802|
|ICTHOME_WLE_CA | 0.019802|
|EDCAT7         | 0.039604|
|WRITHOME       | 0.049505|
#+end_example

#+begin_src R :tangle R/retrain_pimp_rf.R :results output replace
mydd = data.frame(y=ytrain, xtrain[, pimp_vv])
set.seed(1)
## pimp_rf_tune <- train(
##   y = ytrain, x =  xtrain[, pimp_vv],
##   method = "ranger",
##   trControl = my_tr,
##   tuneGrid = expand.grid(
##   mtry = c(2,4),
##   splitrule = "variance",
##   min.node.size = c(5,10,15,20)),
##   ## weights = weights,
##   preProc = NULL,
##   importance = "impurity",
##   num.trees = 500)


## to calulate importance p-value based on purmutation
pimp_rf <- ranger(y~.,data = mydd,
                       num.trees = 500, mtry = min(4,ncol(mydd)-1),
  ## importance = "impurity_corrected",
  importance = "permutation",
  ## importance = "hold-out",
  ## importance = "impurity",
  write.forest = TRUE,
  ## probability = TRUE,
  min.node.size = 5,
  class.weights = NULL, splitrule = "variance", classification = FALSE,
  seed = 99
)

vimp = pimp_rf$variable.importance
vimp = data.frame(Name = names(vimp), imp = vimp)
tmp = var %>% filter(Name %in% names(pimp_rf$variable.importance)) %>% select(Name,Label, 'Value scheme') %>% mutate(imp = pimp_rf$variable.importance)
vimp = plyr::join(tmp, vimp)
vimp[,-3]
#+end_src

#+RESULTS:
#+begin_example
Joining by: Name, imp
         Name
1    AGEG5LFS
2      AETPOP
3  FNFAET12JR
4 FNFAET12NJR
5    READWORK
                                                                                                                    Label
1                                                         Age groups in 5-year intervals based on LFS groupings (derived)
2                 Adult education/training population (AET) - excludes youths 16-24 in initial cycle of studies (derived)
3   Participated in formal or non-formal AET for job-related reasons in 12 months preceding survey (see AETPOP - derived)
4 Participated in formal or non-formal AET for non job-related reasons in 12 mon. preceding survey (see AETPOP - derived)
5                                             Index of use of reading skills at work (prose and document texts - derived)
         imp
1  0.7642215
2  0.7909464
3  0.4303323
4  2.0450399
5 -0.8395592
#+end_example

#+begin_example
Joining by: Name, imp
            Name
1          J_Q08
2       AGEG5LFS
3         EDCAT7
4        ICTHOME
5 ICTHOME_WLE_CA
6       WRITHOME
                                                                Label
1                                Background - Number of books at home
2     Age groups in 5-year intervals based on LFS groupings (derived)
3 Highest level of formal education obtained (7 categories - derived)
4                        Index of use of ICT skills at home (derived)
5       Index of use of ICT skills at home, categorised WLE (derived)
6                    Index of use of writing skills at home (derived)
         imp
1 0.09185096
2 0.15897985
3 0.03301487
4 0.07380785
5 0.04287672
6 0.03834553
#+end_example

#+RESULTS: ranger_full_training

#+begin_src R
saveRDS(pimp_rf, "cor_ranger_pimp_rf.rds")
#+end_src

#+RESULTS:

#+begin_src R
saveRDS(ranger_ff, "cor_ranger_rural_ff.rds")
#+end_src

#+RESULTS:

The below is to save urban prediction model. the predicted probablity will be used for cor prediction!
#+begin_src R
saveRDS(ranger_final, "cor_ranger_pred_rural.rds")
#+end_src

#+RESULTS:

#+begin_src R :results replace :tangle no
caret_wlogloss(ranger_tune$pred)
#+end_src

#+RESULTS:
: [1] 0.316729

#+begin_src R :results replace :tangle no
mean(ranger_tune$resample[, 1]) # incorrect. not using weight
#+end_src

#+RESULTS:
: [1] 0.3166399
* cluster analysis bakcground
:PROPERTIES:
:header-args:R: :results silent
:END:

#+name: lpa_prep
#+begin_src R
library(diprom)
library(tidyLPA)

ldir = c(
"party_invitations-1/",
## "party_invitations-2/",
## "cd_tally/",
## "sprained_ankle-1/",
## "sprained_ankle-2/", ## didn't run
"tickets/",
## "class_attendance/", ## didn't run
## "club_membership-1/",
## "club_membership-2/", ## didn't run
"book_order/"
## "meeting_room/",
## "reply_all/", ## failed
## "locate_email/",
## "lamp_return/"
)

## out_dir="party_invitations-1/"
## out_dir="party_invitations-2/"
## out_dir="cd_tally/"
## out_dir="sprained_ankle-1/"
## out_dir="sprained_ankle-2/"
## out_dir="tickets/"
## out_dir="class_attendance/"
## out_dir="club_membership-1/"
## out_dir="club_membership-2/"
## out_dir="book_order/"
## out_dir="meeting_room/"
## out_dir="reply_all/"
## out_dir="locate_email/"
## out_dir="lamp_return/"

num_chain = 2
stopImplicitCluster()
## doParallel::registerDoParallel(2)
registerDoParallel(cores = detectCores() - 1)
#+end_src

The below is code to perform LPA using optimal number of clusters (determined from prior procedure based on AIC, BIC, etc.)

#+begin_src R :noweb yes :tangle R/lpa_fig.R
<<lpa_prep>>

for (out_dir in ldir) {
  <<draw_lpa_fig>>

  <<draw_lpa_back>>
}
#+end_src

#+name: draw_lpa_fig
#+begin_src R
nc2 = c(4,4,3)
nc3 = c(2,3,2)

kk = which(out_dir == ldir)

lname = list(paste0(out_dir,"tau_imp.txt"), paste0(out_dir,"theta_imp.txt"))

source("R/pmean.R")
source("R/pinfo_preproc.R")

res = list(tau = minfo$tau, theta = minfo$theta)

gg <- item %>%
  group_by(SEQID) %>%
  summarize(ftime = timestamp[1] / 1000, naction = n(), time = timestamp[n()] / 1000, spd = naction / (ftime - time))

ainfo = plyr::join(minfo, gg)
ainfo = ainfo %>% mutate(ltau = log(tau), laction = log(naction))

mod2 = ainfo %>%
  select(tau, theta, naction, spd) %>%
  single_imputation() %>%
  scale() %>%
  estimate_profiles(nc2[kk],
                    variances = c( "varying"),
                    covariances = c( "varying"))

mod3 = ainfo %>%
  select(naction, spd) %>%
  single_imputation() %>%
  scale() %>%
  estimate_profiles(nc3[kk],
                    variances = c( "varying"),
                    covariances = c( "varying"))

pdf(paste0(out_dir,"figure/lpa_plot.pdf"))
mod2 %>% plot_profiles()
mod3 %>% plot_profiles()
dev.off()
#+end_src

#+name: draw_lpa_back
#+begin_src R
source("R/lpa_back.R")
print(classplot + ylim(-5, 5))
ggsave(file=paste0(out_dir,"figure/lpa_back_line.pdf"),width=16,height=9)
#+end_src

** background variables in clustsers
#+begin_src R :noweb yes :tangle R/lpa_back.R
bf = ainfo %>% select(AGEG5LFS, ICTHOME, READHOME, WRITHOME, NUMHOME, NFEHRS, ICTWORK, READWORK, WRITWORK, NUMWORK, INFLUENCE, EARNHRDCL, TASKDISC, LEARNATWORK)
bname = names(bf)
<<PUF_filter>>
fname = "back_var.txt"
toprint = var %>% filter(Name %in% bname) %>% select(Name, Label, 'Value scheme')
kable2file(toprint, fname)
#+end_src

#+begin_src R :tangle R/lpa_back.R
bf[bf==999999999999] = NA ## back to NA
bf = bf %>%
  single_imputation() %>%
  scale() %>% as.data.frame
df = mod2[[1]]
before = df[["dff"]][,1:6]
after = df[["dff"]][,-c(1:6)]
df[["dff"]] = before %>% mutate(AGEG5LFS = bf$AGEG5LFS, ICTHOME = bf$ICTHOME, READHOME = bf$READHOME, WRITHOME = bf$WRITHOME, NUMHOME = bf$NUMHOME, NFEHRS = bf$NFEHRS, ICTWORK = bf$ICTWORK, READWORK = bf$READWORK, WRITWORK = bf$WRITWORK, NUMWORK = bf$NUMWORK, INFLUENCE = bf$INFLUENCE, EARNHRDCL = bf$EARNHRDCL, TASKDISC = bf$TASKDISC, LEARNATWORK = bf$LEARNATWORK) %>% cbind(after)
variables = c("tau", "theta", "naction", "spd", bname)

tt = df[["dff"]]
mm = aggregate(tt, list(tt$Class), FUN=mean)
sd = aggregate(tt, list(tt$Class), FUN=sd)
n = aggregate(tt, list(tt$Class), FUN=length)
se = sd / sqrt(n)

df_raw = diprom:::.get_long_data(df)
#+end_src

#+begin_src R :tangle R/lpa_back.R
x = mod2; ci = .95; sd = TRUE; add_line = FALSE; rawdata = TRUE; bw = FALSE; alpha_range = c(0, .1)

df_plot <- get_estimates(x)

names(df_plot)[match(c("Estimate", "Parameter"), names(df_plot))] <- c("Value", "Variable")
df_plot$Class <- ordered(df_plot$Class)

if(!"Classes" %in% names(df_plot)){
  df_plot$Classes <- length(unique(df_plot$Class))
}
                                        # Drop useless stuff
df_plot <- df_plot[grepl("(^Means$|^Variances$)", df_plot$Category),
                   -match(c("p"), names(df_plot))]
df_plot$Variable <- ordered(df_plot$Variable, levels = unique(df_plot$Variable))
                                        # Select only requested variables, or else, all variables
if (!is.null(variables)) {
  df_plot <- df_plot[tolower(df_plot$Variable) %in% tolower(variables), ]
}
df_plot$Variable <- droplevels(df_plot$Variable)
## variables <- levels(df_plot$Variable)
df_plot$idvar <- paste0(df_plot$Model, df_plot$Classes, df_plot$Class, df_plot$Variable)
df_plot <- reshape(data.frame(df_plot), idvar = "idvar", timevar = "Category", v.names = c("Value", "se"), direction = "wide")

df_plot <- df_plot[, -match("idvar", names(df_plot))]
                                        # Get some classy names
names(df_plot) <- gsub("\\.Means", "", names(df_plot))

dd = df_plot
for (nn in bname) {
for (ii in unique(df_plot$Class)) {
  dd = rbind(dd,data.frame(Variable = nn, Class = ii, Model = dd[1,3], Classes = dd[1,4], Value = mm[ii,nn], se = se[ii,nn], Value.Variances = dd[1,7],
   se.Variances = dd[1,8]))
    }}
df_plot = dd
variables <- levels(df_plot$Variable)
#+end_src

#+begin_src R :tangle R/lpa_back.R
df_raw <- df_raw[, c("model_number", "classes_number", variables, "Class", "Class_prob", "Probability", "id")]
df_raw$Class <- ordered(df_raw$Class_prob, levels = levels(df_plot$Class))
variable_names <- paste("Value", names(df_raw)[-c(1,2, ncol(df_raw)-c(0:3))], sep = "...")
names(df_raw)[-c(1,2, ncol(df_raw)-c(0:3))] <- variable_names
df_raw <- reshape(
  df_raw,
  varying = c(Variable = variable_names),
  idvar = "new_id",
  direction = "long",
  timevar = "Variable",
  sep = "..."
)
if(any(c("Class_prob", "id", "new_id") %in% names(df_raw))){
  df_raw <- df_raw[, -which(names(df_raw) %in% c("Class_prob", "id", "new_id"))]
}
  df_raw$Variable <- ordered(df_raw$Variable,
                             levels = levels(df_plot$Variable))
  names(df_raw)[c(1,2)] <- c("Model", "Classes")
#+end_src

#+begin_src R :tangle R/lpa_back.R
classplot = plot_profiles.default(x =  list(df_plot = df_plot, df_raw = df_raw))
#+end_src

** responses in clusters
#+begin_src R :noweb yes :tangle R/lpa-res.R
<<lpa_prep>>

out_dir = ldir[3]

<<draw_lpa_fig>>

## source("R/lpa_back.R")

<<get_traits>>

## book_order =minfo= has a fewer SEQID than =item=
gres <- item %>%
  group_by(SEQID) %>%
  summarize(res = response[n()])
ginfo = plyr::join(minfo, gres)
rr = ginfo$res
#+end_src

#+begin_src R :noweb yes :tangle R/lpa-res.R
df = mod2[[1]]
before = df[["dff"]][,1:6]
after = df[["dff"]][,-c(1:6)]
df[["dff"]] = before %>% mutate(res = rr) %>% cbind(after)

tt = df[["dff"]]
mm = aggregate(tt, list(tt$Class), FUN=mean)
sd = aggregate(tt, list(tt$Class), FUN=sd)
n = aggregate(tt, list(tt$Class), FUN=length)
se = sd / sqrt(n)

n = n[,4]
mm = mm %>% select(4:8)
sd = sd %>% select(4:8)
mm$n = n;
sd$n = 0*n;

save(mm,sd,n, file = paste0(out_dir, "summaryw_res.RData"))
#+end_src

#+begin_src R
mat2mean_sd(mm,sd,denote_sd = "paren", markup = "markdown") %>% knitr::kable(align = "r")
#+end_src

#+begin_src R :tangle diprom/R/utils.R
## stolen from https://github.com/dewittpe/qwraps2/
frmt <- function(x, digits = 2) {
  sapply(x,
         function(xx) {
           if (is.integer(xx)) {
             formatC(xx, format = "d", big.mark = ",")
           } else {
             formatC(xx, digits = digits, format = "f", big.mark = ",")
           }
         })
}

mat2mean_sd <- function(mm, sd,
                    digits = 2,
                    na_rm = FALSE,
                    show_n = "ifNA",
                    denote_sd = "pm",
                    markup = "latex") {
  n <- sum(!is.na(s))
  m = unlist(mm)
  s = unlist(sd)

  if (all(!(show_n %in% c("ifNA", "always", "never")))) {
    warning("'show_n' should be in c('ifNA', 'always', 'never').  Setting to 'ifNA'.")
    show_n <- "ifNA"
  }

  if (show_n == "always" | (show_n == "ifNA" & any(is.na(s)))) {
    rtn <- paste0(frmt(as.integer(n), digits), "; ", frmt(m, digits), " $\\pm$ ", frmt(s, digits))
  } else {
    rtn <- paste0(frmt(m, digits), " $\\pm$ ", frmt(s, digits))
  }

  if (denote_sd == "paren") {
    rtn <- gsub("\\$\\\\pm\\$\\s(.*)", "\\(\\1\\)", rtn)
  }

  if (markup == "markdown") {
    rtn <- gsub("\\$\\\\pm\\$", "&plusmn;", rtn)
  }

  rtn = as.data.frame(array(rtn,dim=dim(mm)))
  names(rtn) = names(mm)
  return(rtn)
}

#+end_src


* MCMC trace plots
#+begin_src R :tangle diprom/R/post.R
plot_intervals = function(param, trans_ = "log", save_plot = TRUE){
  regexp = paste0("^",param,"\\.")
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_intervals.pdf"))
  p0 <- bayesplot::mcmc_intervals(
    mclist,
    regex_pars = regexp,
    transformations = trans_
  )
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
  print(p0)
  if (save_plot) dev.off(which = dev.cur())
}

plot_parcoord = function(param, trans_ = "log", save_plot = TRUE){
  regexp = paste0("^",param,"\\.")
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_parcoord.pdf"))
  p0 <- bayesplot::mcmc_parcoord(
    mclist,
    regex_pars = regexp,
    transformations = trans_
  )
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
  print(p0)
  if (save_plot) dev.off(which = dev.cur())
}

#+end_src

#+begin_src R :tangle diprom/R/post.R
plot_trace = function(param, max_, trans_ = "log", save_plot = TRUE){
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_trace.pdf"))
    regexp = paste0("^",param,"\\.[0-9]$")
  for (ii in 0:floor(max_/10)) {
 if (ii > 0) regexp = paste0("^",param,"\\.", ii, "[0-9]$")
  p <- bayesplot::mcmc_trace(
    mclist,
    regex_pars = regexp,
    transformations = trans_,
    facet_args = list(nrow = 4, labeller = label_parsed)
  )
  print(p <- p + bayesplot::facet_text(size = 10))
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
 }
  if (save_plot) dev.off(which = dev.cur())
  }
#+end_src

#+RESULTS:

[[file:figure/theta_mcmc_intervals.pdf]]
[[file:figure/tau_mcmc_intervals.pdf]]
[[file:figure/theta_mcmc_intervals.pdf]]
#+begin_src R :tangle R/visual.R
diprom::plot_intervals("kappa")
diprom::plot_intervals("tau")
diprom::plot_intervals("theta", list())
#+end_src

#+RESULTS:
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'

[[file:figure/theta_mcmc_parcoord.pdf]]
[[file:figure/tau_mcmc_parcoord.pdf]]
[[file:figure/theta_mcmc_parcoord.pdf]]
#+begin_src R
diprom::plot_parcoord("kappa")
diprom::plot_parcoord("tau")
diprom::plot_parcoord("theta", list())
#+end_src

#+RESULTS:
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'

[[file:figure/kappa_mcmc_trace.pdf]]
[[file:figure/tau_mcmc_trace.pdf]]
[[file:figure/theta_mcmc_trace.pdf]]
#+begin_src R :tangle R/visual.R
bayesplot::color_scheme_set("mix-blue-pink")
diprom::plot_trace("kappa", M)
diprom::plot_trace("tau", N)
diprom::plot_trace("theta", 20, list())
#+end_src

#+RESULTS:
: Error in facet_text(size = 10) : could not find function "facet_text"
: Error in facet_text(size = 10) : could not find function "facet_text"
: Error in facet_text(size = 10) : could not find function "facet_text"

[[file:figure/other_params_trace.pdf]]
#+begin_src R :tangle R/visual.R
pdf("figure/other_params_trace.pdf")
bayesplot::color_scheme_set("mix-blue-pink")
p <- bayesplot::mcmc_trace(mclist[[1]],
                           pars = c("sigma", "beta", "mu_beta", "sigma_beta", "llike_", "lpri_", "lp_"),
                           facet_args = list(nrow = 3, labeller = label_parsed)
                           )
print(p <- p + bayesplot::facet_text(size = 10))
dev.off(which = dev.cur())
#+end_src

#+RESULTS:
: pdf
:   2

* word2vec code
** COMMENT email transitions
#+begin_src R
library(dplyr)
load('data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
email = PS[PS$CODEBOOK == "U01a000S",]

## only core event description included
## core_event = c("MAIL_DRAG", "MAIL_VIEWED", "FOLDER_VIEWED", "MAIL_MOVED")
core_event = c("MAIL_DRAG", "MAIL_MOVED")
email$event_description[!(email$event_type %in% core_event)] <- ""

email$action = paste(email$event_type, email$event_description)
email$action[email$event_num == 1] = "START"
email$action[email$action == "END END"] = "END"
## email$action = email$event_type
ee = email %>% select(SEQID, event_type, event_description)
#+end_src

#+begin_src R
n = nrow(email)
trans = paste(email$action[1:(n-1)], "->", email$action[2:n])
trans = trans[trans != "END -> START"]
#+end_src

#+RESULTS:

#+begin_src R
aa = email
aa = aa %>% filter(event_description != "") %>% select(SEQID,event_type,event_description)
#+end_src

#+RESULTS:

#+begin_src R
length(unique(email$event_type))
length(unique(email$action))
length(unique(trans))
#+end_src

#+RESULTS:
: [1] 51
: [1] 109
: [1] 752

: [1] 1774

#+begin_src R
tab = table(trans)
ntab = as.numeric(tab)
summary(ntab)
sum(ntab > 1)
#+end_src

#+RESULTS:
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
:     1.0     1.0     2.0    45.1     9.0  3932.0
: [1] 479

#+begin_src R
toend = grepl("END$",trans)
nonext = grepl("NEXT\\_INQUIRY REQUEST ->",trans)
trans[toend & !nonext]
#+end_src

#+RESULTS:
: character(0)

#+begin_src R
texton = grepl("TEXTBOX_ONFOCUS", trans)
textoff = grepl("TEXTBOX_KILLFOCUS", trans)
#+end_src

#+RESULTS:

#+begin_src R
edes = email$event_description
cfolder = grepl("createfoldernameinput", edes)
email$SEQID[cfolder]
#+end_src

#+RESULTS:
: numeric(0)

#+begin_src R :tangle no
email %>% filter(SEQID == 4444) %>% select(event_type, event_description)
#+end_src

#+begin_src R :tangle no
email %>% filter(SEQID == 782) %>% select(event_type, event_description)
#+end_src

#+RESULTS:
#+begin_example
           event_type event_description
1               START
2         MAIL_VIEWED
3         MAIL_VIEWED
4         MAIL_VIEWED
5         MAIL_VIEWED
6         MAIL_VIEWED
7         MAIL_VIEWED
8         MAIL_VIEWED
9         MAIL_VIEWED
10        MAIL_VIEWED
11      FOLDER_VIEWED
12               MENU
13 MENUITEM_newfolder
14      FOLDER_VIEWED
15    TEXTBOX_ONFOCUS
16           KEYPRESS
17  TEXTBOX_KILLFOCUS
18      ADD_FOLDER_ok
19        MAIL_VIEWED
20       TOOLBAR_help
21  TOOLBAR_replymail
22        MAIL_VIEWED
23        MAIL_VIEWED
24        MAIL_VIEWED
25        MAIL_VIEWED
26               MENU
27    MENUITEM_delete
28        MAIL_VIEWED
29               MENU
30    MENUITEM_delete
31    TOOLBAR_mailApp
32      FOLDER_VIEWED
33               MENU
34 MENUITEM_newfolder
35      FOLDER_VIEWED
36      ADD_FOLDER_ok
37       NEXT_INQUIRY
38                END
#+end_example
** COMMENT Keras word2vec
:PROPERTIES:
:header-args:R: :results silent :exports both :noweb yes :eval never-export
:END:

see https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/

#+begin_src emacs-lisp
;; python
(require 'conda)
(conda-env-activate "tf")
#+end_src

#+RESULTS:
: Switched to conda environment: /Users/yunj/.conda/envs/r-tensorflow/

#+begin_src R :tangle word2vec.R
library(readr)
library(stringr)
reviews <- read_lines("finefoods.txt.gz", n_max = 100)
reviews <- reviews[str_sub(reviews, 1, 12) == "review/text:"]
reviews <- str_sub(reviews, start = 14)
reviews <- iconv(reviews, to = "UTF-8")
#+end_src

#+begin_src R :tangle word2vec.R
library(keras)
tokenizer <- text_tokenizer(num_words = 200)
tokenizer %>% fit_text_tokenizer(reviews)
#+end_src

#+begin_src R :tangle word2vec.R
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
  gen <- texts_to_sequences_generator(tokenizer, sample(text))
  function() {
    skip <- generator_next(gen) %>%
      skipgrams(
        vocabulary_size = tokenizer$num_words,
        window_size = window_size,
        negative_samples = 0
      )
    x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
    y <- skip$labels %>% as.matrix(ncol = 1)
    list(x, y)
  }
}
#+end_src

#+begin_src R :tangle word2vec.R
## embedding_size <- 128  # Dimension of the embedding vector.
embedding_size <- 8  # Dimension of the embedding vector.
skip_window <- 1       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
#+end_src

#+begin_src R :tangle word2vec.R
embedding <- layer_embedding(
  input_dim = tokenizer$num_words + 1,
  output_dim = embedding_size,
  input_length = 1,
  name = "embedding"
)

target_vector <- input_target %>%
  embedding() %>%
  layer_flatten()

context_vector <- input_context %>%
  embedding() %>%
  layer_flatten()

dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")

#+end_src

#+begin_src R :tangle word2vec.R


model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
summary(model)

#+end_src


#+begin_src R :tangle word2vec.R
model %>%
  fit_generator(
    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples),
    ## steps_per_epoch = 100000, epochs = 5
    steps_per_epoch = 100, epochs = 5
    )
#+end_src

** email word2vec preprocessing
:PROPERTIES:
:header-args:R: :results silent :tangle email_word2vec_preproc.r :eval never-export
:END:
goal: process actions to be used for =word2vec=
- [-] remove =event_description= with no information
- [-] remove *consecutive* action repetition.
- [X] concat =event_type= and =event_description=
- [X] substitue SPACE with "_"
- [X] concat actions into a sequence
- [X] remove START and END.
- [X] write sequences to a txt file

  #+begin_src R :tangle no
source("email_word2vec_preproc.r")
  #+end_src

- keep a small number of event_description that are highly relevant to movement of emails.
- remove action taking < 50ms ([[https://www.tobiipro.com/learn-and-support/learn/eye-tracking-essentials/how-fast-is-human-perception/][How fast is human perception? About 80 ms | Find out more]])
#+begin_src R
library(dplyr)
library(stringr)
setwd('~/workspace/procmod-code/')
load('./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata')
email = PS %>% filter(CODEBOOK == "U01a000S")

## core_event = c("MAIL_DRAG", "MAIL_MOVED")
core_event = c("MAIL_DRAG", "MAIL_MOVED", "MAIL_VIEWED")
## core_event = c("MAIL_MOVED")
email$event_description[!(email$event_type %in% core_event)] <- ""
## email$event_description <- ""

timestamp = email$timestamp
diff = c(diff(timestamp), 99999)
email$diff = diff
email = email[(diff > 99) || (diff < 0 ), ]

email$event_description = stringr::str_replace(email$event_description, "(.*)\\*\\$target=u01a_(.*)",  "\\1\\2")
email$event_description = stringr::str_replace(email$event_description, "id=u01a_",  "")
email$event_description = stringr::str_replace(email$event_description, "\\*\\$target=",  "")

email = email %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

we need to remove transition between the same action
#+begin_src R
ww = email$word
id = email$SEQID
ww0 = ww[1:(length(ww)-1)]
ww1 = ww[2:(length(ww))]

dup = ww0 == ww1
dup0 = c(dup,  FALSE )
dup1 = c( FALSE ,  dup)

idx = dup1

cw = "NULL"
cid = 9999999999
for (kk in which(dup1)) {
  if(cw != ww[kk] && cid != id[kk]) idx[kk] = FALSE
  cw = ww[kk]
  cid = id[kk]
}

email = email[!idx, ]
#+end_src


#+begin_src R
n = nrow(email)
pre = email$word[1:(n-1)]
cur = email$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

email = email[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(email$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in email$word[email$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(email$word[email$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

this leave only one action in some cases... let's keep start and end
#+begin_src R
## for (i in id) {
##     seqs[i] = gsub("START ", "", seqs[i])
##     seqs[i] = gsub(" END", "", seqs[i])
## }
seqs = seqs[id]
#+end_src

#+RESULTS:


#+begin_src R
data.table::fwrite(as.data.frame(seqs), "email_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R :tangle no
vv = numeric(max(id))
for (i in id) {
  vv[i] =length(email$word[email$SEQID == i])
  }

length(unique(email$word))
n = nrow(email)
trans = paste(email$word[1:(n-1)], "->", email$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
90 * 89 / 2

ee = PS %>% filter(CODEBOOK == "U01a000S")
ww = paste(ee$event_type, ee$event_description)
length(unique(ww))
nn = nrow(ee)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src
** email word2vec
window_size = 2 should be [5,20] for a small data set.
(negative samples) num_ns = 4
See [[file:email_word2vec.ipynb]]
Download the vectors.tsv and metadata.tsv to analyze the obtained embeddings in the Embedding Projector. https://projector.tensorflow.org/
* email subject segmentation
#+begin_src R :results silent
source("email_word2vec_preproc.r")
#+end_src

#+begin_src R :results silent
vv = readr::read_tsv("vectors.tsv")
mm = readr::read_tsv("metadata.tsv")

ww = email$word
#+end_src

#+begin_src R
myvv = matrix(0, length(id), ncol(vv))
mymm=numeric(length(id))

idx =0
for (ii in id) {
  idx=idx+1
  ss=ww[email$SEQID==ii]
  ss=ss[ss != "START"]
  ss=ss[ss != "END  "]
 ss=ss[ss != "END_CANCEL"]
  ## ss=ss[ss != "MENUITEM_help"]
ee = numeric(length(ss))
  for (kk in 1:length(ss)) {
    ee[kk]= which(ss[kk] == mm)
  }
  myvv[idx,]=colMeans(vv[ee,])
  mymm[idx]=email$response[email$SEQID==ii][1]
}

readr::write_tsv(as.data.frame(mymm),"mymm.tsv", col_names=F)
readr::write_tsv(as.data.frame(myvv),"myvv.tsv", col_names=F)
#+end_src

#+RESULTS:

- I didn't remove START and END.
- those with small event duration are deleted.
- centroid of action sequence is calculated and used for the clustering.
- timestamp information is not used for clustring
TSNE can identify many subect groups. wouldn't that be useful for futher analysis?
* Football ticket

#+begin_src R
library(dplyr)
library(stringr)
load('./data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
df = PS %>% filter(CODEBOOK == "U21x000S")

ww = paste(df$event_type, ee$event_description)
length(unique(ww))
nn = nrow(df)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src

#+begin_src R
core_event = c("COMBOBOX", "TAB", "CHECKBOX")
## core_event = c("MAIL_MOVED")
df$event_description[!(df$event_type %in% core_event)] <- ""
timestamp = df$timestamp
diff = c(0, diff(timestamp))
df = df[diff > 50, ]

df$event_description = stringr::str_replace_all(df$event_description, "\\*\\$index=", "")
df$event_description = stringr::str_replace_all(df$event_description, "u021_(.*?)_","")
df$event_description = stringr::str_replace(df$event_description, "id=",  "")
df$event_description = stringr::str_replace(df$event_description, "button",  "")

df = df %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

#+begin_src R
n = nrow(df)
pre = df$word[1:(n-1)]
cur = df$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

df = df[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(df$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in df$word[df$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(df$word[df$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

#+begin_src R
for (i in id) {
    seqs[i] = gsub("START ", "", seqs[i])
    seqs[i] = gsub(" END", "", seqs[i])
}
seqs = seqs[id]
#+end_src

#+RESULTS:


#+begin_src R
data.table::fwrite(as.data.frame(seqs), "ticket_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R
mm = 0
for (i in id) {
  mm  = max(mm , length(df$word[df$SEQID == i]))
  }
mm
length(unique(df$word))
n = nrow(df)
trans = paste(df$word[1:(n-1)], "->", df$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
#+end_src
* CD Tally :ATTACH:
:PROPERTIES:
:header-args:R: :results silent :session *R-PIACC* :exports both :noweb yes :eval never-export
:ID:       673df774-623f-44b5-a262-b22739c9a506
:END:

#+attr_org: :width 700
[[attachment:_20210426_063856screenshot.png]]

[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/CDTallyPart1/pages/cd-start.html][CD Tally Part 1]]
- look at the spreadsheet and calculate the value

#+begin_src R
library(dplyr)
library(stringr)
load('./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_cleaned_1110.rdata')
df = PS %>% filter(CODEBOOK == "U03a000S")

ww = paste(df$event_type, df$event_description)
length(unique(ww))
nn = nrow(df)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src

#+begin_src R
core_event = c("COMBOBOX", "TAB", "CHECKBOX")
## core_event = c("MAIL_MOVED")
df$event_description[!(df$event_type %in% core_event)] <- ""
timestamp = df$timestamp
diff = c(diff(timestamp), 9999)
df = df[diff > 99, ]

df$event_description = stringr::str_replace_all(df$event_description, "\\*\\$index=", "")
df$event_description = stringr::str_replace_all(df$event_description, "u021_(.*?)_","")
df$event_description = stringr::str_replace(df$event_description, "id=",  "")
df$event_description = stringr::str_replace(df$event_description, "button",  "")

df = df %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

#+begin_src R
n = nrow(df)
pre = df$word[1:(n-1)]
cur = df$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

df = df[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(df$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in df$word[df$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(df$word[df$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

#+begin_src R
for (i in id) {
    seqs[i] = gsub("START ", "", seqs[i])
    seqs[i] = gsub(" END", "", seqs[i])
}
seqs = seqs[id]
#+end_src

#+RESULTS:

#+begin_src R
data.table::fwrite(as.data.frame(seqs), "ticket_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R
mm = 0
for (i in id) {
  mm = max(mm, length(df$word[df$SEQID == i]))
  }
mm
length(unique(df$word))
n = nrow(df)
trans = paste(df$word[1:(n-1)], "->", df$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
#+end_src
* COMMENT Local Variables
# Local Variables:
# eval: (jyun/set-org-babel-default-header-args:R)
# eval: (flyspell-mode -1)
# eval: (spell-fu-mode -1)
# End:
