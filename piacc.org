#+TITLE: PIACC data processing

# https://orgmode.org/manual/Export-Settings.html#Export-Settings
#+OPTIONS: H:4 num:nil toc:nil pri:t ::t |:t f:t <:t -:t \n:nil ':t ^:{}
#+OPTIONS: d:nil todo:t tags:not-in-toc tex:t

#+STARTUP: overview inlineimages logdone indent noalign
# noindent

# comment out for reveal.js
# #+SETUPFILE: ~/setup/my-theme-readtheorg.setup

#+PROPERTY: header-args :tangle
#+PROPERTY: header-args :eval never-export
#+PROPERTY: header-args:ein :session localhost
#+PROPERTY: header-args:jupyter-python :session *jupyter-piacc* :kernel tf

* email transitions
#+begin_src R
library(dplyr)
load('Problem solving/Problem-solving_cleaned_1110.rdata')
email = PS[PS$CODEBOOK == "U01a000S",]
email$action = paste(email$event_type, email$event_description)
email$action[email$event_num == 1] = "START"
email$action[email$action == "END END"] = "END"
## email$action = email$event_type
#+end_src

#+begin_src R
n = nrow(email)
trans = paste(email$action[1:(n-1)], "->", email$action[2:n])
trans = trans[trans != "END -> START"]
#+end_src

#+RESULTS:

#+begin_src R
length(unique(trans))
#+end_src

#+begin_src R
tab = table(trans)
ntab = as.numeric(tab)
summary(ntab)
sum(ntab > 1)
#+end_src

#+begin_src R
toend = grepl("END$",trans)
nonext = grepl("NEXT\\_INQUIRY REQUEST ->",trans)
trans[toend & !nonext]
#+end_src

#+begin_src R
texton = grepl("TEXTBOX_ONFOCUS", trans)
textoff = grepl("TEXTBOX_KILLFOCUS", trans)
#+end_src

#+begin_src R
edes = email$event_description
cfolder = grepl("createfoldernameinput", edes)
email$SEQID[cfolder]
#+end_src

#+begin_src R
email %>% filter(SEQID == 4444) %>% select(event_type, event_description)
#+end_src

#+begin_src R
email %>% filter(SEQID == 782) %>% select(event_type, event_description)
#+end_src

* Keras word2vec
:PROPERTIES:
:header-args:R: :results silent :session *R-Keras* :exports both :noweb yes :eval never-export
:END:

see https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/

#+begin_src emacs-lisp
;; python
(require 'conda)
(conda-env-activate "tf")
#+end_src

#+RESULTS:
: Switched to conda environment: /Users/yunj/.conda/envs/r-tensorflow/

#+begin_src R :tangle word2vec.R
library(readr)
library(stringr)
reviews <- read_lines("finefoods.txt.gz", n_max = 100)
reviews <- reviews[str_sub(reviews, 1, 12) == "review/text:"]
reviews <- str_sub(reviews, start = 14)
reviews <- iconv(reviews, to = "UTF-8")
#+end_src

#+begin_src R :tangle word2vec.R
library(keras)
tokenizer <- text_tokenizer(num_words = 200)
tokenizer %>% fit_text_tokenizer(reviews)
#+end_src

#+begin_src R :tangle word2vec.R
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
  gen <- texts_to_sequences_generator(tokenizer, sample(text))
  function() {
    skip <- generator_next(gen) %>%
      skipgrams(
        vocabulary_size = tokenizer$num_words,
        window_size = window_size,
        negative_samples = 0
      )
    x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
    y <- skip$labels %>% as.matrix(ncol = 1)
    list(x, y)
  }
}
#+end_src

#+begin_src R :tangle word2vec.R
## embedding_size <- 128  # Dimension of the embedding vector.
embedding_size <- 8  # Dimension of the embedding vector.
skip_window <- 1       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
#+end_src

#+begin_src R :tangle word2vec.R
embedding <- layer_embedding(
  input_dim = tokenizer$num_words + 1,
  output_dim = embedding_size,
  input_length = 1,
  name = "embedding"
)

target_vector <- input_target %>%
  embedding() %>%
  layer_flatten()

context_vector <- input_context %>%
  embedding() %>%
  layer_flatten()

dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")

#+end_src

#+begin_src R :tangle word2vec.R


model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
summary(model)

#+end_src


#+begin_src R :tangle word2vec.R
model %>%
  fit_generator(
    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples),
    ## steps_per_epoch = 100000, epochs = 5
    steps_per_epoch = 100, epochs = 5
    )
#+end_src

* word2vec preprocessing
goal: process actions to be used for =word2vec=
- [-] remove =event_description= with no information
- [-] remove *consecutive* action repetition.
- [X] concat =event_type= and =event_description=
- [X] substitue SPACE with "_"
- [X] concat actions into a sequence
- [X] remove START and END.
- [X] write sequences to a txt file
#+begin_src R
library(dplyr)
library(stringr)
setwd('~/workspace/procmod-code/')
load('./data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
email = PS %>% filter(CODEBOOK == "U01a000S")
email = email %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"--",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

#+begin_src R
n = nrow(email)
pre = email$word[1:(n-1)]
cur = email$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

email = email[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(email$SEQID)
seqs = character(length(id))
for (i in id) {
  for (word in email$word[email$SEQID == i]) {
    seqs[i] = paste0(seqs[i], " ", word)
  }}
#+end_src

#+begin_src R
for (i in id) {
    seqs[i] = gsub(" START ", "", seqs[i])
    seqs[i] = gsub(" END", "", seqs[i])
}
seqs = seqs[id]
#+end_src


#+begin_src R
data.table::fwrite(as.data.frame(seqs), "email_sentence.txt", col.names=F)
#+end_src

#+begin_src R
mm = 0
for (i in id) {
  mm = max(mm, length(email$word[email$SEQID == i]))
  }
#+end_src

* email word2vec
See [[file:email_word2vec.ipynb]]
* ticket
#+begin_src R
library(dplyr)
library(stringr)
load('.data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
ticket = PS %>% filter(CODEBOOK == "U21x000S")
#+end_src

* COMMENT Local Variables
# Local Variables:
# org-babel-default-header-args:R: ((:session . "*R-Org*") (:export . "both") (:results . "output replace"))
# eval: (flyspell-mode -1)
# eval: (spell-fu-mode -1)
# End:
