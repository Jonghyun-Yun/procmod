#+TITLE: PIACC data processing

#+hugo_base_dir: ./docs
#+hugo_auto_set_lastmod: t
#+hugo_date_format: %Y-%m-%dT%T%z
#+hugo_front_matter_format: toml
#+HUGO_LEVEL_OFFSET: 0

#+hugo_section:
#+hugo_bundle:
#+hugo_categories:

#+hugo_export_rmarkdown:

# https://orgmode.org/manual/Export-Settings.html#Export-Settings
#+options: H:10 num:nil toc:t \n:nil @:t ::t |:t ^:nil ^:{} -:t f:t *:t <:t ':nil -:nil pri:t
#+options: TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:nil

#+startup: overview inlineimages logdone indent

#+latex_class: article
#+latex_class_options: [letterpaper,11pt]

#+latex_compiler: pdflatex

# comment out for reveal.js
# #+setupfile: ~/setup/my-theme-readtheorg.setup
#+setupfile: ~/org/latex_header.setup
#+setupfile: ~/org/orgmode_header.setup
# https://orgmode.org/manual/Export-Settings.html#Export-Settings

#+PROPERTY: header-args :eval never-export
#+PROPERTY: header-args:ein :session localhost
#+PROPERTY: header-args:jupyter-python :session *jupyter-piacc* :kernel tf

#+begin_src emacs-lisp :results silent
  ;; fast, no unicode-math
  (setq org-preview-latex-default-process 'dvipng)
  ;; slow, supprts unicode-math
  (setq org-preview-latex-default-process 'dvisvgm)
#+end_src

#+begin_src emacs-lisp :results silent
  (setq jyun/org-latex-preview-scale 2.5)
  ;; set default org-latex-preview size
  (jyun/org-latex-set-options)
#+end_src

* discussion
- [2021-05-09 Sun] note
  I drop event description and tryed a few more embeddings.
it seems incorrect process actions have low cosine similarity. This means they are difficult to form a semantic cluster(s).
actions associated to the task (or problem solving) has high similarity. so as long as event description reoains the same, I can set the subsequence as a new action. embedding of a new action is determined by the mean (center) embeddings of its members.
** party invitation I :noexport:ATTACH:
:PROPERTIES:
:ID:       468ae777-6949-4e3f-9521-fa63037fdf73
:END:
#+caption: Email invitation
#+attr_org: :width 500
[[attachment:_20210426_142403screenshot.png]]
[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/PartyInvitations1/pages/pi-start.html][Party Invitations Part 1]]

#+begin_src R
source("email_word2vec_preproc.r")
email %>% filter(SEQID == 101) %>% select(event_type,  event_description, timestamp, diff)
#+end_src

#+RESULTS:
#+begin_example
       event_type        event_description timestamp   diff
 1:         START                                  0  32679
 2:     MAIL_DRAG                  item101     32679   2784
 3:   MAIL_VIEWED                  item104     35463      4
 4: FOLDER_VIEWED            CanComeFolder     35467     70
 5:    MAIL_MOVED    item101|CanComeFolder     35537   5294
 6:   MAIL_VIEWED                  item102     40831   4551
 7:     MAIL_DRAG                  item102     45382   1912
 8:   MAIL_VIEWED                  item105     47294      4
 9: FOLDER_VIEWED         CannotComeFolder     47298     56
10:    MAIL_MOVED item102|CannotComeFolder     47354   5049
11:   MAIL_VIEWED                  item104     52403   3589
12:     MAIL_DRAG                  item104     55992   1542
13:   MAIL_VIEWED                  item105     57534      4
14: FOLDER_VIEWED            CanComeFolder     57538     50
15:    MAIL_MOVED    item104|CanComeFolder     57588   4722
16:   MAIL_VIEWED                  item105     62310   6690
17:   MAIL_VIEWED                  item103     69000   4563
18: FOLDER_VIEWED            CanComeFolder     73563   3468
19:   MAIL_VIEWED                  item101     77031   9667
20:  NEXT_INQUIRY                              86698   5229
21:           END                              91927 -91927
       event_type        event_description timestamp   diff
#+end_example
** party invitation II :ATTACH:
:PROPERTIES:
:ID:       b782e36e-45f5-40c7-947c-a3f668b53610
:END:

#+attr_org: :width 500
[[attachment:_20210903_105241screenshot.png]]

** [[mu4e:msgid:CAC72WzTzKpZBEHrobD4TFkHZrUHqVu6NR24qsHOYcDZ-dHFP=g@mail.gmail.com][Re: Questions about Data]] :noexport:
- initial location
  + inbox: 100s
  + can come: 200s
  + cannot come: 301

- response = (0 - 3)
  + can come: 101, 104
  + cannot come: 102
  + can come: 201, 202 (don't move, or move but put back(e.g ~TOOLBAR_trash~))
  + anywhere: 103, 105
** to sequence
- discard actions being taken for very short time (100ms)
- otherwise embeddings doesn't help much
#+begin_src R
seqs[4]
#+end_src

#+RESULTS:
: [1] "MAIL_DRAG-item101 MAIL_MOVED-item101|CanComeFolder MAIL_VIEWED-item102 MAIL_DRAG-item102 MAIL_MOVED-item102|CannotComeFolder MAIL_VIEWED-item104 MAIL_DRAG-item104 MAIL_MOVED-item104|CanComeFolder MAIL_VIEWED-item105 MAIL_VIEWED-item103 FOLDER_VIEWED-CanComeFolder MAIL_VIEWED-item101 NEXT_INQUIRY"
** [[id:02fc4776-ea2d-4770-91ed-92d35c08a81d][word2vec embedding]] :noexport:
https://www.tensorflow.org/tutorials/text/word2vec
- Words that tend to ‚Äúbehave similarly‚Äù end up close to one another in the embedding space. Instead of using the word symbol as a feature in the model, we can use its vector, which exploits such similarities.

\[
  p\left(w_{j} \mid w_{0}, u, v\right)=\frac{\exp \left(u\left(w_{0}\right)^{\top} v\left(w_{j}\right)\right)}{\sum_{w \in V} \exp \left(u\left(w_{0}\right)^{\top} v(w)\right)}
\]
where \(u: V \rightarrow \mathbb{R}^{k} \) and \(v: V \rightarrow \mathbb{R}^{k}\) are functions which map words to a word embedding‚Äîone for the pivot words, and the other for context.
** transition models
- action based transition: >= 500
- embeddings based transition (~embedding_dim~)

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$. It may depend on covariates $\mathbf{z}(t)$, the time t itself, and possibly also the ‚Äúhistory‚Äù of the process up to that time, $\mathbf{F}_t$: the states previously visited or the length of time spent in them.
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta} d_{i,m,l} ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.
** ticket :noexport:ATTACH:
:PROPERTIES:
:ID:       a183bf8a-10f9-4ccb-861b-9658d5b2b9f5
:header-args:R: :results silent :session *R-PIACC* :exports both :noweb yes :eval never-export
:END:
#+attr_org: :width 300
[[attachment:_20210426_040805screenshot.png]]

[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/FootballTickets/pages/ft-home.html][Football Tickets]]
- calendar: TAB-id=tabbutton2
- ticketing: TAB-id=tabbutton1
- event type: COMBOBOX-id=u021_default_menu1|*$index=7 (football)
- location: COMBOBOX-id=u021_default_menu2|*$index=2 (Bakerton)
- response = 1 (correct) 7 (incorrect) 0 missing
  menu1 and 6 = 9 (8 seems ok too)
- N = 1344
  src_R[:session *R-PIACC* :exports results]{length(unique(df$SEQID))}

#+transclude: t
[[id:673df774-623f-44b5-a262-b22739c9a506][CD Tally]]

* multi-state model
- https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2017/10/multistate_enar_webinar.pdf
- https://cran.r-project.org/web/packages/msm/msm.pdf
- [[id:57b08111-9c48-4fa9-b289-10f01fc7a0d6][cite:hill_relaxing_2021]]
* Model V0.1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: model/V0.1
:END:

See cite:jackson_flexsurv_2016 for available baseline functions.
Proportional baseline:
\[
  \lambda_{ml}(dt_{k,n}) = \lambda_{m0}(t) \lambda_{l} \tau_{k} \text{ for } l \in S_{m} \text{and} \lambda_{s_{m,1}}=1.
\]
\(dt_{k,n}\) denotes t_{k,n}^{stop} - t_{k,n}^{start}

Proportional hazard term:
\[
  e^{(\theta_{k} + \beta) D_{ml} }
\]
- add covariate later.
out-of-state, item, person parameters.
- no incercept term in prop. hazard if baseline contains constant in the same level.
- action m leads to more/less coherent action
- \(D_{ml}\) is bi-directional similarity mapping.
- including \(\beta_m\) doesn't make it directional.
- is \(\beta_k\) meaningful for item-specific action space? certainly not! this opens up the question about how actions should be defined. loosely defined without event_desciption or not.
** option1: similar items share the same action space
no event description should be used.
** option2: each item has its own action space (item-specific action space)

- use multi-state modeling framework to explain?
- target journal:
- grant application (check deadline)
- meeting at 4pm (CST)
- online learning platform: interaction with online resources, with instructors, with other people (communication length, contents) - team collaboration.
  - data will be available on Aug.
  + team science program (NIH)


The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$.

\begin{align*}
  q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}},
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated
Common out of state transition: \(\beta_{ml}=\beta_{m}\).

Baseline hazard:
\[
  \lambda_{ml}(t) = \alpha_{m1}(t) \alpha_{l} + \theta_{\lambda} \text{ for } l \neq 1.
\]
Proportional hazard term:
\[
  e^{\beta_j + (\beta_m +  \theta_{\beta}) D_{ml}}
\]
- \(D_{ml}\) is bi-directional similarity embedding between actions $m$ and $l$.

The piecewise-constant baseline hazard is used:
\begin{equation}
\label{eq:1}
\lambda(t) = \lambda_j \text{ if } s_{j-1} \le t < s_{j},
\end{equation}
for $j = 1,\ldots,J$. $\lambda_{j}$ could be a function of the similarity. This would be similar to have a piecewise constant transition matrix (time-inhomogeneous Markov chain), but much simpler as you have a parametric model for constants. The cosine similiarity should be normalized before used.
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t) \exp( \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta} d_{i,m,l} ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.
* Model V0.2
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: model/V0.2
:END:
Let $S$ denotes a set of all possible action. For each state $m \in S$, $A_{m}$ denotes a set of competing transitions $\{l_1, \ldots, l_{n_m}\}$ that can be taken directly after $m$.
Let \(Y_k(t)\) denote an action of k-th respondent at time $t$. All respondents assmed to begin problem solving processes at time $t=0$.

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$:
\begin{align*}
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S,
\end{align*}
where $\mathcal{F}_t$ denotes the process up to time $t$.

Action transition is assumed to follow Semi-Markovian, which means the intensity depends on the sojourn time (\(t - t_{m}\) ; time spent on the current action). This is often called "clock reset" approach as opposed to "clock forward" approach. Let $dt_{m}$ denote the sojourn time.

Cox model
\begin{align}
q_{ml}\left(t ; \mathcal{F}_{t}\right) = & q_{ml} (t - t_{m}; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t))\\
= & \lambda_{ml}(dt_{m})  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}},
\end{align}

for person $k = 1,\ldots,N$, where $\mathbf{z}(t)$ is time-varying covariates, $\lambda_{kml}(t)$ is a baseline intensity function, \(D_{ml} \in [-1,1]\) denotes the cosine similarity between actions $m$ and $l$. The cosine similarity is obtained using ~word2vec~ on action sequences of an item. The closer the cosine value to 1, the greater the similarity between actions. The closer the cosine value to -1, the greater the dis-similarity between actions. This mean there are $n_{m}$ corresponding intensity functions for state $m$, and overall $\sum_{m in S} n_m$ intensity functions.

We use the constant baseline hazard based on out-of-state transition speed and person's transition speed:
\[
  \lambda_{ml}(dt) = \kappa_{m} \tau_{k} \text{ for } l \in A_{m}.
\]

A running model has no coviarate terms:
\[
q_{ml}\left(t ; \mathcal{F}_{t}\right) = q_{ml}(dt) = \kappa_{m} \tau_{k} e^{\theta_{k} D_{ml} }.
\]

- larger $\kappa_{m}$ shorter time staying on action $m$ (faster out-of-state transition)
- larger $\tau_{k}$, faster transition speed
- larger $\theta_{k}$, larger trasition rate towards a similar action. A person with large $\theta_{k}$ tends to choose more coherent actions

*** TODO pattern discovery
  - divide tau by 1) number of actions
  - use 2) total time for visualization
  - 1) divided by 2)
  - find persons (small # of actions, large # of actions: right vs wrong)
    - ÏãúÍ∞ÑÏù¥ ÎßéÏù¥ Í±∏Î¶¨Í≥† ÎßûÏùÄÏÇ¨Îûå vs Ï†ÅÍ≤å Í±∏Î¶¨Í≥† ÎßûÏùÄ ÏÇ¨Îûå. 
    - Ï†ÅÏùÄ Ïï°ÏÖòÏúºÎ°ú ÎßûÏùÄ ÏÇ¨Îûå vs ÎßéÏùÄ Ïï°ÏÖòÏúºÎ°ú ÌãÄÎ¶∞ ÏÇ¨Îûå.
  - Î¨¥Ï°∞Í±¥ Îπ®Î¶¨ ÌëºÎã§Í≥† ÏûòÌïòÎäîÍ≤å ÏïÑÎãàÍ≥†, ÎäêÎ¶¨Í±∞ÎÇò ÌòπÏùÄ ÏÉÅÏù¥Ìïú Ïï°ÏÖò Í∞úÏàòÎ°ú Ï†ïÎãµÏóê Ïù¥Î•¥Îäî ÌîÑÎ°úÏÑ∏Ïä§ Î∞úÍ≤¨Ïóê Ï¥àÏ†ê.

** likelihood

\begin{align*}
    q_{ml} (t ; \boldsymbol{\lambda}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{ml}(t)  e^{(\boldsymbol{\beta}' \mathbf{z}(t) +  \theta_{k}) D_{ml}}\\
q_{ml}\left(t ; \mathcal{F}_{t}\right)= & \lim _{\delta t \rightarrow 0} \frac{P\left(Y(t+\delta t)=l \mid Y(t)=m, \mathcal{F}_{t}\right)}{\delta t}, m \neq l, m, l \in S
\end{align*}


The survival function is 
\[
  S_{ml}(dt) = e^{-\int_{0}^{dt_{m}} q_{ml}(x) \dd x}. 
\]
Let $\nu_{mlk}(t) = 1$ if person $k$ jump from actions $m$ to $l$ at time $t$; 0 otherwise.
\[
  f_{ml}(t) = q_{ml}(t) S_{ml}(t)
\]
\[
  likelihood =\prod_{k} f_{ml}(dt) \prod_{g \in A_{m}} S_{mg}(t),
\]
\[
  f_{ml} = q_{ml}(t) S_{ml}(t),
  S_{ml}(t) = e^{-\int_{0}^{t^{stop} - t^{start}} q_{ml}(t)\dd t}
\]

\[
  S_{ml}(dt) =  e^{-dt \kappa_{m} \omega_{l} \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }}
\]

\(n = 1,\ldots,M_{k}\): n-th action of k-th person, $M_k$: sequence length

\(  \delta_{k,n,m} = 1 \) if person k's n-th action is m.

\( \delta_{k,n,m}  \delta_{k,n+1,l} = 1 \) for $n < M_{k}$ if person k's n-th transition is m to l.

time at starting state (one after START) is set to the first action (n=1), and the corresponding time is set to 0.
** prior
The proposed method use a fully Bayesian approach for estimating the proposed latent space model, using MCMC methods. Our prior specification is as follows:

\begin{align*}
\pi\left(\kappa_{m}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\tau_{k}\right) & \sim \operatorname{Gamma}\left(a_{\kappa}, b_{\kappa})\right); \\
\pi\left(\theta_{k} | \sigma^{2}\right) & \sim \mathrm{N}\left(0, \sigma^{2}\right); \\
\pi\left(\sigma^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma}, b_{\sigma}\right); \\
\end{align*}

# \pi\left(\beta_{k} |\mu_{\beta}, \sigma_{\beta}^{2}\right) & \sim \mathrm{N}\left(\mu_{\beta}, \sigma_{\beta}^{2}\right); \\
# \pi\left(\mu_{\beta}|\sigma_{\beta}) & \sim \mathrm{N}\left(0, \sigma^{2}_{\mu_{\beta}}\right);\\
# \pi\left(\sigma_{\beta}^{2}\right) & \sim \operatorname{lnv}-\operatorname{Gamma}\left(a_{\sigma_{\beta}}, b_{\sigma_{\beta}}\right),\\

# \begin{align*}
# \pi\left(\sigma^{2}\right) & \propto \sigma^{-2}\\
# \pi\left(\mu_{\beta}, \sigma_{\beta}^{2}\right) & \propto \sigma_{\beta}^{-2},
# \end{align*}

inv-Gamma(\theta|\alpha,\beta)
\[
p(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha+1)} e^{-\beta / \theta}, \quad \theta>0
\]

where hyperparameters are chosen as
\[a_{\sigma}=0.0001, b_{\sigma}=0.0001, \mu_{\theta}=0, \text { and } ....\]

Based on our experience, the inference of $\mathbf{\Theta}$ is highly sensitive to its variance $\sigma^2$. Also, the configuration of latent embeddings highly depends on the scale parameter $\gamma$ of the latent space. Rather than choosing sub-optimal tuning parameters, we use a layer of hyper-priors to learn optimal values of these parameters from data. We choose hyperparameters such that priors are minimally informative to facilitate the flexible Bayesian learning.
** pseudo code
:PROPERTIES:
:ID:       5c29e214-f86d-41d5-89a0-e164602bf6b8
:END:
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::79;;1][3.2 Normal data with a noninformative prior distribution org-id:{ce3939d9-fb55-4b01-8747-0f486c98c9e7}:org-id]]
- [[skim:///Users/yunj/Zotero/storage/9D6G7MNF/gelman_bayesian_2014.pdf::591;;1][Continuous distributions org-id:{5c29e214-f86d-41d5-89a0-e164602bf6b8}:org-id]]
*** update \(\kappa_{m}\)
- all $k$ person's having action m, all l \in A_m (all possible actions that can jump from m)
- transition start and stop time $dt_{k,n}$ for all $\delta_{k,n,m} = 1$
- For each $k,c$, a symmetric MH jumping $J(\kappa_{m}^{(l-1)} \rightarrow \kappa_{m}^{* })$ is used to propose a new sample.
- We accept $\kappa_{m}^{(l)} = \kappa_{m}^{* }$ with probability $\min(1, r_{{\kappa_{m}}^{* }})$ where
\begin{align*}
\log r_{{\kappa_{m}}^{* }} =&
\sum \delta_{k,n,m} (\log \kappa_{m}^{* } - \log \kappa_{m}^{(l-1)})\\
&-\sum dt (\kappa_{m}^{* } - \kappa_{m}^{(l-1)}) \tau_{k} e^{(\theta_{k} + \beta) D_{ml} }
+ \log \frac{\pi(\kappa_{m}^{* })}{\pi(\kappa_{m}^{t})}.
\end{align*}

*** update \(\tau_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
\begin{align*}
\log r_{{\tau_{k}}^*} =&
\sum \delta_{k,n,m} (\log \tau_{k}^* - \log \tau_{k}^{(l-1)})\\
&-\sum dt \kappa_{m}e^{(\theta_{k} + \beta) D_{ml}} ( \tau_{k}^* -  \tau_{k}^{(l-1)} )
+ \log \frac{\pi(\tau_{k}^*)}{\pi(\theta_{k}^{t})}.
\end{align*}
*** update \(\theta_{k}\)
- all $k$ person's m and l \in A_m
- all kth person's transition start and stop time
- For each $k$, a symmetric MH jumping $J(\theta_{k}^{(l-1)} \rightarrow \theta_{k}^{* }$ is used to propose a new sample.
- We accept $\theta_{k}^{(l)} = \theta_{k}^{* }$ with probability $\min(1, r_{{\theta_{k}}^{* )}})$ where

\begin{align*}
\log r_{{\theta_{k}}^{* }} =& \sum \delta_{k,n,m} (\theta_{k}^{* } - \theta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \beta D_{ml} }(e^{\theta_{k}^{* }D_{ml}} -  e^{\theta_{k}^{(l-1)} D_{ml} })
+ \log \frac{\pi(\theta_{k}^{* })}{\pi(\theta_{k}^{(l-1)})}.\\
\end{align*}

*** update \(\sigma\)
\[
 p( \sigma^2|e.e.) \propto invGamma(\sigma^{2}|a,b) \prod N(\theta_{k} | \mu, \sigma^2)
\]
\(\sigma^{2} \sim inv-gamma(a + 0.5 * N, b + 0.5 + \sum \theta_{k}^2)\)
with flat prior:
\(\sigma^{2} \sim inv-gamma(0.5 * N, 0.5 + \sum \theta_{k}^2)\)
*** update \(\omega_{l}\) :ARCHIVE:
- all $k$ person's having action l, all m \in B_l (all possible actions that can jump to l)
- transition start and stop time $dt_{k,n-1}$ for all $\delta_{k,n,l} = 1$
*** update \(\beta\) :ARCHIVE:
- a symmetric MH jumping $J(\beta_{k}^{(l-1)} \rightarrow \beta_{k}^{* })$ is used to propose a new sample.
- We accept $\beta_{k}^{(l)} = \beta_{k}^{* }$ with probability $\min(1, r_{{\beta_{k}}^{* )}})$ where
\begin{align*}
\log r_{{\beta_{k}}^{* }} =&
\sum \delta_{k,n,m} (\beta_{k}^{* } - \beta_{k}^{(l-1)})D_{ml}\\
&-\sum dt \kappa_{m} \tau_{k} e^{ \theta_k D_{ml} }(e^{\beta^{* }D_{ml}} -  e^{\beta^{(l-1)} D_{ml} })
+ \log \frac{\pi(\beta_{k}^{* })}{\pi(\beta_{k}^{(l-1)})}.
\end{align*}
*** update \(\mu_{\beta}, \sigma_{\beta}\) :ARCHIVE:
\begin{align*}
  \rho &= 1/\sigma_{\beta}^{2} + 1/\sigma_{\mu_{\beta}}^2 \\
  p(\mu_{\beta}|..)&= N(\frac{1/\sigma_{\beta}^2 \times \beta}{\rho}, 1/\rho )
\end{align*}
\[
  p(\sigma_{\beta}^{2}|ee) = inv-Gamma(a + 0.5, b + 0.5 (\beta - \mu_{\beta})^2)
\]

** data structure
*** mstate R package
- how to format data to follow the below?
| person | entry time | exit time | from | to | observed | covariate1 | covariate2 | covariate3    | time dependent covaraite |
|      1 |          0 |        10 |    1 |  2 |        1 | D_{12}        | \theta D_{12}      | total actions | none                     |
|      1 |          0 |        10 |    1 |  3 |        0 | D_{12}        | \theta D_{12}      | total actions | none                     |
|        |            |           |      |    |          |            |            |               |                          |
|        |            |           |      |    |          |            |            |               |                          |
- mstate: cannot handle recurrent states
#+begin_src R
library(mstate)
# Transition matrix for illness-death model
tmat <- trans.illdeath()
# Data in wide format, for transition 1 this is dataset E1 of
# Therneau & Grambsch (T&G)
tg <- data.frame(id=1:6,illt=c(1,1,6,6,8,9),ills=c(1,0,1,1,0,1),
                 dt=c(5,1,9,7,8,12),ds=c(1,1,1,1,1,1),
                 x1=c(1,1,1,0,0,0),x2=c(6:1))
# Data in long format using msprep
tglong <- msprep(time=c(NA,"illt","dt"),status=c(NA,"ills","ds"),
                 data=tg,keep=c("x1","x2"),trans=tmat, id="id")
#+end_src

#+RESULTS:
: Loading required package: survival

#+begin_src R
# Same thing in etm format
tra <- trans2tra(tmat)
tgetm <- msdata2etm(tglong, id="id")
tgetm <- msdata2etm(tglong, id="id", covs=c("x1", "x2")) # with covariates
# And back
etm2msdata(tgetm, id="id", tra=tra)
etm2msdata(tgetm, id="id", tra=tra, covs=c("x1", "x2")) # with covariates
#+end_src

*** msm R package
https://www.rdocumentation.org/packages/msm/versions/1.6.8/topics/msm2Surv
#+begin_src R
library(msm)
msmdat <- data.frame(
 subj = c(1, 1, 1, 1, 1, 2, 2, 2),
 days = c(0, 27, 75, 97, 1106, 0, 90, 1037),
 status = c(1, 2, 3, 4, 4, 1, 2, 2),
 age = c(66, 66, 66, 66, 69, 49, 49, 51),
 treat = c(1, 1, 1, 1, 1, 0, 0, 0)
)
# transitions only allowed to next state up or state 4
Q <- rbind(c(1, 1, 0, 1),
           c(0, 1, 1, 1),
           c(0, 0, 1, 1),
           c(0, 0, 0, 0))
dat <- msm2Surv(data=msmdat, subject="subj", time="days", state="status",
         Q=Q)
dat
attr(dat, "trans")
#+end_src

* piacc preproc
#+begin_src emacs-lisp
(delete-directory "input" t)
(make-directory "input")
#+end_src
** all-in-one R script
:PROPERTIES:
:header-args:R: :tangle R/preproc-data.R
:END:

- preprocess data, word2vec, prepare for C, run C, post analysis!
#+begin_src R :tangle R/all_in_one.R
ipath = "input"
if (dir.exists(paths)) {
  do.call(file.remove, list(list.files("input", full.names = TRUE)))
  } else dir.create(ipath)
source("R/itemcode.R")
out_dir = paste0(booklet$NAME[booklet$CODEBOOK == item_code], "/")
system(paste0("figlet ", item_code))
system(paste0("figlet ", out_dir))
source("R/preproc-data.R")
system(paste0("sh run.sh ", out_dir))
#+end_src

#+begin_src R :tangle R/run_all_in_one.R
## item_code = "U04a000S" ## cannot convert to long format (msm)
## item_code = "U01b000S"

item_code = "U19a000S"
source("R/all_in_one.R")

item_code = "U07x000S"
source("R/all_in_one.R")

item_code = "U02x000S"
source("R/all_in_one.R")

item_code = "U16x000S"
source("R/all_in_one.R")

item_code = "U11b000S"
source("R/all_in_one.R")

item_code = "U23x000S"
source("R/all_in_one.R")

item_code = "U06a000S"
source("R/all_in_one.R")
#+end_src

#+begin_src R :results value list drawer silent
library(diprom)
## library(dplyr)
## library(stringr)
## library(msm)
## library(foreach)
## library(doParallel)
stopImplicitCluster()
registerDoParallel(cores = detectCores() - 1)
## doParallel::registerDoParallel(2)
#+end_src

#+begin_src R :results value list drawer silent
setwd('~/workspace/procmod-code/')
piacc_orig_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_cleaned_1110.rdata"
piacc_path = "./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata"
## piacc_na.omit()
piacc_background_path = "./data/PIAAC_cleaned_data_1110/Prgusap1_2017.csv"
piacc_background_spss = "./data/PIAAC_cleaned_data_1110/Prgusap1_2017.sav"
## xxx = foreign::read.spss("./data/PIAAC_cleaned_data_1110/Prgusap1_2017.sav")
#+end_src

#+begin_src R :results value list drawer silent
item = read_piacc(piacc_path, item_code, sub_str, ignore_str)
item2sen(item)
item %>% group_by(SEQID) %>% summarise(mnum = max(event_num))
#+end_src

Run Python codes to do ~word2vec~.
#+begin_src R :results silent
system(". activate tf; python piacc_word2vec.py")
#+end_src

#+begin_src R :results silent
metadata_path = "input/metadata.tsv"
vectors_path = "input/vectors.tsv"
metadata = readr::read_tsv(metadata_path, col_names = FALSE)
vectors = readr::read_tsv(vectors_path, col_names = FALSE)

item = filter_item(item)
Q = get_trans(item)
Dml = get_cosdist()
#+end_src

#+begin_src R :results none
dat = item2long(item)

M = length(state)
N = length(unique(dat$id))
dq = nrow(Q)
dn = nrow(dat)
dc = ncol(dat)
#+end_src

#+begin_src R :results none
write_data()
source("R/init.R")
write_loop_index()
#+end_src

#+RESULTS:

** internal functions
#+begin_src R :tangle diprom/R/preproc.R :results value list drawer silent
read_piacc = function(piacc_path, item_code, sub_str, ignore_str, core_event = NULL) {
  load(piacc_path)
  item = PS %>% filter(CODEBOOK == item_code)

## core_event = c("MAIL_DRAG", "MAIL_MOVED", "MAIL_VIEWED")
## email$event_description[!(email$event_type %in% core_event)] <- ""

  timestamp = item$timestamp
  diff = c(diff(timestamp), 99999)
  item$diff = diff
  item = item[(diff >= 10) || (diff < 0 ), ]

  if (length(sub_str) != 0) {
    for (ii in 1:nrow(sub_str)){
      item$event_description = stringr::str_replace_all(item$event_description,
                                                    sub_str[ii,1], sub_str[ii,2])}
  }

  if (length(ignore_desc) != 0) {
    for (ii in 1:length(ignore_desc)) {
      item = item %>% mutate(event_description = ifelse(event_type == ignore_desc[ii],"",event_description)) }
  }

  item = item %>%
    mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
    mutate(word = gsub(" ", "_", event_concat))

  return(item)
}
#+end_src

#+begin_src R :tangle diprom/R/preproc.R
item2sen = function(item) {
ww = item$word
id = item$SEQID
ww0 = ww[1:(length(ww)-1)]
ww1 = ww[2:(length(ww))]

dup = ww0 == ww1
dup0 = c(dup,  FALSE )
dup1 = c(FALSE ,  dup)

idx = dup1

cw = "NULL"
cid = 9999999999
for (kk in which(dup1)) {
  if(cw != ww[kk] && cid != id[kk]) idx[kk] = FALSE
  cw = ww[kk]
  cid = id[kk]
}

item = item[!idx, ]

n = nrow(item)
pre = item$word[1:(n-1)]
cur = item$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

item = item[!idx,]

id = unique(item$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in item$word[item$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(item$word[item$SEQID == i] , collapse = " ")
}

## for (i in id) {
##     seqs[i] = gsub("START ", "", seqs[i])
##     seqs[i] = gsub(" END", "", seqs[i])
## }
seqs = seqs[id]

data.table::fwrite(as.data.frame(seqs), "input/item_sentence.txt", col.names=F)
}
#+end_src

#+begin_src R :tangle diprom/R/preproc.R
filter_item = function(item) {
  ## a few more processing
  ## state to global env
item =  item %>% filter(word !="START")
state <<- unique(item$word)
ntate = numeric(nrow(item))
ii = 0
for (ww in item$word) {
  ii = ii + 1
  ntate[ii] = which(state == ww)
}
item$state = ntate

uid = unique(item$SEQID)
pid = numeric(nrow(item))
ii = 0
for (nn in item$SEQID) {
  ii = ii + 1
  pid[ii] = which(uid == nn)
  }
item$pid = pid
return(item)}

get_trans = function(item) {
id = unique(item$SEQID)
L = length(state <<- unique(item$word))
Q = matrix(0, L, L)
for (i in id) {
  ## for (word in item$word[item$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seq = item$state[item$SEQID == i]
  for (k in 1:(length(seq)-1))
    Q[seq[k], seq[k+1]] = 1
  ## cat(seq[length(seq)-1])
}

rsq = rowSums(Q) > 0
diag(Q) = 1 * rsq

return(Q)}
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
get_cosdist = function() {
dem = ncol(vectors)
vv = array(0, dim=c(M <- length(state), dem))
for (m in 1:M) {
  if (sum(state[m] == metadata[,1]) > 0)
  vv[m, ] = unlist(vectors[which(state[m] == metadata[,1]), ] )
}
Dml = cosdist(as.matrix(vv))

return(Dml)
}
#+end_src

#+RESULTS:

** write data
:PROPERTIES:
:header-args:R: :tangle R/write-data.R
:END:
#+begin_src R :results none :tangle diprom/R/preproc.R
item2long = function(item) {
df = item %>% select(pid, timestamp, response, state)
dat <- msm::msm2Surv(data=df, subject="pid", time="timestamp", state="state",
         Q=Q)
##dat
## attr(dat, "trans")
dat$time = dat$time / max(dat$Tstop)
dat$Tstart = dat$Tstart / max(dat$Tstop)
dat$Tstop = dat$Tstop / max(dat$Tstop)
dat$dist = foreach (i = 1:nrow(dat), .combine="c") %dopar%
  Dml[dat$from[i], dat$to[i]]
return(dat)
}
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
write_data = function(){
## R to C index conversion
wat = dat
wat$id = wat$id - 1
wat$from = wat$from - 1
wat$to = wat$to - 1

## to be read from cpp
readr::write_csv(as.data.frame(item),"input/item.csv", col_names = TRUE)
readr::write_csv(as.data.frame(state),"input/state.csv", col_names = FALSE)
readr::write_csv(wat,"input/dat.csv", col_names = FALSE)
readr::write_csv(as.data.frame(Q),"input/trans.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(M, N, dq, dn, dc)),"input/mvar.csv", col_names = FALSE)
readr::write_csv(data.frame(Dml), "input/Dml.csv", col_names = FALSE)
}
#+end_src

** background
:PROPERTIES:
:header-args:R: :tangle R/baclground.R
:END:
#+begin_src R 
binfo = readr::read_csv("./data/PIAAC_cleaned_data_1110/Prgusap1_2017.csv")
item = readr::read_csv("input/item.csv")
pal = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUFs_values.csv")
#+end_src

#+RESULTS:
#+begin_example

[36m‚îÄ‚îÄ[39m [1m[1mColumn specification[1m[22m [36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[39m
cols(
  .default = col_character(),
  CNTRYID = [32mcol_double()[39m,
  CNTRYID_E = [32mcol_double()[39m,
  SEQID = [32mcol_double()[39m,
  GENDER_R = [32mcol_double()[39m,
  DISP_CIBQ = [32mcol_double()[39m,
  PBROUTE = [32mcol_double()[39m,
  REG_TL2 = [32mcol_double()[39m,
  CTRYRGN = [32mcol_double()[39m,
  ISCO1C = [32mcol_double()[39m,
  ISCO1L = [32mcol_double()[39m,
  LITSTATUS = [32mcol_double()[39m,
  NUMSTATUS = [32mcol_double()[39m,
  PSLSTATUS = [32mcol_double()[39m,
  VEMETHODN = [32mcol_double()[39m,
  VEFAYFAC = [32mcol_double()[39m,
  VENREPS = [32mcol_double()[39m,
  VARSTRAT = [32mcol_double()[39m,
  VARUNIT = [32mcol_double()[39m,
  SPFWT0 = [32mcol_double()[39m,
  SPFWT1 = [32mcol_double()[39m
  # ... with 89 more columns
)
[36m‚Ñπ[39m Use [90m[40m[90m[40m`spec()`[40m[90m[49m[39m for the full column specifications.

[36m‚îÄ‚îÄ[39m [1m[1mColumn specification[1m[22m [36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[39m
cols(
  CNTRYID = [31mcol_character()[39m,
  SEQID = [32mcol_double()[39m,
  booklet_id = [31mcol_character()[39m,
  item_id = [32mcol_double()[39m,
  event_name = [31mcol_character()[39m,
  event_type = [31mcol_character()[39m,
  timestamp = [32mcol_double()[39m,
  event_description = [31mcol_character()[39m,
  NAME = [31mcol_character()[39m,
  CODEBOOK = [31mcol_character()[39m,
  response = [32mcol_double()[39m,
  event_num = [32mcol_double()[39m,
  diff = [32mcol_double()[39m,
  event_concat = [31mcol_character()[39m,
  word = [31mcol_character()[39m,
  state = [32mcol_double()[39m,
  pid = [32mcol_double()[39m
)

[36m‚îÄ‚îÄ[39m [1m[1mColumn specification[1m[22m [36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[39m
cols(
  `Variable Name` = [31mcol_character()[39m,
  `Value Label` = [31mcol_character()[39m,
  `Value (SAS)` = [31mcol_character()[39m,
  `Value (SPSS)` = [32mcol_double()[39m,
  `Value Type` = [31mcol_character()[39m
)

Warning: 3050 parsing failures.
 row          col expected actual                                             file
5373 Value (SPSS) a double      A './data/PIAAC_cleaned_data_1110/PUFs_values.csv'
5374 Value (SPSS) a double      B './data/PIAAC_cleaned_data_1110/PUFs_values.csv'
5375 Value (SPSS) a double      C './data/PIAAC_cleaned_data_1110/PUFs_values.csv'
5376 Value (SPSS) a double      D './data/PIAAC_cleaned_data_1110/PUFs_values.csv'
5377 Value (SPSS) a double      E './data/PIAAC_cleaned_data_1110/PUFs_values.csv'
.... ............ ........ ...... ................................................
See problems(...) for more details.
#+end_example

#+begin_src R
pal[,1] = pal[,1] %>% unlist %>% toupper
vname = unique(pal[,1]) %>% unlist
bname = names(binfo)
SAS = pal[,3] %>% unlist %>% str_replace_all("\\.", "")
SPSS = pal[,4] %>% unlist
for (ii in 1:length(vname)) {
  if (any(vname[ii] == bname)) {
binfo[,vname[ii]] = plyr::mapvalues(unlist(binfo[,vname[ii]]), SAS[pal[,1] == vname[ii]], SPSS[pal[,1] == vname[ii]]) %>% as.numeric
}}
binfo[is.na(binfo)] = 999999999999
readr::write_csv(binfo, "./data/PIAAC_cleaned_data_1110/PUFs_spss.csv")

for (ii in 6:ncol(binfo)) {
## binfo[,ii] = plyr::mapvalues(unlist(binfo[,ii]), "N", 9^50) %>% as.numeric
binfo[is.na(binfo[,ii]),ii] = -9^50
}
readr::write_csv(binfo, "./data/PIAAC_cleaned_data_1110/PUFs_noN.csv")
#+end_src

#+RESULTS:
#+begin_example
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: A, N
The following `from` values were not present in `x`: 3, 4, 5, D, R
The following `from` values were not present in `x`: 3, D, R
The following `from` values were not present in `x`: 5, D, R
The following `from` values were not present in `x`: 4, 5, 6, 8, 10, 16, V, R
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: V, M
The following `from` values were not present in `x`: V, M
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: 3, D, R
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: 5, V, D, R
The following `from` values were not present in `x`: V, M
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 36, 40, 56, 124, 196, 203, 208, 233, 246, 250, 276, 372, 380, 392, 410, 528, 578, 616, 643, 703, 724, 752, 826, 840
The following `from` values were not present in `x`: 36, 40, 124, 196, 203, 208, 233, 246, 250, 276, 372, 380, 392, 410, 528, 578, 616, 643, 703, 724, 752, 926, 928, 956, 1241, 1242, 8261
The following `from` values were not present in `x`: 1, 2, 3, 4, 5, 6, 7, 8, N
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: 6
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: 4, 5, 6, 8, 10, 15
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: 0, 3, 4, 5, 12, 13, 14, 15, 16, 17, 18, 21, 24, 25, 27, 90, N
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 9
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: U
The following `from` values were not present in `x`: A
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: N
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: NA, NA, NA, NA, NA
The following `from` values were not present in `x`: NA, NA, NA, NA, NA
The following `from` values were not present in `x`: 0, 1, 2, 3, 6, 7, 8, 95, NA, NA, NA, NA, NA
The following `from` values were not present in `x`: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 62, 63, 95, NA, NA, NA, NA, NA
The following `from` values were not present in `x`: NA, NA, NA, NA, NA
The following `from` values were not present in `x`: U, NA, NA, NA, NA, NA
The following `from` values were not present in `x`: 03, 06, 12, 15, 39, 98, A, D, E, I, J, K, L, M, N, O, P, Q, R, S, T, U, NA, NA, NA, NA, NA
The following `from` values were not present in `x`: 02, 03, 07, 08, 09, 12, 13, 14, 15, 17, 26, 37, 38, 39, 46, 50, 51, 60, 72, 77, 98, 99, A, D, E, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, NA, NA, NA, NA, NA
The following `from` values were not present in `x`: V, D, R
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: 8, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: D
The following `from` values were not present in `x`: D, R
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: 1, 5
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: 3, V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: 4, V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: 3, V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: 4, V
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: U
The following `from` values were not present in `x`: V, D, R, N
The following `from` values were not present in `x`: aar, abk, ace, ach, ada, ady, afa, afh, afr, ain, aka, akk, ale, alg, alt, amh, ang, anp, apa, ara, arc, arg, arn, arp, art, arw, asm, ast, ath, aus, ava, ave, awa, aym, aze, bad, bai, bak, bal, bam, ban, bas, bat, bej, bel, bem, ben, ber, bho, bih, bik, bin, bis, bla, bnt, bod, bos, bra, bre, btk, bua, bug, bul, byn, cad, cai, car, cat, cau, ceb, cel, ces, cha, chb, che, chg, chk, chm, chn, cho, chp, chr, chu, chv, chy, cmc, cop, cor, cos, cpe, cpf, cpp, cre, crh, crp, csb, cus, cym, dak, dan, dar, day, del, den, deu, dgr, din, div, doi, dra, dsb, dua, dum, dyu, dzo, efi, egy, eka, ell, elx, enm, epo, est, eus, ewe, ewo, fan, fao, fas, fat, fij, fil, fin, fiu, fon, fra, frm, fro, frr, frs, fry, ful, fur, gaa, gay, gba, gem, gez, gil, gla, gle, glg, glv, gmh, goh, gon, gor, got, grb, grc, grn, gsw, guj, gwi, hai, hat, hau, haw, heb, her, hil, him, hin, hit, hmn, hmo, hrv, hsb, hun, hup, hye, iba, ibo, ido, iii, ijo, iku, ile, ilo, ina, inc, ind, ine, inh, ipk, ira, iro, isl, ita, jav, jbo, jpn, jpr, jrb, kaa, kab, kac, kal, kam, kan, kar, kas, kat, kau, kaw, kaz, kbd, kha, khi, khm, kho, kik, kin, kir, kmb, kok, kom, kon, kor, kos, kpe, krc, krl, kro, kru, kua, kum, kur, kut, lad, lah, lam, lao, lat, lav, lez, lim, lin, lit, lol, loz, ltz, lua, lub, lug, lui, lun, luo, lus, mad, mag, mah, mai, mak, mal, man, map, mar, mas, mdf, mdr, men, mga, mic, min, mis, mkd, mkh, mlg, mlt, mnc, mni, mno, moh, mon, mos, mri, msa, mul, mun, mus, mwl, mwr, mya, myn, myv, nah, nai, nap, nau, nav, nbl, nde, ndo, nds, nep, new, nia, nic, niu, nld, nno, nob, nog, non, nor, nqo, nso, nub, nwc, nya, nym, nyn, nyo, nzi, oci, oji, ori, orm, osa, oss, ota, oto, paa, pag, pal, pam, pan, pap, pau, peo, phi, phn, pli, pol, pon, por, pra, pro, pus, que, raj, rap, rar, roa, roh, rom, ron, run, rup, rus, sad, sag, sah, sai, sal, sam, san, sas, sat, scn, sco, sel, sem, sga, sgn, shn, sid, sin, sio, sit, sla, slk, slv, sma, sme, smi, smj, smn, smo, sms, sna, snd, snk, sog, som, son, sot, sqi, srd, srn, srp, srr, ssa, ssw, suk, sun, sus, sux, swa, swe, syc, syr, tah, tai, tam, tat, tel, tem, ter, tet, tgk, tgl, tha, tig, tir, tiv, tkl, tlh, tli, tmh, tog, ton, tpi, tsi, tsn, tso, tuk, tum, tup, tur, tut, tvl, twi, tyv, udm, uga, uig, ukr, umb, und, urd, uzb, vai, ven, vie, vol, vot, wak, wal, war, was, wen, wln, wol, xal, xho, yao, yap, yid, yor, ypk, zap, zbl, zen, zha, zho, znd, zul, zun, zxx, zza, NA, NA, NA, NA
The following `from` values were not present in `x`: aar, abk, ace, ach, ada, ady, afa, afh, afr, ain, aka, akk, ale, alg, alt, amh, ang, anp, apa, ara, arc, arg, arn, arp, art, arw, asm, ast, ath, aus, ava, ave, awa, aym, aze, bad, bai, bak, bal, bam, ban, bas, bat, bej, bel, bem, ben, ber, bho, bih, bik, bin, bis, bla, bnt, bod, bos, bra, bre, btk, bua, bug, bul, byn, cad, cai, car, cat, cau, ceb, cel, ces, cha, chb, che, chg, chk, chm, chn, cho, chp, chr, chu, chv, chy, cmc, cop, cor, cos, cpe, cpf, cpp, cre, crh, crp, csb, cus, cym, dak, dan, dar, day, del, den, deu, dgr, din, div, doi, dra, dsb, dua, dum, dyu, dzo, efi, egy, eka, ell, elx, enm, epo, est, eus, ewe, ewo, fan, fao, fas, fat, fij, fil, fin, fiu, fon, fra, frm, fro, frr, frs, fry, ful, fur, gaa, gay, gba, gem, gez, gil, gla, gle, glg, glv, gmh, goh, gon, gor, got, grb, grc, grn, gsw, guj, gwi, hai, hat, hau, haw, heb, her, hil, him, hin, hit, hmn, hmo, hrv, hsb, hun, hup, hye, iba, ibo, ido, iii, ijo, iku, ile, ilo, ina, inc, ind, ine, inh, ipk, ira, iro, isl, ita, jav, jbo, jpn, jpr, jrb, kaa, kab, kac, kal, kam, kan, kar, kas, kat, kau, kaw, kaz, kbd, kha, khi, khm, kho, kik, kin, kir, kmb, kok, kom, kon, kor, kos, kpe, krc, krl, kro, kru, kua, kum, kur, kut, lad, lah, lam, lao, lat, lav, lez, lim, lin, lit, lol, loz, ltz, lua, lub, lug, lui, lun, luo, lus, mad, mag, mah, mai, mak, mal, man, map, mar, mas, mdf, mdr, men, mga, mic, min, mis, mkd, mkh, mlg, mlt, mnc, mni, mno, moh, mon, mos, mri, msa, mul, mun, mus, mwl, mwr, mya, myn, myv, nah, nai, nap, nau, nav, nbl, nde, ndo, nds, nep, new, nia, nic, niu, nld, nno, nob, nog, non, nor, nqo, nso, nub, nwc, nya, nym, nyn, nyo, nzi, oci, oji, ori, orm, osa, oss, ota, oto, paa, pag, pal, pam, pan, pap, pau, peo, phi, phn, pli, pol, pon, por, pra, pro, pus, que, raj, rap, rar, roa, roh, rom, ron, run, rup, rus, sad, sag, sah, sai, sal, sam, san, sas, sat, scn, sco, sel, sem, sga, sgn, shn, sid, sin, sio, sit, sla, slk, slv, sma, sme, smi, smj, smn, smo, sms, sna, snd, snk, sog, som, son, sot, spa, sqi, srd, srn, srp, srr, ssa, ssw, suk, sun, sus, sux, swa, swe, syc, syr, tah, tai, tam, tat, tel, tem, ter, tet, tgk, tgl, tha, tig, tir, tiv, tkl, tlh, tli, tmh, tog, ton, tpi, tsi, tsn, tso, tuk, tum, tup, tur, tut, tvl, twi, tyv, udm, uga, uig, ukr, umb, und, urd, uzb, vai, ven, vie, vol, vot, wak, wal, war, was, wen, wln, wol, xal, xho, yao, yap, yid, yor, ypk, zap, zbl, zen, zha, zho, znd, zul, zun, zxx, zza, NA, NA, NA, NA
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: V, D
The following `from` values were not present in `x`: U, D, R
The following `from` values were not present in `x`: U
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V, D, R, N
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 4
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 4
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 4, 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 4, 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 7
The following `from` values were not present in `x`: 8
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V, D, R, N
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: AT11, AT12, AT13, AT21, AT22, AT31, AT32, AT33, AT34, AU1, AU2, AU3, AU4, AU5, AU6, AU7, AU8, BE1, BE2, BE3, CA10, CA11, CA12, CA13, CA24, CA35, CA46, CA47, CA48, CA59, CA60, CA61, CH01, CH02, CH03, CH04, CH05, CH06, CH07, CL01, CL02, CL03, CL04, CL05, CL06, CL07, CL08, CL09, CL10, CL11, CL12, CL13, CL14, CL15, CN010, CN020, CN030, CN040, CN050, CN060, CN070, CN080, CN090, CN100, CN110, CN120, CN130, CN140, CN150, CN160, CN170, CN180, CN190, CN191, CN200, CN210, CN220, CN230, CN240, CN250, CN260, CN270, CN280, CN290, CN300, CN310, CY00, CZ01, CZ02, CZ03, CZ04, CZ05, CZ06, CZ07, CZ08, DE1, DE2, DE3, DE4, DE5, DE6, DE7, DE8, DE9, DEA, DEB, DEC, DED, DEE, DEF, DEG, DK01, DK02, DK03, DK04, DK05, EE00, ES11, ES12, ES13, ES21, ES22, ES23, ES24, ES30, ES41, ES42, ES43, ES51, ES52, ES53, ES61, ES62, ES63, ES64, ES70, FI13, FI18, FI19, FI1A, FI20, FR10, FR21, FR22, FR23, FR24, FR25, FR26, FR30, FR41, FR42, FR43, FR51, FR52, FR53, FR61, FR62, FR63, FR71, FR72, FR81, FR82, FR83, GR1, GR2, GR3, GR4, HU10, HU21, HU22, HU23, HU31, HU32, HU33, IE01, IE02, IL01, IL02, IL03, IL04, IL05, IL06, IL07, IN010, IN030, IN040, IN050, IN060, IN070, IN080, IN090, IN100, IN110, IN120, IN130, IN140, IN150, IN160, IN210, IN220, IN230, IN250, IN270, IN280, IN290, IN330, IN340, IN350, IS01, IS02, ITC1, ITC2, ITC3, ITC4, ITD1, ITD2, ITD3, ITD4, ITD5, ITE1, ITE2, ITE3, ITE4, ITF1, ITF2, ITF3, ITF4, ITF5, ITF6, ITG1, ITG2, JPA, JPB, JPC, JPD, JPE, JPF, JPG, JPH, JPI, JPJ, KR01, KR02, KR03, KR04, KR05, KR06, KR07, LU00, ME01, ME02, ME03, ME04, ME05, ME06, ME07, ME08, ME09, ME10, ME11, ME12, ME13, ME14, ME15, ME16, ME17, ME18, ME19, ME20, ME21, ME22, ME23, ME24, ME25, ME26, ME27, ME28, ME29, ME30, ME31, ME32, nl1, nl2, nl3, nl4, NO01, NO02, NO03, NO04, NO05, NO06, NO07, NZ01, NZ02, PL11, PL12, PL21, PL22, PL31, PL32, PL33, PL34, PL41, PL42, PL43, PL51, PL52, PL61, PL62, PL63, PT11, PT15, PT16, PT17, PT18, PT20, PT30, RU01, RU02, RU03, RU04, RU05, RU06, RU07, RU08, RU09, RU10, RU11, RU12, RU13, RU14, RU15, RU16, RU17, RU18, RU19, RU20, RU21, RU22, RU23, RU24, RU25, RU26, RU27, RU28, RU29, RU30, RU31, RU32, RU33, RU34, RU35, RU36, RU37, RU38, RU39, RU40, RU41, RU42, RU43, RU44, RU45, RU46, RU47, RU48, RU49, RU50, RU51, RU52, RU53, RU54, RU55, RU56, RU57, RU58, RU59, RU60, RU61, RU62, RU63, RU64, RU65, RU66, RU67, RU68, RU69, RU70, RU71, RU72, RU73, RU74, RU75, RU76, RU77, RU78, RU79, RU80, se11, se12, se21, se22, se23, se31, se32, se33, SI01, SI02, SK01, SK02, SK03, SK04, TR10, TR21, TR22, TR31, TR32, TR33, TR41, TR42, TR51, TR52, TR61, TR62, TR63, TR71, TR72, TR81, TR82, TR83, TR90, TRA1, TRA2, TRB1, TRB2, TRC1, TRC2, TRC3, UKC, UKD, UKE, UKF, UKG, UKH, UKI, UKJ, UKK, UKL, UKM, UKN, US01, US02, US04, US05, US06, US08, US09, US10, US11, US12, US13, US15, US16, US17, US18, US19, US20, US21, US22, US23, US24, US25, US26, US27, US28, US29, US30, US31, US32, US33, US34, US35, US36, US37, US38, US39, US40, US41, US42, US44, US45, US46, US47, US48, US49, US50, US51, US53, US54, US55, US56, NA
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: 0
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: BRR, FAY, JK1
The following `from` values were not present in `x`: 1, 3, 4
The following `from` values were not present in `x`: R
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V
The following `from` values were not present in `x`: V, R
The following `from` values were not present in `x`: V, M
There were 30 warnings (use warnings() to see them)
#+end_example

** init
:PROPERTIES:
:header-args:R:
:END:
#+begin_src R :tangle R/init.R
init_val = list(
kappa = rep(0.1, M),
tau = rep(0.1, N),
theta = rep(0.1, N),
beta = 0,
sigma = 1,
mu_beta = 0,
sigma_beta = 1
)
#+end_src

#+RESULTS:

#+begin_src R :tangle R/init.R
## lambda
params = c(init_val, list(
a_kappa = matrix(0.1,M,1),
b_kappa = matrix(0.1,M,1),
jump_kappa = matrix(0.5,M,1),

jump_beta = matrix(0.1,1,1),

## mu_theta = matrix(0.0,N,1)
## sigma_theta = matrix(1.0,N,1)
jump_theta = matrix(1.0,N,1),

a_sigma = 1.0,
b_sigma = 1.0,

a_tau = matrix(0.1,N,1),
b_tau = matrix(0.1,N,1),
jump_tau = matrix(0.5,N,1),

a_sigma_beta = 1.0,
b_sigma_beta = 1.0,

mu_mu_beta = 0.0,
sigma_mu_beta = 10.0
))
#+end_src

#+RESULTS:

#+begin_src R :tangle R/init.R
write_init(params)
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/preproc.R
write_init = function(params) {
## parameter init
readr::write_csv(as.data.frame(params$kappa),"input/init_kappa.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$tau),"input/init_tau.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$theta),"input/init_theta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$sigma),"input/init_sigma.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$beta),"input/init_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$mu_beta),"input/init_mu_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$sigma_beta),"input/init_sigma_beta.csv", col_names = FALSE)
## hyper-parameter + jump
readr::write_csv(as.data.frame(cbind(params$a_kappa,params$b_kappa,params$jump_kappa)),"input/hyper_kappa.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$jump_beta),"input/hyper_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(params$jump_theta),"input/hyper_theta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_sigma,params$b_sigma)),"input/hyper_sigma.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_tau,params$b_tau,params$jump_tau)),"input/hyper_tau.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$a_sigma_beta,params$b_sigma_beta)),"input/hyper_sigma_beta.csv", col_names = FALSE)
readr::write_csv(as.data.frame(cbind(params$mu_mu_beta,params$sigma_mu_beta)),"input/hyper_mu_beta.csv", col_names = FALSE)
  }
#+end_src

#+RESULTS:

** for-loop index
:PROPERTIES:
:header-args:R: :tangle diprom/R/preproc.R
:END:
#+begin_src R
write_loop_index = function() {
  fstr = NULL
  for (m in 1:M) {
    if(sum(dat$from==m)>0) {
      fstr = c(fstr, paste(which(dat$from == m) - 1, collapse = " ")) # R to C index conversion
    } else fstr = c(fstr, "-99")
  }
  fname = "input/out-of-state_index.txt"
  if (file.exists(fname)) {
    ##Delete file if it exists
    file.remove(fname)
  }
  fcon = file(fname)
  writeLines(fstr, fcon) # R to C index conversion
  close(fcon)

  fstr = NULL
  for (k in 1:max(item$pid)) {
    if(sum(dat$id==k)>0) {
      fstr = c(fstr, paste(which(dat$id == k) - 1, collapse = " ")) # R to C index conversion
    } else fstr = c(fstr, "-99")
}
fname = "input/person_index.txt"
if (file.exists(fname)) {
  ##Delete file if it exists
  file.remove(fname)
}
fcon = file(fname)
writeLines(fstr, fcon) # R to C index conversion
close(fcon)
}
#+end_src

#+begin_src R
write_loop_index_deprecated = function() {
sink("input/out-of-state_index.txt")
for (m in 1:M)
  if(sum(dat$from==m)>0) {
cat(which(dat$from == m) - 1,"\n") # R to C index conversion
  } else cat(-99,"\n")
sink()
sink("input/person_index.txt")
for (k in 1:max(item$pid))
  if(sum(dat$id==k)>0) {
cat(which(dat$id == k) - 1,"\n") # R to C index conversion
  } else cat(-99,"\n")
sink()
}
#+end_src

#+RESULTS:

* draft
We assume a continuously observed process, recurrent actions. The process seems to be irreversible, but I need to double check. See https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2017/10/multistate_enar_webinar.pdf

The intensity function $q_{ml}(\cdot)$ represents the instantaneous risk of moving from action $m$ to $l$. It may depend on covariates $\mathbf{z}(t)$, the time t itself, and possibly also the ‚Äúhistory‚Äù of the process up to that time, $\mathbf{F}_t$: the states previously visited or the length of time spent in them.

\begin{align*}
  q_{ml} (t ; \boldsymbol{\alpha}, \boldsymbol{\beta}, \mathbf{z}(t)) = & \lambda_{k,m \rightarrow l}(t) \exp( \alpha_m + \alpha_l + \boldsymbol{\beta}_{m,l}' \mathbf{z}_{i,m,l}(t) ),
\end{align*}
where $\boldsymbol{\alpha}$ is a vector of intercepts, and $\boldsymbol{\beta}$ is coefficients associated with $\mathbf{z}(t)$, $\lambda_{k,m\rightarrow l}(t)$ is a baseline intensity function. For each state $l$, there are competing transitions $m_1, \ldots, m_{n_l}$. This mean there are $n_{l}$ corresponding survival models for state $l$, and overall $K=\sum_l n_l$ models. Models with no shared parameters can be estimated separately.

What questions can you answer using these models? The below are quantities of interests in the multi-state survival model. All functions of the transition intensities (similar to the fact that estimable quantities are all functions of hazard rates in competing risk analysis) are estimable.

- total time that is expected to spend in state $l$ before time $t$.
- expected first passage time (first visit time to a state)
- expected number of visits to a state
- if possible, prob. of first action

In Discussion, we decided to use time-scale: in-homogeneous and number of terminal states: one.
* R packaging
#+begin_src emacs-lisp
(progn (setq default-directory "~/Dropbox/research/procmod/procmod-code/diprom")
(ess-r-devtools-clean-and-rebuild-package))
#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/misc.R :results silent
cl_box = function(y, cl, myname = NULL) {
  ## side by side box plot
dd <- data.frame(y = y, cl = cl)
boxp <- ggplot(dd, aes(x = factor(cl), y = y, fill = factor(cl))) +
  geom_boxplot() +
  theme(legend.position = "none")
if (!is.null(myname)) {
boxp = boxp + labs(y = myname[1], x = myname[2])}
print(boxp)
    }

cl_plot <- function(x, cl, myname = NULL, size_ = 1, xlim = NA, ylim = NA) {
  ## plot 2d array x
  ## mark points by groups specified by cl

  cl <- as.factor(cl)
  position <- as.data.frame(x)
  ndim <- dim(x)[2]

  colnames(position) <- paste("position", 1:ndim, sep = "")

  padding <- 1.05
  if (any(is.na(xlim))) {
    x1 <- -max(abs(position[, 1])) * padding
    x2 <- max(abs(position[, 1])) * padding
  } else {
    x1 <- xlim[1]
    x2 <- xlim[2]
  }
  if (any(is.na(ylim))) {
    y1 <- -max(abs(position[, 2])) * padding
    y2 <- max(abs(position[, 2])) * padding
  } else {
    y1 <- ylim[1]
    y2 <- ylim[2]
  }

  mytheme <- theme(
    axis.line = element_line(colour = "black"),
    ## panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    ## panel.border = element_blank(),
    panel.background = element_blank()
  )

  ## plot
  pp <- ggplot(position, aes(x = position1, y = position2, colour = cl)) +
    ## theme(text=element_text(size=20)) +
    ## geom_point()+
    xlim(x1, x2) +
    ylim(y1, y2) +
    xlab(myname[1]) +
    ylab(myname[2]) +
    ## xlab("Position 1") + ylab("Position 2") +
    geom_hline(yintercept = 0, color = "gray70", linetype = 2) +
    geom_vline(xintercept = 0, color = "gray70", linetype = 2) +
    mytheme
  pp <- pp + geom_point(size = size_)
  ## pp +
  ##   geom_text(
  ##     data = subset(position, idx == "w"), aes(x = position1, y = position2, label = (1:n_w), colour = cl_w),
  ##     ## segment.color = "grey50",
  ##     check_overlap = FALSE, show.legend = FALSE, size = 4
  ##   )
}
#+end_src

* Shell script to run C-code
#+begin_src sh :tangle run.sh
#!/usr/bin/env bash
# out_dir="party_invitations-1/"
# out_dir="cd_tally/"
# out_dir="sprained_ankle-1/"
# prename="R/party_invitations-1-preprocess.R"
# initname="R/party_invitations-1-init.R"

# first argument = output dir
out_dir=$1
n_chain=2
figlet "running MCMC"

echo "================================"
echo "output dir:" $out_dir
echo "preprocessing:" $prename
echo "initializing:" $initname
echo "n_chain:" $n_chain
echo "================================"

export STAN_NUM_THREADS=2
mkdir -p output
rm output/*
# rm input/*

# Rscript $prename
cp -r input output/
cp run.sh output/
# cp $prename output/
# cp $initname output/

for ((v = 1; v <= $n_chain; v++))
do
    # Rscript $initname
    ./main $v 10000 10000 10 parallel
done
## main function argument
 # chain #, iteration, burn-in, thin
 # parallel serial -> parallel computation?

mkdir -p $out_dir
mv output/* $out_dir
echo "Outputs are moved to" $out_dir"."
echo "================================"

Rscript R/run-analysis.R $out_dir $n_chain
echo "Post analysis is finished for " $out_dir"."
echo "================================"
#+end_src

* R post MCMC analysis
#+begin_src R :tangle R/run-analysis.R
#!/usr/bin/env Rscript
args <- commandArgs(trailingOnly = TRUE)
# test if there is at least one argument: if not, return an error

if (length(args) < 1) {
  stop("out_dir must be supplied.\n", call. = FALSE)
} else out_dir <- args[1]

if (length(args) == 2) {
num_chain <- args[2]
  } else if (length(args) == 1) {
num_chain = NULL
num_chain = get_nchain(out_dir)
  }
## out_dir <- "chessB-singleZ-singleW/"
system(paste0("rm figure/*.pdf"))
source("R/analysis.R")
system(paste0("mkdir -p ", out_dir, "figure/"))
system(paste0("rsync -rv figure/*.pdf ", out_dir, "figure/"))
#+end_src

set the output directory. run the chain of analytic tools. move the results to the output dir.
#+begin_src R :results none :async
setwd("~/workspace/procmod-code")

num_chain = 2

ldir = c(
"party_invitations-1/",
"party_invitations-2/",
"cd_tally/",
"sprained_ankle-1/",
## "sprained_ankle-2/", ## didn't run
"tickets/",
## "class_attendance/", ## didn't run
"club_membership-1/",
## "club_membership-2/", ## didn't run
"book_order/",
"meeting_room/",
## "reply_all/", ## failed
"locate_email/",
"lamp_return/")

## out_dir="party_invitations-1/"
## out_dir="party_invitations-2/"
## out_dir="cd_tally/"
## out_dir="sprained_ankle-1/"
## out_dir="sprained_ankle-2/"
## out_dir="tickets/"
## out_dir="class_attendance/"
## out_dir="club_membership-1/"
## out_dir="club_membership-2/"
## out_dir="book_order/"
## out_dir="meeting_room/"
## out_dir="reply_all/"
## out_dir="locate_email/"
## out_dir="lamp_return/"

for (out_dir in ldir) {
system(paste0("rm figure/*.pdf"))
system(paste0("mkdir -p figure/"))

source("R/analysis.R")

system(paste0("mkdir -p ", out_dir, "figure/"))
system(paste0("rsync -rv figure/*.pdf ", out_dir,"figure/"))
}
#+end_src

#+begin_src R :results none :tangle R/Renviron.R
library(diprom)
## library(coda)
## library(dplyr)
## library(ggplot2)
## library(stringr)
## library(magrittr)
## library(bayesplot)
## library(foreach)
## library(doParallel)
stopImplicitCluster()
## doParallel::registerDoParallel(2)
registerDoParallel(cores = detectCores() - 1)
#+end_src

#+RESULTS:
#+begin_src R :tangle R/analysis.R
source("R/Renviron.R")
source("R/load-outputs.R")
source("R/summary.R")
## source("R/visual.R")
#+end_src

* load outputs
This section contains scripts to create an mcmc object. 1) read MCMC samples, 2) set their column names, and 3) Procrustean matching.
#+begin_src R :tangle diprom/R/post.R
set_cnames = function(M = M_, N = N_, dq = dq_, dn = dn_, dc = dc_) {
  cnames <- c(".chain", ".iteration")

  ## CSVFormat prints by row-major order!
  for (m in 1:M) {
    cnames <- c(cnames, paste0("kappa.", m))
  }
  for (m in 1:N) {
    cnames <- c(cnames, paste0("tau.", m))
  }
  for (k in 1:N) {
    cnames <- c(cnames, paste0("theta.", k))
  }
  cnames <- c(cnames, "sigma", "beta", "mu_beta", "sigma_beta", "llike_", "lpri_", "lp_")
  return(cnames)
}

get_nchain = function(out_dir) {
  ## use all sample files if nchain is not set
  ## assign if NULL
  if (is.null(num_chain)) {
    num_chain <- length(list.files(path = out_dir, pattern="sample_chain[0-9]+\\.csv"))
  }
  return(num_chain)
}

read_output = function(num_chain, cnames) {
  dlist <- list()
  for (cid in 1:num_chain) {
    dlist[[cid]] <- readr::read_csv(paste0(out_dir, "sample_chain", cid, ".csv"), col_names = F, skip = 0) %>% as.data.frame()
    colnames(dlist[[cid]]) <- cnames
  }
  return(dlist)
}

#+end_src

#+begin_src R :tangle R/load-outputs.R :results silent
mvar <- readr::read_csv(paste0(out_dir, "input/mvar.csv"), col_names = F) %>% as.matrix()
M = M_ = mvar[1]
N = N_ = mvar[2]
dq = dq_ = mvar[3]
dn = dn_ = mvar[4]
dc = dc_ = mvar[5]

cnames = diprom::set_cnames(M, N, dq, dn, dc)
dlist = diprom::read_output(num_chain, cnames)

## unlist
## df <- bind_rows(dlist, .id = "column_label")

mclist <- mcmc.list()
for (cid in 1:num_chain) {
  mclist[[cid]] <- mcmc(dlist[[cid]])
}
#+end_src

#+RESULTS:
* summary
The posterior means of \beta, \theta, \lambda are exported to CSV files.
#+begin_src R :tangle R/summary.R
param_path = paste0(out_dir, "param_mean.csv")
if (file.exists(param_path)) {
  dout = readr::read_csv(param_path)
} else {
  ss <- summary(mclist)
  mm <- ss$statistics[, "Mean"]
  ## rr <- c(grep("^kappa", names(mm)), grep("^tau", names(mm)))
  ## dout <- data.frame(vname = names(mm[rr]), mean = mm[rr])
  dout <- data.frame(vname = names(mm), mean = mm)
  readr::write_csv(dout, param_path)
}
#+end_src

#+RESULTS:
:
: [36m‚îÄ‚îÄ[39m [1m[1mColumn specification[1m[22m [36m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[39m
: cols(
:   vname = [31mcol_character()[39m,
:   mean = [32mcol_double()[39m
: )

[[file:figure/theta_res.pdf]]
[[file:figure/theta_tau_res.pdf]]
[[file:figure/tau_res.pdf]]
[[file:figure/time_action.pdf]]
[[file:figure/time_action_more.pdf]]
#+begin_src R :results silent :tangle R/summary.R
mtheta <- dout[grep("^theta", dout$vname), 2]
mtau <- dout[grep("^tau", dout$vname), 2]
mkappa <- dout[grep("^kappa", dout$vname), 2]
dat <- readr::read_csv(paste0(out_dir, "input/dat.csv"), col_names = FALSE)
rr <- dat[, c(1, 8)] %>% as.data.frame()
rr <- rr[!duplicated(rr), 2]
#+end_src

#+begin_src R :results silent :tangle R/summary.R
item <- readr::read_csv(paste0(out_dir, "input/item.csv"), col_names = TRUE)
gg <- item %>%
  group_by(pid) %>%
  summarize(ftime = timestamp[1] / 1000, naction = n(), spd = naction / ftime, time = timestamp[n()] / 1000)
binfo = readr::read_csv("./data/PIAAC_cleaned_data_1110/PUFs_noN.csv")
#+end_src

#+begin_src R
pid = unique(item$SEQID)

#+end_src

#+RESULTS:



#+begin_src R :results silent :tangle R/summary.R
pdf("figure/theta_res.pdf")
dd <- data.frame(theta = mtheta, res = rr)
boxp <- ggplot(dd, aes(x = factor(res), y = theta, fill = factor(res))) +
  geom_boxplot() +
  theme(legend.position = "none")
print(boxp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/tau_res.pdf")
dd <- data.frame(tau = mtau, res = rr)
boxp <- ggplot(dd, aes(x = factor(res), y = tau, fill = factor(res))) +
  geom_boxplot() +
  theme(legend.position = "none")
print(boxp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/theta_tau_res.pdf")
pp <- cl_plot(cbind(mtheta, log(mtau)), rr, c("theta", "log(tau)"))
print(pp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results silent :tangle R/summary.R
pdf("figure/time_action_more.pdf")
cl_box(log(gg$time), rr, c("log(time)", "res"))
cl_box(log(gg$ftime / gg$time), rr, c("log(ftime / time)", "res"))
cl_box(log(gg$time - gg$ftime), rr, c("log(time - ftime)", "res"))
cl_box(log(gg$naction / gg$time), rr, c("log(#action / time)", "res"))
pp <- cl_plot(cbind(log(gg$time), log(gg$naction)), rr, c("log(time)", "log(#actions)"))
print(pp)
pp <- cl_plot(cbind(log(gg$time), log(gg$ftime)), rr, c("log(time)", "log(ftime)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$time)), rr, c("theta", "log(time)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$time - gg$ftime)), rr, c("theta", "log(time - ftime)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$naction / gg$time)), rr, c("theta", "log(#action / time)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$naction / (gg$time - gg$ftime))), rr, c("theta", "log(#action / (time - ftime))"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$time)), rr, c("tau", "log(time)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$time - gg$ftime)), rr, c("tau", "log(time - ftime)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$naction / gg$time)), rr, c("tau", "log(#action / time)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$naction / (gg$time - gg$ftime))), rr, c("tau", "log(#action / (time - ftime))"))
print(pp)
dev.off(which = dev.cur())
#+end_src

- first action time and number of actions
#+begin_src R :results silent :tangle R/summary.R
pdf("figure/time_action.pdf")
cl_box(log(gg$ftime), rr, c("log(first_action_time)", "res"))
cl_box(log(gg$naction), rr, c("log(naction)", "res"))
pp <- cl_plot(cbind(mtheta, log(gg$ftime)), rr, c("theta", "log(first_action_time)"))
print(pp)
pp <- cl_plot(cbind(mtheta, log(gg$naction)), rr, c("theta", "log(#actions)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$ftime)), rr, c("tau", "log(first_action_time)"))
print(pp)
pp <- cl_plot(cbind(mtau, log(gg$naction)), rr, c("tau", "log(#actions)"))
print(pp)
pp <- cl_plot(cbind(log(gg$ftime), log(gg$naction)), rr, c("log(first_action_time)", "log(#action)"))
print(pp)
dev.off(which = dev.cur())
#+end_src

#+begin_src R :results none
## sink("output/mcmc_summary.txt")
cat("==================================")
cat("Rejection Rate")
cat("==================================")
rejectionRate(mclist)
cat("==================================")
cat("Effective Size")
cat("==================================")
effectiveSize(mclist)
cat("==================================")
cat("Summary")
cat("==================================")
summary(mclist)
## sink()
#+end_src

* MCMC trace plots
#+begin_src R :tangle diprom/R/post.R
plot_intervals = function(param, trans_ = "log", save_plot = TRUE){
  regexp = paste0("^",param,"\\.")
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_intervals.pdf"))
  p0 <- bayesplot::mcmc_intervals(
    mclist,
    regex_pars = regexp,
    transformations = trans_
  )
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
  print(p0)
  if (save_plot) dev.off(which = dev.cur())
}

plot_parcoord = function(param, trans_ = "log", save_plot = TRUE){
  regexp = paste0("^",param,"\\.")
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_parcoord.pdf"))
  p0 <- bayesplot::mcmc_parcoord(
    mclist,
    regex_pars = regexp,
    transformations = trans_
  )
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
  print(p0)
  if (save_plot) dev.off(which = dev.cur())
}

#+end_src

#+RESULTS:

#+begin_src R :tangle diprom/R/post.R
plot_trace = function(param, max_, trans_ = "log", save_plot = TRUE){
  if (save_plot) pdf(paste0("figure/",param,"_mcmc_trace.pdf"))
    regexp = paste0("^",param,"\\.[0-9]$")
  for (ii in 0:floor(max_/10)) {
 if (ii > 0) regexp = paste0("^",param,"\\.", ii, "[0-9]$")
  p <- bayesplot::mcmc_trace(
    mclist,
    regex_pars = regexp,
    transformations = trans_,
    facet_args = list(nrow = 4, labeller = label_parsed)
  )
  print(p <- p + bayesplot::facet_text(size = 10))
  ## mcmc_areas(
  ##  lambda0.sam,
  ##  prob = 0.8, # 80% intervals
  ##  prob_outer = 0.99, # 99%
  ##  point_est = "mean"
  ## )
 }
  if (save_plot) dev.off(which = dev.cur())
  }
#+end_src

#+RESULTS:

[[file:figure/theta_mcmc_intervals.pdf]]
[[file:figure/tau_mcmc_intervals.pdf]]
[[file:figure/theta_mcmc_intervals.pdf]]
#+begin_src R :tangle R/visual.R
diprom::plot_intervals("kappa")
diprom::plot_intervals("tau")
diprom::plot_intervals("theta", list())
#+end_src

#+RESULTS:
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'

[[file:figure/theta_mcmc_parcoord.pdf]]
[[file:figure/tau_mcmc_parcoord.pdf]]
[[file:figure/theta_mcmc_parcoord.pdf]]
#+begin_src R
diprom::plot_parcoord("kappa")
diprom::plot_parcoord("tau")
diprom::plot_parcoord("theta", list())
#+end_src

#+RESULTS:
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'
: Error in array(NA, dim = c(n_iter, n_chain, n_param)) :
:   'list' object cannot be coerced to type 'integer'

[[file:figure/kappa_mcmc_trace.pdf]]
[[file:figure/tau_mcmc_trace.pdf]]
[[file:figure/theta_mcmc_trace.pdf]]
#+begin_src R :tangle R/visual.R
bayesplot::color_scheme_set("mix-blue-pink")
diprom::plot_trace("kappa", M)
diprom::plot_trace("tau", N)
diprom::plot_trace("theta", 20, list())
#+end_src

#+RESULTS:
: Error in facet_text(size = 10) : could not find function "facet_text"
: Error in facet_text(size = 10) : could not find function "facet_text"
: Error in facet_text(size = 10) : could not find function "facet_text"

[[file:figure/other_params_trace.pdf]]
#+begin_src R :tangle R/visual.R
pdf("figure/other_params_trace.pdf")
bayesplot::color_scheme_set("mix-blue-pink")
p <- bayesplot::mcmc_trace(mclist[[1]],
                           pars = c("sigma", "beta", "mu_beta", "sigma_beta", "llike_", "lpri_", "lp_"),
                           facet_args = list(nrow = 3, labeller = label_parsed)
                           )
print(p <- p + bayesplot::facet_text(size = 10))
dev.off(which = dev.cur())
#+end_src

#+RESULTS:
: pdf
:   2

* word2vec code
** COMMENT email transitions
#+begin_src R
library(dplyr)
load('data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
email = PS[PS$CODEBOOK == "U01a000S",]

## only core event description included
## core_event = c("MAIL_DRAG", "MAIL_VIEWED", "FOLDER_VIEWED", "MAIL_MOVED")
core_event = c("MAIL_DRAG", "MAIL_MOVED")
email$event_description[!(email$event_type %in% core_event)] <- ""

email$action = paste(email$event_type, email$event_description)
email$action[email$event_num == 1] = "START"
email$action[email$action == "END END"] = "END"
## email$action = email$event_type
ee = email %>% select(SEQID, event_type, event_description)
#+end_src

#+begin_src R
n = nrow(email)
trans = paste(email$action[1:(n-1)], "->", email$action[2:n])
trans = trans[trans != "END -> START"]
#+end_src

#+RESULTS:

#+begin_src R
aa = email
aa = aa %>% filter(event_description != "") %>% select(SEQID,event_type,event_description)
#+end_src

#+RESULTS:

#+begin_src R
length(unique(email$event_type))
length(unique(email$action))
length(unique(trans))
#+end_src

#+RESULTS:
: [1] 51
: [1] 109
: [1] 752

: [1] 1774

#+begin_src R
tab = table(trans)
ntab = as.numeric(tab)
summary(ntab)
sum(ntab > 1)
#+end_src

#+RESULTS:
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
:     1.0     1.0     2.0    45.1     9.0  3932.0
: [1] 479

#+begin_src R
toend = grepl("END$",trans)
nonext = grepl("NEXT\\_INQUIRY REQUEST ->",trans)
trans[toend & !nonext]
#+end_src

#+RESULTS:
: character(0)

#+begin_src R
texton = grepl("TEXTBOX_ONFOCUS", trans)
textoff = grepl("TEXTBOX_KILLFOCUS", trans)
#+end_src

#+RESULTS:

#+begin_src R
edes = email$event_description
cfolder = grepl("createfoldernameinput", edes)
email$SEQID[cfolder]
#+end_src

#+RESULTS:
: numeric(0)

#+begin_src R :tangle no
email %>% filter(SEQID == 4444) %>% select(event_type, event_description)
#+end_src

#+begin_src R :tangle no
email %>% filter(SEQID == 782) %>% select(event_type, event_description)
#+end_src

#+RESULTS:
#+begin_example
           event_type event_description
1               START
2         MAIL_VIEWED
3         MAIL_VIEWED
4         MAIL_VIEWED
5         MAIL_VIEWED
6         MAIL_VIEWED
7         MAIL_VIEWED
8         MAIL_VIEWED
9         MAIL_VIEWED
10        MAIL_VIEWED
11      FOLDER_VIEWED
12               MENU
13 MENUITEM_newfolder
14      FOLDER_VIEWED
15    TEXTBOX_ONFOCUS
16           KEYPRESS
17  TEXTBOX_KILLFOCUS
18      ADD_FOLDER_ok
19        MAIL_VIEWED
20       TOOLBAR_help
21  TOOLBAR_replymail
22        MAIL_VIEWED
23        MAIL_VIEWED
24        MAIL_VIEWED
25        MAIL_VIEWED
26               MENU
27    MENUITEM_delete
28        MAIL_VIEWED
29               MENU
30    MENUITEM_delete
31    TOOLBAR_mailApp
32      FOLDER_VIEWED
33               MENU
34 MENUITEM_newfolder
35      FOLDER_VIEWED
36      ADD_FOLDER_ok
37       NEXT_INQUIRY
38                END
#+end_example
** COMMENT Keras word2vec
:PROPERTIES:
:header-args:R: :results silent :exports both :noweb yes :eval never-export
:END:

see https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/

#+begin_src emacs-lisp
;; python
(require 'conda)
(conda-env-activate "tf")
#+end_src

#+RESULTS:
: Switched to conda environment: /Users/yunj/.conda/envs/r-tensorflow/

#+begin_src R :tangle word2vec.R
library(readr)
library(stringr)
reviews <- read_lines("finefoods.txt.gz", n_max = 100)
reviews <- reviews[str_sub(reviews, 1, 12) == "review/text:"]
reviews <- str_sub(reviews, start = 14)
reviews <- iconv(reviews, to = "UTF-8")
#+end_src

#+begin_src R :tangle word2vec.R
library(keras)
tokenizer <- text_tokenizer(num_words = 200)
tokenizer %>% fit_text_tokenizer(reviews)
#+end_src

#+begin_src R :tangle word2vec.R
library(reticulate)
library(purrr)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
  gen <- texts_to_sequences_generator(tokenizer, sample(text))
  function() {
    skip <- generator_next(gen) %>%
      skipgrams(
        vocabulary_size = tokenizer$num_words,
        window_size = window_size,
        negative_samples = 0
      )
    x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
    y <- skip$labels %>% as.matrix(ncol = 1)
    list(x, y)
  }
}
#+end_src

#+begin_src R :tangle word2vec.R
## embedding_size <- 128  # Dimension of the embedding vector.
embedding_size <- 8  # Dimension of the embedding vector.
skip_window <- 1       # How many words to consider left and right.
num_sampled <- 1       # Number of negative examples to sample for each word.
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
#+end_src

#+begin_src R :tangle word2vec.R
embedding <- layer_embedding(
  input_dim = tokenizer$num_words + 1,
  output_dim = embedding_size,
  input_length = 1,
  name = "embedding"
)

target_vector <- input_target %>%
  embedding() %>%
  layer_flatten()

context_vector <- input_context %>%
  embedding() %>%
  layer_flatten()

dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")

#+end_src

#+begin_src R :tangle word2vec.R


model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
summary(model)

#+end_src


#+begin_src R :tangle word2vec.R
model %>%
  fit_generator(
    skipgrams_generator(reviews, tokenizer, skip_window, negative_samples),
    ## steps_per_epoch = 100000, epochs = 5
    steps_per_epoch = 100, epochs = 5
    )
#+end_src

** email word2vec preprocessing
:PROPERTIES:
:header-args:R: :results silent :tangle email_word2vec_preproc.r :eval never-export
:END:
goal: process actions to be used for =word2vec=
- [-] remove =event_description= with no information
- [-] remove *consecutive* action repetition.
- [X] concat =event_type= and =event_description=
- [X] substitue SPACE with "_"
- [X] concat actions into a sequence
- [X] remove START and END.
- [X] write sequences to a txt file

  #+begin_src R :tangle no
source("email_word2vec_preproc.r")
  #+end_src

- keep a small number of event_description that are highly relevant to movement of emails.
- remove action taking < 50ms ([[https://www.tobiipro.com/learn-and-support/learn/eye-tracking-essentials/how-fast-is-human-perception/][How fast is human perception? About 80 ms | Find out more]])
#+begin_src R
library(dplyr)
library(stringr)
setwd('~/workspace/procmod-code/')
load('./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_no_missing.rdata')
email = PS %>% filter(CODEBOOK == "U01a000S")

## core_event = c("MAIL_DRAG", "MAIL_MOVED")
core_event = c("MAIL_DRAG", "MAIL_MOVED", "MAIL_VIEWED")
## core_event = c("MAIL_MOVED")
email$event_description[!(email$event_type %in% core_event)] <- ""
## email$event_description <- ""

timestamp = email$timestamp
diff = c(diff(timestamp), 99999)
email$diff = diff
email = email[(diff > 99) || (diff < 0 ), ]

email$event_description = stringr::str_replace(email$event_description, "(.*)\\*\\$target=u01a_(.*)",  "\\1\\2")
email$event_description = stringr::str_replace(email$event_description, "id=u01a_",  "")
email$event_description = stringr::str_replace(email$event_description, "\\*\\$target=",  "")

email = email %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

we need to remove transition between the same action
#+begin_src R
ww = email$word
id = email$SEQID
ww0 = ww[1:(length(ww)-1)]
ww1 = ww[2:(length(ww))]

dup = ww0 == ww1
dup0 = c(dup,  FALSE )
dup1 = c( FALSE ,  dup)

idx = dup1

cw = "NULL"
cid = 9999999999
for (kk in which(dup1)) {
  if(cw != ww[kk] && cid != id[kk]) idx[kk] = FALSE
  cw = ww[kk]
  cid = id[kk]
}

email = email[!idx, ]
#+end_src


#+begin_src R
n = nrow(email)
pre = email$word[1:(n-1)]
cur = email$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

email = email[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(email$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in email$word[email$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(email$word[email$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

this leave only one action in some cases... let's keep start and end
#+begin_src R
## for (i in id) {
##     seqs[i] = gsub("START ", "", seqs[i])
##     seqs[i] = gsub(" END", "", seqs[i])
## }
seqs = seqs[id]
#+end_src

#+RESULTS:


#+begin_src R
data.table::fwrite(as.data.frame(seqs), "email_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R :tangle no
vv = numeric(max(id))
for (i in id) {
  vv[i] =length(email$word[email$SEQID == i])
  }

length(unique(email$word))
n = nrow(email)
trans = paste(email$word[1:(n-1)], "->", email$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
90 * 89 / 2

ee = PS %>% filter(CODEBOOK == "U01a000S")
ww = paste(ee$event_type, ee$event_description)
length(unique(ww))
nn = nrow(ee)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src
** email word2vec
window_size = 2 should be [5,20] for a small data set.
(negative samples) num_ns = 4
See [[file:email_word2vec.ipynb]]
Download the vectors.tsv and metadata.tsv to analyze the obtained embeddings in the Embedding Projector. https://projector.tensorflow.org/
* email subject segmentation
#+begin_src R :results silent
source("email_word2vec_preproc.r")
#+end_src

#+begin_src R :results silent
vv = readr::read_tsv("vectors.tsv")
mm = readr::read_tsv("metadata.tsv")

ww = email$word
#+end_src

#+begin_src R
myvv = matrix(0, length(id), ncol(vv))
mymm=numeric(length(id))

idx =0
for (ii in id) {
  idx=idx+1
  ss=ww[email$SEQID==ii]
  ss=ss[ss != "START"]
  ss=ss[ss != "END  "]
 ss=ss[ss != "END_CANCEL"]
  ## ss=ss[ss != "MENUITEM_help"]
ee = numeric(length(ss))
  for (kk in 1:length(ss)) {
    ee[kk]= which(ss[kk] == mm)
  }
  myvv[idx,]=colMeans(vv[ee,])
  mymm[idx]=email$response[email$SEQID==ii][1]
}

readr::write_tsv(as.data.frame(mymm),"mymm.tsv", col_names=F)
readr::write_tsv(as.data.frame(myvv),"myvv.tsv", col_names=F)
#+end_src

#+RESULTS:

- I didn't remove START and END.
- those with small event duration are deleted.
- centroid of action sequence is calculated and used for the clustering.
- timestamp information is not used for clustring
TSNE can identify many subect groups. wouldn't that be useful for futher analysis?
* Football ticket

#+begin_src R
library(dplyr)
library(stringr)
load('./data/PIAAC_cleaned_data_1110/Problem solving/Problem-solving_cleaned_1110.rdata')
df = PS %>% filter(CODEBOOK == "U21x000S")

ww = paste(df$event_type, ee$event_description)
length(unique(ww))
nn = nrow(df)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src

#+begin_src R
core_event = c("COMBOBOX", "TAB", "CHECKBOX")
## core_event = c("MAIL_MOVED")
df$event_description[!(df$event_type %in% core_event)] <- ""
timestamp = df$timestamp
diff = c(0, diff(timestamp))
df = df[diff > 50, ]

df$event_description = stringr::str_replace_all(df$event_description, "\\*\\$index=", "")
df$event_description = stringr::str_replace_all(df$event_description, "u021_(.*?)_","")
df$event_description = stringr::str_replace(df$event_description, "id=",  "")
df$event_description = stringr::str_replace(df$event_description, "button",  "")

df = df %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

#+begin_src R
n = nrow(df)
pre = df$word[1:(n-1)]
cur = df$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

df = df[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(df$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in df$word[df$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(df$word[df$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

#+begin_src R
for (i in id) {
    seqs[i] = gsub("START ", "", seqs[i])
    seqs[i] = gsub(" END", "", seqs[i])
}
seqs = seqs[id]
#+end_src

#+RESULTS:


#+begin_src R
data.table::fwrite(as.data.frame(seqs), "ticket_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R
mm = 0
for (i in id) {
  mm  = max(mm , length(df$word[df$SEQID == i]))
  }
mm
length(unique(df$word))
n = nrow(df)
trans = paste(df$word[1:(n-1)], "->", df$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
#+end_src
* CD Tally :ATTACH:
:PROPERTIES:
:header-args:R: :results silent :session *R-PIACC* :exports both :noweb yes :eval never-export
:ID:       673df774-623f-44b5-a262-b22739c9a506
:END:

#+attr_org: :width 700
[[attachment:_20210426_063856screenshot.png]]

[[https://piaac-logdata.tba-hosting.de/confidential/problemsolving/CDTallyPart1/pages/cd-start.html][CD Tally Part 1]]
- look at the spreadsheet and calculate the value

#+begin_src R
library(dplyr)
library(stringr)
load('./data/PIAAC_cleaned_data_1110/Problem_solving/Problem-solving_cleaned_1110.rdata')
df = PS %>% filter(CODEBOOK == "U03a000S")

ww = paste(df$event_type, df$event_description)
length(unique(ww))
nn = nrow(df)
trans = paste(ww[1:(nn-1)], "->", ww[2:nn])
length(unique(trans))
#+end_src

#+begin_src R
core_event = c("COMBOBOX", "TAB", "CHECKBOX")
## core_event = c("MAIL_MOVED")
df$event_description[!(df$event_type %in% core_event)] <- ""
timestamp = df$timestamp
diff = c(diff(timestamp), 9999)
df = df[diff > 99, ]

df$event_description = stringr::str_replace_all(df$event_description, "\\*\\$index=", "")
df$event_description = stringr::str_replace_all(df$event_description, "u021_(.*?)_","")
df$event_description = stringr::str_replace(df$event_description, "id=",  "")
df$event_description = stringr::str_replace(df$event_description, "button",  "")

df = df %>% mutate(event_description = ifelse(event_type == "START","",event_description)) %>%
mutate(event_description = ifelse(event_type == "END","",event_description)) %>%
  mutate(event_description = ifelse(event_type == "KEYPRESS","",event_description)) %>%
  mutate(event_concat = ifelse(event_description == "", event_type, paste0(event_type,"-",event_description))) %>%
mutate(word = gsub(" ", "_", event_concat))
#+end_src

#+begin_src R
n = nrow(df)
pre = df$word[1:(n-1)]
cur = df$word[2:n]
dif = c(TRUE, !(cur == pre))

idx = logical(n)
for (i in 2:n) {
if(dif[i] == FALSE && dif[i-1] == FALSE) {
  idx[i] = TRUE
  }
}

df = df[!idx,]
#+end_src

#+RESULTS:

#+begin_src R
id = unique(df$SEQID)
seqs = character(length(id))
for (i in id) {
  ## for (word in df$word[df$SEQID == i]) {
    ## seqs[i] = paste0(seqs[i], " ", word)
  ## }
  seqs[i] = paste(df$word[df$SEQID == i] , collapse = " ")
}
#+end_src

#+RESULTS:

#+begin_src R
for (i in id) {
    seqs[i] = gsub("START ", "", seqs[i])
    seqs[i] = gsub(" END", "", seqs[i])
}
seqs = seqs[id]
#+end_src

#+RESULTS:

#+begin_src R
data.table::fwrite(as.data.frame(seqs), "ticket_sentence.txt", col.names=F)
#+end_src

#+RESULTS:

#+begin_src R
mm = 0
for (i in id) {
  mm = max(mm, length(df$word[df$SEQID == i]))
  }
mm
length(unique(df$word))
n = nrow(df)
trans = paste(df$word[1:(n-1)], "->", df$word[2:n])
trans = trans[trans != "END -> START"]
length(unique(trans))
#+end_src
* Documentations
** Home
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_TITLE: Process data modeling
:END:
#+begin_export html
# WIP: Process data modeling
To be shared during meetings.
#+end_export

** PIACC
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: piacc
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 1
:EXPORT_TITLE: PIACC
:END:

#+begin_src toml :front_matter_extra t :exports none
pre = "<b>1. </b>"
#+end_src

#+begin_export html
### Chapter 1

# PIACC
#+end_export

Programme for the International Assessment of Adult Competencies (PIAAC)

** item home
:PROPERTIES:
:header-args:R: :tangle R/itemcode.R :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: piacc/item_home
:END:

See the codebook for details....... [[file:data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv][PS_BOOKLET_ITEM.csv]]


#+begin_src R
booklet = readr::read_csv("~/Dropbox/research/procmod/procmod-code/data/PIAAC_cleaned_data_1110/Problem_solving/PS_BOOKLET_ITEM.csv")
booklet$NAME = stringr::str_replace_all(booklet$NAME, " ", "_")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "_-", "-")
booklet$NAME = stringr::str_replace_all(booklet$NAME, "-_", "-")
booklet$NAME = tolower(booklet$NAME)
#+end_src

**** party_invitations-1 :ATTACH:
:PROPERTIES:
:ID:       67fea2ad-8240-45dc-a4eb-da07f34a9e45
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U01a000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01a_(.*)","\\1\\2"), c("id=u01a_",  ""), c("\\*\\$target=",  ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_171956pi-start.png]]

**** party_invitations-2 :ATTACH:
:PROPERTIES:
:ID:       da5e5603-defc-43fb-a01c-16bfb293c5d8
:END:
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)

if (item_code == "U01b000S") {
sub_str = rbind(c("(.*)\\*\\$target=u01b_(.*)","\\1\\2"), c("id=u01b_",  ""), c("\\*\\$target=",  ""), c("\\*\\$value=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172034pi-home.png]]

**** cd_tally :ATTACH:
:PROPERTIES:
:ID:       0cfd0f21-14b5-4993-a006-d6c13f9027ef
:END:
#+begin_src R :results value list drawer silent
## instruction is not clear. too many actions
## respondents got confused (create or not create a folder to organize??)
if (item_code == "U03a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172043cd-start.png]]

**** sprained_ankle-1 :ATTACH:
:PROPERTIES:
:ID:       9c54e9eb-4153-4770-8b4c-0616c15bcebe
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U06a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

[[attachment:_20210904_172056sa-start.png]]

**** sprained_ankle-2 :ATTACH:
:PROPERTIES:
:ID:       c9b8890f-d9d6-4aa1-9505-48ed98d4f657
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U06b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172105sa-start2.png]]

**** tickets :ATTACH:
:PROPERTIES:
:ID:       47fa0d97-e0ab-44e8-87c7-dc6f6946f786
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U21x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172128ft-home.png]]

**** class_attendance :ATTACH:
:PROPERTIES:
:ID:       994a4a41-2040-4276-bc5f-f67966d69bde
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U04a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172140ca-email-home.png]]

**** club_membership-1 :ATTACH:
:PROPERTIES:
:ID:       50a5f4a2-d333-4bb2-8639-9a0dbf3d7a09
:END:
#+begin_src R :results value list drawer silent
if (item_code == "U19a000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172148cm-start.png]]

**** club_membership-2 :ATTACH:
:PROPERTIES:
:ID:       549f8614-8703-4373-b745-b44e3a7b84e3
:END:
#+begin_src R
if (item_code == "U19b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172157cm-home.png]]

**** book_order :ATTACH:
:PROPERTIES:
:ID:       fcf87b1e-7433-422d-9ce4-3770e71cbb6a
:END:
#+begin_src R
if (item_code == "U07x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172208bo-start.png]]

**** meeting_room :ATTACH:
:PROPERTIES:
:ID:       d9736569-11b6-4084-94a8-9782473453e1
:END:
#+begin_src R
if (item_code == "U16x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172218mr-email-home.png]]

**** reply_all :ATTACH:
:PROPERTIES:
:ID:       265dfcf6-ec9b-483e-b82b-8745a606ad0d
:END:
#+begin_src R
if (item_code == "U02x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172228ra-home.png]]

**** locate_email :ATTACH:
:PROPERTIES:
:ID:       ccbb321c-a463-47ba-bcf7-ac40fe1e8ff6
:END:
#+begin_src R
if (item_code == "U11b000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172236le-home.png]]

**** lamp_return :ATTACH:
:PROPERTIES:
:ID:       e0aef17d-55dd-4641-a529-f63b2acd9d92
:END:
#+begin_src R
if (item_code == "U23x000S") {
sub_str = rbind(
  c("(.*)\\*\\$target=u03a_(.*)","\\1\\2"),
  c("id=u03a_",  ""),
  c("id=u04a_",  ""),
  c("id=u06a_",  ""),
  c("\\*\\$target=", ""),
  c("\\*\\$value=", ""),
  c("\\*\\$index=", ""),
  c("\\*\\$href=", ""))
ignore_desc = c(
"START",
"END",
"KEYPRESS")
}
#+end_src

#+attr_org: :width 700
[[attachment:_20210904_172248lr-home.png]]

** item codebook
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: piacc/item_codebook
:END:

| ITEM  | NAME                  | CODEBOOK | RESPONSE                                                                 |
|-------+-----------------------+----------+--------------------------------------------------------------------------|
| PS1_1 | Party Invitations - 1 | U01a000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS1_2 | Party Invitations - 2 | U01b000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_3 | CD Tally              | U03a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_4 | Sprained Ankle - 1    | U06a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_5 | Sprained Ankle - 2    | U06b000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_6 | Tickets               | U21x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS1_7 | Class Attendance      | U04a000S | POLYTOMOUS (0 to 2), No response (R ), Not reached / not attempted (N)   |
| PS2_1 | Club Membership - 1   | U19a000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_2 | Club Membership - 2   | U19b000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_3 | Book Order            | U07x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_4 | Meeting Room          | U02x000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_5 | Reply All             | U16x000S | MISSING (0), CORRECT (1), INCORRECT (7), Not reached / not attempted (N) |
| PS2_6 | Locate Email          | U11b000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
| PS2_7 | Lamp Return           | U23x000S | POLYTOMOUS (0 to 3), No response (R ), Not reached / not attempted (N)   |
** item results
:PROPERTIES:
:header-args: :exports none
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: piacc/item_results
:END:

- Total or first action time has weak association with \theta.
- # of actions and first action time have weak or no assciation.
- # of actions has some association with the response.
- \tau and first action time has negative correaltion.
- \tau and # of actions have weak or no association
- # of actions and \theta time have positive correlation.

**** party_invitations-1
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: party_invitations-1
:END:
#+begin_src sh :async
out_dir="party_invitations-1/"
cd $out_dir
cd figure
# convert -density 300 theta_tau_res.pdf theta_tau_res-%d.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+RESULTS:

[[file:party_invitations-1/figure/theta_tau_res.pdf]]
[[file:party_invitations-1/figure/time_action-3.png]]
[[file:party_invitations-1/figure/time_action_more-2.png]]
[[file:party_invitations-1/figure/time_action_more-5.png]]
[[file:party_invitations-1/figure/time_action_more-7.png]]
[[file:party_invitations-1/figure/time_action_more-8.png]]
[[file:party_invitations-1/figure/time_action_more-9.png]]
[[file:party_invitations-1/figure/time_action_more-10.png]]
[[file:party_invitations-1/figure/time_action_more-11.png]]
[[file:party_invitations-1/figure/time_action_more-13.png]]

**** party_invitations-2
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_BUNDLE: party_invitations-2
:END:

#+begin_src sh
out_dir="party_invitations-2/"
cd $out_dir
cd figure
# convert -density 300 theta_tau_res.pdf theta_tau_res-%d.png
convert -density 300 time_action_more.pdf time_action_more-%d.png
convert -density 300 time_action.pdf time_action-%d.png
#+end_src

#+RESULTS:

[[file:party_invitations-2/figure/theta_tau_res.pdf]]
[[file:party_invitations-2/figure/time_action-3.png]]
[[file:party_invitations-2/figure/time_action_more-2.png]]
[[file:party_invitations-2/figure/time_action_more-5.png]]
[[file:party_invitations-2/figure/time_action_more-7.png]]
[[file:party_invitations-2/figure/time_action_more-8.png]]
[[file:party_invitations-2/figure/time_action_more-9.png]]
[[file:party_invitations-2/figure/time_action_more-10.png]]
[[file:party_invitations-2/figure/time_action_more-11.png]]
[[file:party_invitations-2/figure/time_action_more-13.png]]
**** cd_tally
#+begin_src sh
out_dir="cd_tally/"
#+end_src
**** sprained_ankle-1
#+begin_src sh
out_dir="sprained_ankle-1/"
#+end_src
**** sprained_ankle-2
#+begin_src sh
out_dir="sprained_ankle-2/"
#+end_src
**** tickets
#+begin_src sh
out_dir="tickets/"
#+end_src
**** class_attendance
#+begin_src sh
out_dir="class_attendance/"
#+end_src
**** club_membership-1
#+begin_src sh
out_dir="club_membership-1/"
#+end_src
**** club_membership-2
#+begin_src sh
out_dir="club_membership-2/"
#+end_src
**** book_order
#+begin_src sh
out_dir="book_order/"
#+end_src
**** meeting_room
#+begin_src sh
out_dir="meeting_room/"
#+end_src
**** reply_all
#+begin_src sh
out_dir="reply_all/"
#+end_src
**** locate_email
#+begin_src sh
out_dir="locate_email/"
#+end_src
**** lamp_return
#+begin_src sh
out_dir="lamp_return/"
#+end_src

** Model
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION: model
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :chapter true
:EXPORT_HUGO_WEIGHT: 2
:END:
* COMMENT Local Variables
# Local Variables:
# org-babel-default-header-args:R: ((:session . "*R-PIACC*") (:export . "both") (:results . "output replace"))
# eval: (flyspell-mode -1)
# eval: (spell-fu-mode -1)
# End:
